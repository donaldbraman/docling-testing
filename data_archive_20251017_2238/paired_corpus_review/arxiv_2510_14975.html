<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>WithAnyone: Towards Controllable and ID Consistent Image Generation</title>
<!--Generated on Thu Oct 16 14:35:36 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2510.14975v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S1" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S2" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title">Single-ID Preservation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title">Multi-ID Preservation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S2.SS0.SSS0.Px3" title="In 2 Related Work ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title">ID-Centric Datasets and Benchmarks.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S3" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>MultiID-2M: Paired Multi-Person Dataset Construction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S4" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>MultiID-Bench: Comprehensive ID Customization Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S5" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>WithAnyone: Controllable and ID-Consistent Generation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S5.SS1" title="In 5 WithAnyone: Controllable and ID-Consistent Generation ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Training Objectives</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S5.SS2" title="In 5 WithAnyone: Controllable and ID-Consistent Generation ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Training pipeline</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.SS1" title="In 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Quantitative Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.SS2" title="In 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Qualitative Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.SS3" title="In 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Ablation and User Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S7" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A1" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Family of WithAnyone</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A2" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>MultiID-2M Construction Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A2.SS1" title="In Appendix B MultiID-2M Construction Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Dataset Construction Pipeline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A2.SS2" title="In Appendix B MultiID-2M Construction Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Dataset Statistics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A3" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Benchmark and Metrics Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A4" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Galleries of WithAnyone</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A5" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Model Framework Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A6" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Experimental Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A6.SS1" title="In Appendix F Experimental Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F.1 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A6.SS2" title="In Appendix F Experimental Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F.2 </span>More Discussion on the Quantitative Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A7" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Ablation Study Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A8" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>User Study Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A8.SS1" title="In Appendix H User Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H.1 </span>Correlation Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A8.SS2" title="In Appendix H User Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H.2 </span>Participant Instructions</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A8.SS2.SSS0.Px1" title="In H.2 Participant Instructions ‣ Appendix H User Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title">Identity similarity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A8.SS2.SSS0.Px2" title="In H.2 Participant Instructions ‣ Appendix H User Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title">Copy-and-paste effect (excessive mimicry of the reference)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A8.SS2.SSS0.Px3" title="In H.2 Participant Instructions ‣ Appendix H User Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title">Prompt following</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A8.SS2.SSS0.Px4" title="In H.2 Participant Instructions ‣ Appendix H User Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title">Aesthetics</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A9" title="In WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span>Prompts for Language Models</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A9.SS1" title="In Appendix I Prompts for Language Models ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I.1 </span>Dataset Captioning</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text" style="position:relative; bottom:-6.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="41" id="g1" src="files/withanyone_logo_v3.jpg" width="71"/></span>
WithAnyone: Towards Controllable and ID Consistent Image Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Hengyuan Xu<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1,2</span></sup>
 Wei Cheng<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2,†</span></sup>
 Peng Xing<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>
 Yixiao Fang<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>
 Shuhan Wu<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>
<br class="ltx_break"/> Rui Wang<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>
 Xianfang Zeng<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>
 Daxin Jiang<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>
 Gang Yu<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2,‡</span></sup>
 Xingjun Ma<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1,‡</span></sup>
 Yu-Gang Jiang<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"/>
<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup> Fudan University
 <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup> StepFun

<br class="ltx_break"/>
<span class="ltx_text" style="position:relative; bottom:-0.2pt;--ltx-fg-color:#809FE1;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="27" id="g2" src="x1.png" width="27"/></span><span class="ltx_text" style="--ltx-fg-color:#809FE1;"> <a class="ltx_ref ltx_href ltx_font_bold" href="https://doby-xu.github.io/WithAnyone/" title="">Project Page</a>
 <span class="ltx_text" style="position:relative; bottom:-0.2pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="27" id="g3" src="x2.png" width="27"/></span> <a class="ltx_ref ltx_href ltx_font_bold" href="https://huggingface.co/datasets/WithAnyone/MultiID-2M" title="">MultiID-2M</a>
 <span class="ltx_text" style="position:relative; bottom:-0.2pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="27" id="g4" src="x3.png" width="27"/></span> <a class="ltx_ref ltx_href ltx_font_bold" href="https://huggingface.co/datasets/WithAnyone/MultiID-Bench" title="">MultiID-Bench</a>
 <span class="ltx_text" style="position:relative; bottom:-0.2pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="27" id="g5" src="x4.png" width="27"/></span> <a class="ltx_ref ltx_href ltx_font_bold" href="https://huggingface.co/WithAnyone/WithAnyone" title="">Models</a>
 <span class="ltx_text" style="position:relative; bottom:-0.2pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="27" id="g6" src="x5.png" width="27"/></span> <a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/doby-xu/WithAnyone" title="">Code</a>
</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term <span class="ltx_text ltx_font_bold ltx_font_italic">copy-paste</span>, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation.
To address these limitations, we (1) construct a large-scale paired dataset <span class="ltx_text ltx_font_bold">MultiID-2M</span> tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in <span class="ltx_text ltx_font_bold">WithAnyone</span>, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity.
Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.</p>
</div>
<div class="ltx_logical-block">
<div class="ltx_para" id="p2">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="466" id="g7" src="x6.png" width="951"/>
</div>
<figure class="ltx_figure ltx_align_center" id="S0.F1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Showcases of WithAnyone.<span class="ltx_text ltx_font_medium"> WithAnyone is capable of generating high-quality, controllable, and ID-consistent images by leveraging ID-contrastive training on the proposed </span>MultiID-2M<span class="ltx_text ltx_font_medium"> dataset.</span></span></figcaption>
</figure>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><math alttext="{\dagger}" class="ltx_Math" display="inline" id="footnotex1.m1" intent=":literal"><semantics><mo>†</mo><annotation encoding="application/x-tex">{\dagger}</annotation></semantics></math> Wei Cheng leads this project; ‡Corresponding authors.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">With the rapid progress of generative artificial intelligence, controllable image generation via reference images or image prompting <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib57" title="">57</a>]</cite> and identity-consistent (ID-consistent) generation <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib15" title="">15</a>]</cite> have achieved remarkable advances: modern models can synthesize portraits that closely match the provided individual. Recent efforts <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib4" title="">4</a>]</cite> push resemblance toward near-perfect reproduction. While pursuing higher similarity seems natural, beyond a certain point, excessive fidelity becomes counterproductive.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">In real photographs of the same person, identity similarity varies substantially due to natural changes in pose, expression, makeup, and illumination (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">2</span></a>). By contrast, many generative models adhere to the reference image far more rigidly than this natural range of variation. Although such over-optimization may seem beneficial, it suppresses legitimate variation, reducing controllability and limiting practical usability. We term this failure mode the <span class="ltx_text ltx_font_bold">copy-paste artifact</span>: rather than synthesizing an identity in a flexible, controllable manner, the model effectively copies the reference image into the output (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">2</span></a>). In this work, we formalize this artifact, develop metrics to quantify it, and propose a novel training strategy to mitigate it.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">Mitigating copy-paste artifacts is fundamentally constrained by the lack of suitable training data. While numerous large-scale face datasets exist <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib51" title="">51</a>]</cite>, they remain ill-suited for controllable multi-identity generation. Critically, few datasets provide paired references for each identity-multiple images of the same person across diverse expressions, poses, hairstyles, and viewpoints.
As a result, most prior work resorts to single-person, reconstruction-based training <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib50" title="">50</a>]</cite>, where the reference and target coincide. This setup inherently promotes copying and exacerbates copy-paste artifacts. Constructing datasets with multiple references per identity, particularly in group photos, and developing methods to effectively exploit such data remain open challenges.</p>
</div>
<figure class="ltx_figure" id="S1.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="238" id="S1.F2.g1" src="x7.png" width="789"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="560" id="S1.F2.g2" src="x8.png" width="747"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="260" id="S1.F2.g3" src="x9.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Our Observation<span class="ltx_text ltx_font_medium">. Natural variations, such as head pose, expression, and makeup, may cause more face similarity decrease than expected. Copying reference image limits models’ ability to respond to expression and makeup adjustment prompts.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p">In this work, we introduce a large-scale open-source Multi-ID dataset, <span class="ltx_text ltx_font_bold">MultiID-2M</span>, together with a comprehensive benchmark, <span class="ltx_text ltx_font_bold">MultiID-Bench</span>, designed for intrinsic evaluation of multi-identity image generation. MultiID-2M contains 500k group photos featuring 1–5 recognizable celebrities. For each celebrity, hundreds of individual images are provided as paired references, covering diverse expressions, hairstyles, and viewing angles. In addition, 1.5M unpaired group photos without references are included.
MultiID-Bench establishes a standardized evaluation protocol for multi-identity generation. Beyond widely adopted metrics such as ID similarity <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib11" title="">11</a>]</cite>, it quantifies copy-paste artifacts by measuring distances between generated images, references, and ground truth. Evaluation on 12 state-of-the-art customization models highlights a clear trade-off between ID similarity and copy-paste artifacts (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S5.F5" title="Figure 5 ‣ 5.1 Training Objectives ‣ 5 WithAnyone: Controllable and ID-Consistent Generation ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p">Furthermore, we present <span class="ltx_text ltx_font_bold">WithAnyone</span>, a novel identity customization model built on the FLUX <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib27" title="">27</a>]</cite> architecture, as a step toward mitigating copy-paste artifacts. WithAnyone maintains state-of-the-art identity similarity (with regard to target image) while substantially reducing copy-paste, thereby breaking the long-observed trade-off between fidelity and artifacts. This advance is enabled by a paired-training strategy combined with an ID contrastive loss enhanced with a large negative pool, both made possible by our paired dataset. The labeled identities and their reference images enable the construction of an extended negative pool (images of different identities), which provides stronger discrimination signals during optimization.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p">In summary, our main contributions are:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">MultiID-2M:</span> A large-scale dataset of 500k group photos containing multiple identifiable celebrities, each with hundreds of reference images capturing diverse variations, along with 1.5M additional unpaired group photos. This resource supports pre-training and evaluation of multi-identity generation models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">MultiID-Bench:</span> A comprehensive benchmark with standardized evaluation protocols for identity customization, enabling systematic and intrinsic assessment of multi-identity image generation methods.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">WithAnyone:</span> A novel ID customization model built on FLUX that achieves state-of-the-art performance, generating high-fidelity multi-identity images while mitigating copy-paste artifacts and enhancing visual quality.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="385" id="S2.F3.g1" src="x10.png" width="814"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Overview of WithAnyone.<span class="ltx_text ltx_font_medium"> It builds on a large-scale dataset, MultiID-2M, constructed through a four-step pipeline: (1) collect and cluster single-ID data based on identity similarity; (2) gather multi-ID data via targeted searches using desired identity names with negative keywords for filtering; (3) form image pairs by matching faces between single-ID and multi-ID data; and (4) apply post-processing for quality control and stylization. Training proceeds in four stages: (1) pre-train on single-ID, multi-ID, and open-domain images with fixed prompts; (2) train with image-caption supervision; (3) fine-tune with ID-paired data; and (4) perform quality tuning using a curated high-quality subset.</span></span></figcaption>
</figure>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Single-ID Preservation.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p">The generation of Identity-preserving images is a core topic in customized synthesis <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib35" title="">35</a>]</cite>. Many methods in the UNet/Stable Diffusion era inject learned embeddings (e.g., CLIP or ArcFace) via cross‑attention or adapters <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib42" title="">42</a>]</cite>. With the rise of DiT‑style backbones <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib27" title="">27</a>]</cite> (e.g., SD3, FLUX), progress in ID preservation like PuLID <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib14" title="">14</a>]</cite>, also attracts great attention.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Multi-ID Preservation.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p">Multi‑ID preservation remains relatively underexplored. Some works target spatial control of multiple identities <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib68" title="">68</a>]</cite>, while others focus on identity fidelity. Methods such as XVerse <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib4" title="">4</a>]</cite> and UMO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib8" title="">8</a>]</cite> use VAE‑derived face embeddings concatenated with model inputs, which can produce pixel‑level copy‑paste artifacts and reduce controllability. DynamicID <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib18" title="">18</a>]</cite><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Excluded from our experiments due to unavailability of code and pretrained models.</span></span></span> achieves improved controllability but is constrained by limited task‑specific data and evaluation standards. Other general-purpose customization and editing models <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib53" title="">53</a>]</cite> can also synthesize images containing multiple identities, but their ID similarity is often compromised for generality.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">ID-Centric Datasets and Benchmarks.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p">Although there are numerous single‑ID datasets <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib51" title="">51</a>]</cite> and multi‑ID collections <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib22" title="">22</a>]</cite>, paired reference images are scarce, so reconstruction remains the dominant training objective for multi‑ID datasets. Representative datasets are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A2.T4" title="Table 4 ‣ B.2 Dataset Statistics ‣ Appendix B MultiID-2M Construction Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">4</span></a>. Evaluation protocols are underdeveloped: several works (e.g., PuLID <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib14" title="">14</a>]</cite>, UniPortrait <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib15" title="">15</a>]</cite>, and others <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib68" title="">68</a>]</cite>) construct test sets by sampling identities from CelebA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib29" title="">29</a>]</cite>, which undermines reproducibility. Recent efforts benchmark multiple reference generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib54" title="">54</a>]</cite> while focusing on general customization. To address this, we release a curated multi‑ID benchmark with standardized splits and comprehensive metrics to facilitate future research.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>MultiID-2M: Paired Multi-Person Dataset Construction</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">MultiID-2M is a large-scale multi-person dataset constructed via a four-stage pipeline: (1) collect single-ID images from the web and construct a clean reference bank by clustering ArcFace <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib11" title="">11</a>]</cite> embeddings, yielding <math alttext="\sim" class="ltx_Math" display="inline" id="S3.p1.m1" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>1M reference images across <math alttext="\sim" class="ltx_Math" display="inline" id="S3.p1.m2" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>3k identities (averaging 400 per identity); (2) retrieve candidate group photos via multi-name and scene-aware queries and detect faces; (3) assign identities by matching ArcFace embeddings to single-ID cluster centers using cosine similarity (threshold 0.4); and (4) perform automated filtering and annotation, including Recognize Anything <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib69" title="">69</a>]</cite>, aesthetic scoring <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib12" title="">12</a>]</cite>, OCR-based watermark/logo removal, and LLM-based caption generation <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib1" title="">1</a>]</cite>. The final corpus comprises <math alttext="\sim" class="ltx_Math" display="inline" id="S3.p1.m3" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>500k identified multi-ID images with matched references from the reference bank, as well as <math alttext="\sim" class="ltx_Math" display="inline" id="S3.p1.m4" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>1.5M additional unidentified multi-ID images for reconstruction training, covering <math alttext="\sim" class="ltx_Math" display="inline" id="S3.p1.m5" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>25k unique identities, with diverse nationalities and ethnicities.
Further details of the construction pipeline and dataset statistics are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A2" title="Appendix B MultiID-2M Construction Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="313" id="S3.F4.sf1.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Model Architecture</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="346" id="S3.F4.sf2.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Training Objectives</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" style="font-size:90%;">(a) <span class="ltx_text ltx_font_bold">Architecture of WithAnyone</span>: Each reference is encoded by both a face-recognition network and a general image encoder, yielding identity-discriminative signals and complementary mid-level features. Face embeddings are restricted to attend only to image tokens within their corresponding face regions. (b) <span class="ltx_text ltx_font_bold">Training Objectives of WithAnyone</span>: In addition to the diffusion loss, we incorporate an ID contrastive loss and a ground-truth–aligned ID loss, which together provide consistent and accurate identity supervision.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>MultiID-Bench: Comprehensive ID Customization Evaluation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p">MultiID-Bench is a unified benchmark for group-photo (multi-ID) generation. It samples rare, long-tail identities with no overlap to training data, yielding 435 test cases. Each case consists of one ground-truth (GT) image containing 1–4 people, the corresponding 1–4 reference images as inputs, and a prompt describing the GT. Detailed statistics are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A2" title="Appendix B MultiID-2M Construction Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p">Evaluation considers both identity fidelity and generation quality. Let <math alttext="\mathbf{r},\mathbf{t},\mathbf{g}" class="ltx_Math" display="inline" id="S4.p2.m1" intent=":literal"><semantics><mrow><mi>𝐫</mi><mo>,</mo><mi>𝐭</mi><mo>,</mo><mi>𝐠</mi></mrow><annotation encoding="application/x-tex">\mathbf{r},\mathbf{t},\mathbf{g}</annotation></semantics></math> denote the face embeddings of the reference identity, the target (ground-truth), and the generated image, respectively. We define similarity between two embeddings as <math alttext="\mathrm{Sim}(\mathbf{a},\mathbf{b})" class="ltx_Math" display="inline" id="S4.p2.m2" intent=":literal"><semantics><mrow><mi>Sim</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐚</mi><mo>,</mo><mi>𝐛</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{Sim}(\mathbf{a},\mathbf{b})</annotation></semantics></math>, specifically we term the generated image’s face similarity with regard to GT as <math alttext="\mathrm{Sim_{GT}}" class="ltx_Math" display="inline" id="S4.p2.m3" intent=":literal"><semantics><msub><mi>Sim</mi><mi>GT</mi></msub><annotation encoding="application/x-tex">\mathrm{Sim_{GT}}</annotation></semantics></math>, and to reference as <math alttext="\mathrm{Sim_{Ref}}" class="ltx_Math" display="inline" id="S4.p2.m4" intent=":literal"><semantics><msub><mi>Sim</mi><mi>Ref</mi></msub><annotation encoding="application/x-tex">\mathrm{Sim_{Ref}}</annotation></semantics></math>,</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{Sim}(\mathbf{a},\mathbf{b})=\frac{\mathbf{a}^{\top}\mathbf{b}}{\|\mathbf{a}\|\,\|\mathbf{b}\|}," class="ltx_Math" display="block" id="S4.E1.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>Sim</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐚</mi><mo>,</mo><mi>𝐛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msup><mi>𝐚</mi><mo>⊤</mo></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝐛</mi></mrow><mrow><mrow><mo stretchy="false">‖</mo><mi>𝐚</mi><mo stretchy="false">‖</mo></mrow><mo lspace="0.170em" rspace="0em">​</mo><mrow><mo stretchy="false">‖</mo><mi>𝐛</mi><mo stretchy="false">‖</mo></mrow></mrow></mfrac></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathrm{Sim}(\mathbf{a},\mathbf{b})=\frac{\mathbf{a}^{\top}\mathbf{b}}{\|\mathbf{a}\|\,\|\mathbf{b}\|},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">Specially, we denote <math alttext="\mathrm{Sim_{Ref}}=\mathrm{Sim}(\mathbf{r},\mathbf{g})" class="ltx_Math" display="inline" id="S4.p2.m5" intent=":literal"><semantics><mrow><msub><mi>Sim</mi><mi>Ref</mi></msub><mo>=</mo><mrow><mi>Sim</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐫</mi><mo>,</mo><mi>𝐠</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathrm{Sim_{Ref}}=\mathrm{Sim}(\mathbf{r},\mathbf{g})</annotation></semantics></math> and <math alttext="\mathrm{Sim_{GT}}=\mathrm{Sim}(\mathbf{t},\mathbf{g})" class="ltx_Math" display="inline" id="S4.p2.m6" intent=":literal"><semantics><mrow><msub><mi>Sim</mi><mi>GT</mi></msub><mo>=</mo><mrow><mi>Sim</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐭</mi><mo>,</mo><mi>𝐠</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathrm{Sim_{GT}}=\mathrm{Sim}(\mathbf{t},\mathbf{g})</annotation></semantics></math>.
Prior works <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib8" title="">8</a>]</cite> has largely reported only <math alttext="\mathrm{Sim_{Ref}}" class="ltx_Math" display="inline" id="S4.p2.m7" intent=":literal"><semantics><msub><mi>Sim</mi><mi>Ref</mi></msub><annotation encoding="application/x-tex">\mathrm{Sim_{Ref}}</annotation></semantics></math>, which inadvertently favors trivial copy-paste: directly replicating the reference appearance maximizes the score, even when the prompt specifies changes in pose, expression, or viewpoint. In contrast, MultiID-Bench uses <math alttext="\mathrm{Sim_{GT}}" class="ltx_Math" display="inline" id="S4.p2.m8" intent=":literal"><semantics><msub><mi>Sim</mi><mi>GT</mi></msub><annotation encoding="application/x-tex">\mathrm{Sim_{GT}}</annotation></semantics></math> the similarity to the ground-truth identity described by the prompt as the primary metric. This design penalizes excessive copying when natural variations (e.g., pose, expression, occlusion) are expected, while rewarding faithful realization of the prompted scene.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p">We define the angular distance as <math alttext="\theta_{ab}=\arccos(\mathrm{Sim}(a,b))" class="ltx_Math" display="inline" id="S4.p3.m1" intent=":literal"><semantics><mrow><msub><mi>θ</mi><mrow><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow></msub><mo>=</mo><mrow><mi>arccos</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Sim</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta_{ab}=\arccos(\mathrm{Sim}(a,b))</annotation></semantics></math> (geodesic distance on the unit sphere). The Copy-Paste metric is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{M_{CP}}(\mathbf{g}\mid\mathbf{t},\mathbf{r})=\frac{\theta_{gt}-\theta_{gr}}{\max(\theta_{tr},\,\varepsilon)}\in[-1,1]," class="ltx_Math" display="block" id="S4.E2.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi mathvariant="normal">M</mi><mi>CP</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝐠</mi><mo>∣</mo><mrow><mi>𝐭</mi><mo>,</mo><mi>𝐫</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>θ</mi><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msub><mo>−</mo><msub><mi>θ</mi><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mi>r</mi></mrow></msub></mrow><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>r</mi></mrow></msub><mo rspace="0.337em">,</mo><mi>ε</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac><mo>∈</mo><mrow><mo stretchy="false">[</mo><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathrm{M_{CP}}(\mathbf{g}\mid\mathbf{t},\mathbf{r})=\frac{\theta_{gt}-\theta_{gr}}{\max(\theta_{tr},\,\varepsilon)}\in[-1,1],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\varepsilon" class="ltx_Math" display="inline" id="S4.p3.m2" intent=":literal"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math> is a small constant for numerical stability. The metric thus captures the relative bias of <math alttext="\mathbf{g}" class="ltx_Math" display="inline" id="S4.p3.m3" intent=":literal"><semantics><mi>𝐠</mi><annotation encoding="application/x-tex">\mathbf{g}</annotation></semantics></math> toward the reference <math alttext="\mathbf{r}" class="ltx_Math" display="inline" id="S4.p3.m4" intent=":literal"><semantics><mi>𝐫</mi><annotation encoding="application/x-tex">\mathbf{r}</annotation></semantics></math> versus the ground truth <math alttext="\mathbf{t}" class="ltx_Math" display="inline" id="S4.p3.m5" intent=":literal"><semantics><mi>𝐭</mi><annotation encoding="application/x-tex">\mathbf{t}</annotation></semantics></math>, normalized by angular distance of <math alttext="\mathbf{r}" class="ltx_Math" display="inline" id="S4.p3.m6" intent=":literal"><semantics><mi>𝐫</mi><annotation encoding="application/x-tex">\mathbf{r}</annotation></semantics></math> and <math alttext="\mathbf{t}" class="ltx_Math" display="inline" id="S4.p3.m7" intent=":literal"><semantics><mi>𝐭</mi><annotation encoding="application/x-tex">\mathbf{t}</annotation></semantics></math>. A score of <math alttext="1" class="ltx_Math" display="inline" id="S4.p3.m8" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> means <math alttext="\mathbf{g}" class="ltx_Math" display="inline" id="S4.p3.m9" intent=":literal"><semantics><mi>𝐠</mi><annotation encoding="application/x-tex">\mathbf{g}</annotation></semantics></math> fully coincides with the reference (perfect copy-paste), while <math alttext="-1" class="ltx_Math" display="inline" id="S4.p3.m10" intent=":literal"><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math> means full agreement with the ground truth.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p">We additionally report identity blending, prompt fidelity (CLIP I/T), and aesthetics; formal definitions and further details are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A3" title="Appendix C Benchmark and Metrics Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>WithAnyone: Controllable and ID-Consistent Generation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">Building on the scale and paired‑reference supervision of the MultiID‑2M, we devise training strategies and tailored objectives that transcend reconstruction to enable robust, identity‑conditioned synthesis. This rich, identity‑labeled supervision not only substantially improves identity fidelity but also suppresses trivial copy–paste artifacts and affords finer control over multi‑identity composition. Motivated by these advantages, we introduce WithAnyone - a unified architecture and training recipe designed for controllable, high‑fidelity multi‑ID generation. Architectural schematics and implementation details are provided in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S3.F4" title="Figure 4 ‣ 3 MultiID-2M: Paired Multi-Person Dataset Construction ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">4</span></a> and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A5" title="Appendix E Model Framework Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Training Objectives</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Diffusion Loss.</span> We adopt the mini-batch empirical flow-matching loss. For each batch, we sample a data latent <math alttext="x_{1}\sim p_{\text{data}}" class="ltx_Math" display="inline" id="S5.SS1.p1.m1" intent=":literal"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>∼</mo><msub><mi>p</mi><mtext>data</mtext></msub></mrow><annotation encoding="application/x-tex">x_{1}\sim p_{\text{data}}</annotation></semantics></math>, Gaussian noise <math alttext="x_{0}\sim\mathcal{N}(0,I)" class="ltx_Math" display="inline" id="S5.SS1.p1.m2" intent=":literal"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">x_{0}\sim\mathcal{N}(0,I)</annotation></semantics></math>, and a timestep <math alttext="t\sim\mathcal{U}(0,1)" class="ltx_Math" display="inline" id="S5.SS1.p1.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒰</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">t\sim\mathcal{U}(0,1)</annotation></semantics></math>. We then form the interpolated latent <math alttext="x_{t}=(1-t)x_{0}+tx_{1}" class="ltx_Math" display="inline" id="S5.SS1.p1.m4" intent=":literal"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>t</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mn>0</mn></msub></mrow><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mn>1</mn></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">x_{t}=(1-t)x_{0}+tx_{1}</annotation></semantics></math> and regress the target velocity <math alttext="(x_{1}-x_{0})" class="ltx_Math" display="inline" id="S5.SS1.p1.m5" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><msub><mi>x</mi><mn>0</mn></msub></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_{1}-x_{0})</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\mathrm{diff}}=\big\|v_{\theta}(x_{t}^{(i)},t^{(i)},c^{(i)})-(x_{1}^{(i)}-x_{0}^{(i)})\big\|_{2}^{2}," class="ltx_Math" display="block" id="S5.E3.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>diff</mi></msub><mo>=</mo><msubsup><mrow><mo maxsize="1.200em" minsize="1.200em" stretchy="true">‖</mo><mrow><mrow><msub><mi>v</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>x</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>c</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>x</mi><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><msubsup><mi>x</mi><mn>0</mn><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="1.200em" minsize="1.200em" stretchy="true">‖</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\mathrm{diff}}=\big\|v_{\theta}(x_{t}^{(i)},t^{(i)},c^{(i)})-(x_{1}^{(i)}-x_{0}^{(i)})\big\|_{2}^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="c^{(i)}" class="ltx_Math" display="inline" id="S5.SS1.p1.m6" intent=":literal"><semantics><msup><mi>c</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">c^{(i)}</annotation></semantics></math> denotes the conditioning signal.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Quantitative comparison on the single-person subset of MultiID-Bench and OmniContext<span class="ltx_text ltx_font_medium">. <span class="ltx_rule" style="width:7.7pt;height:7.7pt;--ltx-bg-color:black;display:inline-block;"> </span>, <span class="ltx_rule" style="width:7.7pt;height:7.7pt;--ltx-bg-color:black;display:inline-block;"> </span>, and <span class="ltx_rule" style="width:7.7pt;height:7.7pt;--ltx-bg-color:black;display:inline-block;"> </span> indicate the first-, second-, and third-best performance, respectively. For Copy-Paste ranking, only cases with <math alttext="\mathrm{Sim(GT)}&gt;0.40" class="ltx_Math" display="inline" id="S5.T1.m2" intent=":literal"><semantics><mrow><mrow><mi>Sim</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>GT</mi><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mn>0.40</mn></mrow><annotation encoding="application/x-tex">\mathrm{Sim(GT)}&gt;0.40</annotation></semantics></math> are considered.</span></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S5.T1.st1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">MultiID-Bench</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:278.5pt;height:161.3pt;vertical-align:-78.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-59.7pt,34.6pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span class="ltx_text ltx_font_bold">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Identity Metrics</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Generation Quality</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t">Sim(GT) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.st1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">Sim(Ref) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.st1.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CP <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.st1.m3" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">CLIP-I <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.st1.m4" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">CLIP-T <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.st1.m5" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">Aes <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.st1.m6" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">DreamO</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.454</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.694</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.303</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.793</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.322</td>
<td class="ltx_td ltx_align_center ltx_border_t">4.877</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OmniGen</th>
<td class="ltx_td ltx_align_center">0.398</td>
<td class="ltx_td ltx_align_center">0.602</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.248</td>
<td class="ltx_td ltx_align_center">0.780</td>
<td class="ltx_td ltx_align_center">0.317</td>
<td class="ltx_td ltx_align_center">5.069</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OmniGen2</th>
<td class="ltx_td ltx_align_center">0.365</td>
<td class="ltx_td ltx_align_center">0.475</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.142</td>
<td class="ltx_td ltx_align_center">0.787</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.331</span></td>
<td class="ltx_td ltx_align_center">4.991</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">FLUX.1 Kontext</th>
<td class="ltx_td ltx_align_center">0.324</td>
<td class="ltx_td ltx_align_center">0.408</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.099</td>
<td class="ltx_td ltx_align_center">0.755</td>
<td class="ltx_td ltx_align_center">0.327</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">5.319</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Qwen-Image-Edit</th>
<td class="ltx_td ltx_align_center">0.324</td>
<td class="ltx_td ltx_align_center">0.409</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.093</td>
<td class="ltx_td ltx_align_center">0.776</td>
<td class="ltx_td ltx_align_center">0.316</td>
<td class="ltx_td ltx_align_center">5.056</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GPT-4o Native</th>
<td class="ltx_td ltx_align_center">0.425</td>
<td class="ltx_td ltx_align_center">0.579</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.178</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.794</span></td>
<td class="ltx_td ltx_align_center">0.311</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">5.344</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UNO</th>
<td class="ltx_td ltx_align_center">0.304</td>
<td class="ltx_td ltx_align_center">0.428</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.141</td>
<td class="ltx_td ltx_align_center">0.765</td>
<td class="ltx_td ltx_align_center">0.314</td>
<td class="ltx_td ltx_align_center">4.923</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">USO</th>
<td class="ltx_td ltx_align_center">0.401</td>
<td class="ltx_td ltx_align_center">0.635</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.286</td>
<td class="ltx_td ltx_align_center">0.790</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.329</span></td>
<td class="ltx_td ltx_align_center">5.077</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UMO</th>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.458</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.732</span></td>
<td class="ltx_td ltx_align_center ltx_border_r">0.359</td>
<td class="ltx_td ltx_align_center">0.783</td>
<td class="ltx_td ltx_align_center">0.305</td>
<td class="ltx_td ltx_align_center">4.850</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">UniPortrait</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.447</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.677</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.265</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.793</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.319</td>
<td class="ltx_td ltx_align_center ltx_border_t">5.018</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ID-Patch</th>
<td class="ltx_td ltx_align_center">0.426</td>
<td class="ltx_td ltx_align_center">0.633</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.231</span></td>
<td class="ltx_td ltx_align_center">0.792</td>
<td class="ltx_td ltx_align_center">0.312</td>
<td class="ltx_td ltx_align_center">4.900</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">InfU</th>
<td class="ltx_td ltx_align_center">0.439</td>
<td class="ltx_td ltx_align_center">0.630</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.233</td>
<td class="ltx_td ltx_align_center">0.772</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.328</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">5.359</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">PuLID</th>
<td class="ltx_td ltx_align_center">0.452</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.705</span></td>
<td class="ltx_td ltx_align_center ltx_border_r">0.315</td>
<td class="ltx_td ltx_align_center">0.779</td>
<td class="ltx_td ltx_align_center">0.305</td>
<td class="ltx_td ltx_align_center">4.839</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">InstantID</th>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.464</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.734</span></td>
<td class="ltx_td ltx_align_center ltx_border_r">0.337</td>
<td class="ltx_td ltx_align_center">0.764</td>
<td class="ltx_td ltx_align_center">0.295</td>
<td class="ltx_td ltx_align_center">5.255</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Ours</th>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.460</span></td>
<td class="ltx_td ltx_align_center">0.578</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.144</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.798</span></td>
<td class="ltx_td ltx_align_center">0.313</td>
<td class="ltx_td ltx_align_center">4.783</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">GT</th>
<td class="ltx_td ltx_align_center ltx_border_t">1.000</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.521</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-0.999</td>
<td class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_t">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_t">N/A</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Ref</th>
<td class="ltx_td ltx_align_center ltx_border_bb">0.521</td>
<td class="ltx_td ltx_align_center ltx_border_bb">1.000</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.999</td>
<td class="ltx_td ltx_align_center ltx_border_bb">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_bb">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_bb">N/A</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S5.T1.st2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">OmniContext Single Character Subset<span class="ltx_text ltx_font_medium"> </span></span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:191.3pt;height:164.8pt;vertical-align:-80.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.9pt,20.6pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span class="ltx_text ltx_font_bold">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2"><span class="ltx_text ltx_font_bold">Quality Metrics</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold">Overall</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t">PF <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.st2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.st2.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">Overall <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.st2.m3" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">DreamO</th>
<td class="ltx_td ltx_align_center ltx_border_t">8.13</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.09</td>
<td class="ltx_td ltx_align_center ltx_border_t">7.02</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OmniGen</th>
<td class="ltx_td ltx_align_center">7.50</td>
<td class="ltx_td ltx_align_center ltx_border_r">5.52</td>
<td class="ltx_td ltx_align_center">5.47</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OmniGen2</th>
<td class="ltx_td ltx_align_center">8.64</td>
<td class="ltx_td ltx_align_center ltx_border_r">8.50</td>
<td class="ltx_td ltx_align_center">8.34</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">FLUX.1 Kontext</th>
<td class="ltx_td ltx_align_center">7.72</td>
<td class="ltx_td ltx_align_center ltx_border_r">8.60</td>
<td class="ltx_td ltx_align_center">7.94</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Qwen-Image-Edit</th>
<td class="ltx_td ltx_align_center">7.66</td>
<td class="ltx_td ltx_align_center ltx_border_r">8.16</td>
<td class="ltx_td ltx_align_center">7.51</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GPT-4o Native</th>
<td class="ltx_td ltx_align_center">7.98</td>
<td class="ltx_td ltx_align_center ltx_border_r">9.06</td>
<td class="ltx_td ltx_align_center">8.12</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UNO</th>
<td class="ltx_td ltx_align_center">7.22</td>
<td class="ltx_td ltx_align_center ltx_border_r">7.72</td>
<td class="ltx_td ltx_align_center">7.04</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">USO</th>
<td class="ltx_td ltx_align_center">6.96</td>
<td class="ltx_td ltx_align_center ltx_border_r">7.88</td>
<td class="ltx_td ltx_align_center">6.70</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UMO</th>
<td class="ltx_td ltx_align_center">6.56</td>
<td class="ltx_td ltx_align_center ltx_border_r">7.92</td>
<td class="ltx_td ltx_align_center">6.79</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">UniPortrait</th>
<td class="ltx_td ltx_align_center ltx_border_t">6.62</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">6.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">5.55</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ID-Patch</th>
<td class="ltx_td ltx_align_center">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">InfU</th>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">7.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_r">4.62</td>
<td class="ltx_td ltx_align_center">4.70</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">PuLID</th>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">6.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">6.83</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">5.78</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">InstantID</th>
<td class="ltx_td ltx_align_center">4.89</td>
<td class="ltx_td ltx_align_center ltx_border_r">5.49</td>
<td class="ltx_td ltx_align_center">4.35</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">7.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">7.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">6.52</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Ground-truth-Aligned ID Loss.</span> Since ArcFace embedding requires landmark detection and alignment, directly extracting landmarks from <math alttext="I_{\text{gen}}" class="ltx_Math" display="inline" id="S5.SS1.p2.m1" intent=":literal"><semantics><msub><mi>I</mi><mtext>gen</mtext></msub><annotation encoding="application/x-tex">I_{\text{gen}}</annotation></semantics></math> is unreliable because generated images are obtained through noisy diffusion or one-step denoising. Prior methods compromise: PortraitBooth <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib39" title="">39</a>]</cite> applies the loss only at low noise levels (<math alttext="t&lt;0.25" class="ltx_Math" display="inline" id="S5.SS1.p2.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>&lt;</mo><mn>0.25</mn></mrow><annotation encoding="application/x-tex">t&lt;0.25</annotation></semantics></math>), discarding supervision at higher noise, while PuLID <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib14" title="">14</a>]</cite> fully denoises generated results at significant computational cost. In contrast, we align the generated image using GT landmarks, thereby avoiding noisy landmark extraction. We minimize the cosine distance between GT-aligned ArcFace embeddings of the generated and ground-truth (GT) faces:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{ID}}=1-\cos(\mathbf{g},\mathbf{t})" class="ltx_Math" display="block" id="S5.E4.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>ID</mtext></msub><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>cos</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝐠</mi><mo>,</mo><mi>𝐭</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{ID}}=1-\cos(\mathbf{g},\mathbf{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathbf{g}" class="ltx_Math" display="inline" id="S5.SS1.p2.m3" intent=":literal"><semantics><mi>𝐠</mi><annotation encoding="application/x-tex">\mathbf{g}</annotation></semantics></math> and <math alttext="\mathbf{t}" class="ltx_Math" display="inline" id="S5.SS1.p2.m4" intent=":literal"><semantics><mi>𝐭</mi><annotation encoding="application/x-tex">\mathbf{t}</annotation></semantics></math> are ArcFace embeddings of the generated and GT images. This design (1) enables applying the ID loss across all noise levels, (2) incurs negligible overhead throughout training, and (3) implicitly supervises generated landmarks. Ablation studies (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.SS3" title="6.3 Ablation and User Studies ‣ 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">6.3</span></a>) demonstrate more accurate identity measurement and substantially improved identity preservation.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p">Denoting the face recognition model as <math alttext="f(\cdot,\cdot)" class="ltx_Math" display="inline" id="S5.SS1.p3.m1" intent=":literal"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\cdot)</annotation></semantics></math> (Arcface <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib11" title="">11</a>]</cite>, in our case), and the coupled detection model as <math alttext="g(\cdot)" class="ltx_Math" display="inline" id="S5.SS1.p3.m2" intent=":literal"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math> (RetinaFace <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib10" title="">10</a>]</cite>), the generated image as <math alttext="\mathbf{G}" class="ltx_Math" display="inline" id="S5.SS1.p3.m3" intent=":literal"><semantics><mi>𝐆</mi><annotation encoding="application/x-tex">\mathbf{G}</annotation></semantics></math>, and the ground-truth image as <math alttext="\mathbf{T}" class="ltx_Math" display="inline" id="S5.SS1.p3.m4" intent=":literal"><semantics><mi>𝐓</mi><annotation encoding="application/x-tex">\mathbf{T}</annotation></semantics></math>, a embedding extraction should be performed as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{t}=f(g(\mathbf{T}),\mathbf{T})," class="ltx_Math" display="block" id="S5.E5.m1" intent=":literal"><semantics><mrow><mrow><mi>𝐭</mi><mo>=</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐓</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>𝐓</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{t}=f(g(\mathbf{T}),\mathbf{T}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="g(\mathbf{T})" class="ltx_Math" display="inline" id="S5.SS1.p3.m5" intent=":literal"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐓</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\mathbf{T})</annotation></semantics></math> are the detected landmarks, and <math alttext="f(\cdot,\cdot)" class="ltx_Math" display="inline" id="S5.SS1.p3.m6" intent=":literal"><semantics><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\cdot,\cdot)</annotation></semantics></math> extracts the aligned face embedding. Instead of using <math alttext="g(\mathbf{G})" class="ltx_Math" display="inline" id="S5.SS1.p3.m7" intent=":literal"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐆</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\mathbf{G})</annotation></semantics></math> as landmarks for <math alttext="\mathbf{G}" class="ltx_Math" display="inline" id="S5.SS1.p3.m8" intent=":literal"><semantics><mi>𝐆</mi><annotation encoding="application/x-tex">\mathbf{G}</annotation></semantics></math>, our GT-aligned ID loss is computed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{id}}=1-\cos(f(g(\mathbf{T}),\mathbf{G}),f(g(\mathbf{T}),\mathbf{T}))." class="ltx_Math" display="block" id="S5.E6.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>id</mtext></msub><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mrow><mi>cos</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐓</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>𝐆</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐓</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>𝐓</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{id}}=1-\cos(f(g(\mathbf{T}),\mathbf{G}),f(g(\mathbf{T}),\mathbf{T})).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">ID Contrastive Loss With Extended Negatives.</span> To further strengthen identity preservation, we introduce an ID contrastive loss that explicitly pulls the generated image closer to its reference images in the face embedding space while pushing it away from other identities. The loss follows the InfoNCE <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib31" title="">31</a>]</cite> formulation:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{CL}}=-\log\frac{\exp(\cos(\mathbf{g},\mathbf{r})/\tau)}{\sum_{j=1}^{M}\exp(\cos(\mathbf{g},\mathbf{n}_{j}))/\tau)}," class="ltx_math_unparsed" display="block" id="S5.E7.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>CL</mtext></msub><mo>=</mo><mrow><mo rspace="0.167em">−</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>cos</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝐠</mi><mo>,</mo><mi>𝐫</mi><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><mi>τ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mi>exp</mi><mrow><mo stretchy="false">(</mo><mi>cos</mi><mrow><mo stretchy="false">(</mo><mi>𝐠</mi><mo>,</mo><msub><mi>𝐧</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo>/</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{CL}}=-\log\frac{\exp(\cos(\mathbf{g},\mathbf{r})/\tau)}{\sum_{j=1}^{M}\exp(\cos(\mathbf{g},\mathbf{n}_{j}))/\tau)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathbf{r}" class="ltx_Math" display="inline" id="S5.SS1.p4.m1" intent=":literal"><semantics><mi>𝐫</mi><annotation encoding="application/x-tex">\mathbf{r}</annotation></semantics></math> is the embedding of a reference image of the same identity as the generated image, <math alttext="\mathbf{n}_{j}" class="ltx_Math" display="inline" id="S5.SS1.p4.m2" intent=":literal"><semantics><msub><mi>𝐧</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\mathbf{n}_{j}</annotation></semantics></math> are embeddings of <math alttext="M" class="ltx_Math" display="inline" id="S5.SS1.p4.m3" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> negatives from different identities, and <math alttext="\tau" class="ltx_Math" display="inline" id="S5.SS1.p4.m4" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math> is a temperature hyperparameter. This formulation relies on ID-labeled datasets, which make it possible to draw thousands of negatives per sample from the reference bank, thereby greatly enriching the diversity of negative examples.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p">The overall training objective is a weighted sum of the above losses:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\mathcal{L}_{\mathrm{diff}}+\lambda_{\text{ID}}\mathcal{L}_{\text{ID}}+\lambda_{\text{CL}}\mathcal{L}_{\text{CL}}," class="ltx_Math" display="block" id="S5.E8.m1" intent=":literal"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>diff</mi></msub><mo>+</mo><mrow><msub><mi>λ</mi><mtext>ID</mtext></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>ID</mtext></msub></mrow><mo>+</mo><mrow><msub><mi>λ</mi><mtext>CL</mtext></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>CL</mtext></msub></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}=\mathcal{L}_{\mathrm{diff}}+\lambda_{\text{ID}}\mathcal{L}_{\text{ID}}+\lambda_{\text{CL}}\mathcal{L}_{\text{CL}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\lambda_{\text{ID}}" class="ltx_Math" display="inline" id="S5.SS1.p5.m1" intent=":literal"><semantics><msub><mi>λ</mi><mtext>ID</mtext></msub><annotation encoding="application/x-tex">\lambda_{\text{ID}}</annotation></semantics></math> and <math alttext="\lambda_{\text{CL}}" class="ltx_Math" display="inline" id="S5.SS1.p5.m2" intent=":literal"><semantics><msub><mi>λ</mi><mtext>CL</mtext></msub><annotation encoding="application/x-tex">\lambda_{\text{CL}}</annotation></semantics></math> are hyper-parameters controlling the contributions of the ID loss and contrastive loss, respectively. Both are set to <math alttext="0.1" class="ltx_Math" display="inline" id="S5.SS1.p5.m3" intent=":literal"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math> across all training phases described below.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="386" id="S5.F5.sf1.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text" style="font-size:90%;">Single-ID subset</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="386" id="S5.F5.sf2.g1" src="x14.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text" style="font-size:90%;">Multi-ID subset</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Trade-off between Face Similarity and Copy-paste.<span class="ltx_text ltx_font_medium"> Except for WithAnyone, the other models fall roughly on a fitted curve, illustrating a clear trade-off between face similarity and copy-paste. Upper-right corner is desired.</span></span></figcaption>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Quantitative comparison on the multi-person subset of MultiID-Bench<span class="ltx_text ltx_font_medium">. <span class="ltx_rule" style="width:7.7pt;height:7.7pt;--ltx-bg-color:black;display:inline-block;"> </span>, <span class="ltx_rule" style="width:7.7pt;height:7.7pt;--ltx-bg-color:black;display:inline-block;"> </span>, and <span class="ltx_rule" style="width:7.7pt;height:7.7pt;--ltx-bg-color:black;display:inline-block;"> </span> indicate the first-, second-, and third-best performance, respectively. For Copy-Paste ranking, only cases with <math alttext="\mathrm{Sim(GT)}&gt;0.35" class="ltx_Math" display="inline" id="S5.T2.m2" intent=":literal"><semantics><mrow><mrow><mi>Sim</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>GT</mi><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mn>0.35</mn></mrow><annotation encoding="application/x-tex">\mathrm{Sim(GT)}&gt;0.35</annotation></semantics></math> are considered. GPT exhibits prior knowledge of identities from TV series in subsets with more than two IDs, leading to abnormally high similarity scores.</span></span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S5.T2.st1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">2-people Subset</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:246.0pt;height:80.4pt;vertical-align:-38.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-82.0pt,26.8pt) scale(0.6,0.6) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span class="ltx_text ltx_font_bold">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4"><span class="ltx_text ltx_font_bold">Identity Metrics</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Generation Quality</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t">Sim(GT) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.st1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">Sim(Ref) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.st1.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">CP <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.st1.m3" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Bld <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.st1.m4" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">CLIP-I <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.st1.m5" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">CLIP-T <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.st1.m6" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">Aes <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.st1.m7" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">DreamO</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.359</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.514</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.179</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.105</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.763</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.319</td>
<td class="ltx_td ltx_align_center ltx_border_t">4.764</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OmniGen</th>
<td class="ltx_td ltx_align_center">0.345</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.529</span></td>
<td class="ltx_td ltx_align_center">0.209</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.110</td>
<td class="ltx_td ltx_align_center">0.750</td>
<td class="ltx_td ltx_align_center">0.326</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">5.152</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OmniGen2</th>
<td class="ltx_td ltx_align_center">0.283</td>
<td class="ltx_td ltx_align_center">0.353</td>
<td class="ltx_td ltx_align_center">0.081</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.112</td>
<td class="ltx_td ltx_align_center">0.763</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.334</span></td>
<td class="ltx_td ltx_align_center">4.547</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GPT</th>
<td class="ltx_td ltx_align_center">0.332</td>
<td class="ltx_td ltx_align_center">0.400</td>
<td class="ltx_td ltx_align_center">0.061</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.092</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.774</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.328</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">5.676</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UNO</th>
<td class="ltx_td ltx_align_center">0.223</td>
<td class="ltx_td ltx_align_center">0.274</td>
<td class="ltx_td ltx_align_center">0.043</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.082</span></td>
<td class="ltx_td ltx_align_center">0.735</td>
<td class="ltx_td ltx_align_center">0.325</td>
<td class="ltx_td ltx_align_center">4.805</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">UMO</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.328</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.491</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.176</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.111</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.743</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.316</td>
<td class="ltx_td ltx_align_center ltx_border_t">4.772</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UniPortrait</th>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.367</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.601</span></td>
<td class="ltx_td ltx_align_center">0.254</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.075</span></td>
<td class="ltx_td ltx_align_center">0.750</td>
<td class="ltx_td ltx_align_center">0.323</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">5.187</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ID-Patch</th>
<td class="ltx_td ltx_align_center">0.350</td>
<td class="ltx_td ltx_align_center">0.517</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.183</span></td>
<td class="ltx_td ltx_align_center ltx_border_r">0.085</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.767</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.326</span></td>
<td class="ltx_td ltx_align_center">4.671</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.405</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.551</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.161</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.079</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.770</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.321</td>
<td class="ltx_td ltx_align_center ltx_border_bb">4.883</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S5.T2.st2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">3-and-4-people Subset</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:246.0pt;height:80.4pt;vertical-align:-38.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-82.0pt,26.8pt) scale(0.6,0.6) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span class="ltx_text ltx_font_bold">Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4"><span class="ltx_text ltx_font_bold">Identity Metrics</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Generation Quality</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t">Sim(GT) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.st2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">Sim(Ref) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.st2.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">CP <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.st2.m3" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Bld <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.st2.m4" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">CLIP-I <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.st2.m5" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">CLIP-T <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.st2.m6" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">Aes <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.st2.m7" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">DreamO</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.311</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.427</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.116</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.081</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.709</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.317</td>
<td class="ltx_td ltx_align_center ltx_border_t">4.695</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OmniGen</th>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.345</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.529</span></td>
<td class="ltx_td ltx_align_center">0.209</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.110</td>
<td class="ltx_td ltx_align_center">0.750</td>
<td class="ltx_td ltx_align_center">0.326</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">5.152</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">OmniGen2</th>
<td class="ltx_td ltx_align_center">0.288</td>
<td class="ltx_td ltx_align_center">0.374</td>
<td class="ltx_td ltx_align_center">0.099</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.071</td>
<td class="ltx_td ltx_align_center">0.734</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.329</span></td>
<td class="ltx_td ltx_align_center">4.664</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GPT</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_italic">0.445</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_italic">0.484</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_italic">0.048</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_italic">0.044</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.815</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.320</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">5.647</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UNO</th>
<td class="ltx_td ltx_align_center">0.228</td>
<td class="ltx_td ltx_align_center">0.276</td>
<td class="ltx_td ltx_align_center">0.046</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.065</td>
<td class="ltx_td ltx_align_center">0.717</td>
<td class="ltx_td ltx_align_center">0.319</td>
<td class="ltx_td ltx_align_center">4.880</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">UMO</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.318</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.465</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.180</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.070</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.717</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.309</td>
<td class="ltx_td ltx_align_center ltx_border_t">4.946</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UniPortrait</th>
<td class="ltx_td ltx_align_center">0.343</td>
<td class="ltx_td ltx_align_center">0.517</td>
<td class="ltx_td ltx_align_center">0.178</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.048</span></td>
<td class="ltx_td ltx_align_center">0.708</td>
<td class="ltx_td ltx_align_center">0.323</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">5.090</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ID-Patch</th>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.379</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.543</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.195</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.059</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.781</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.329</span></td>
<td class="ltx_td ltx_align_center">4.547</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.414</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.561</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.171</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.045</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.771</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.325</td>
<td class="ltx_td ltx_align_center ltx_border_bb">4.955</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Training pipeline</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p">Copy–paste artifacts largely arise from reconstruction-only training, which encourages models to replicate the reference image rather than learn robust identity-conditioned generation. Leveraging our paired dataset, we employ a four-phase training pipeline that gradually transitions the objective from reconstruction toward controllable, identity-preserving synthesis.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Phase 1: Reconstruction pre-training with fixed prompt.</span> We begin with reconstruction pre-training to initialize the backbone, as this task is simpler than full identity-conditioned generation and can exploit large-scale unlabeled data. For the first few thousand steps, the caption is fixed to a constant dummy prompt (e.g., “two people”), ensuring the model prioritizes learning the identity-conditioning pathway rather than drifting toward text-conditioned styling. The full MultiID-2M is used in this phase, which typically lasts for <math alttext="20\text{k}" class="ltx_Math" display="inline" id="S5.SS2.p2.m1" intent=":literal"><semantics><mrow><mn>20</mn><mo lspace="0em" rspace="0em">​</mo><mtext>k</mtext></mrow><annotation encoding="application/x-tex">20\text{k}</annotation></semantics></math> steps, at which point the model achieves satisfactory identity similarity. To further enhance data diversity, CelebA-HQ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib23" title="">23</a>]</cite>, FFHQ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib24" title="">24</a>]</cite>, and a subset of FaceID-6M <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib51" title="">51</a>]</cite> are also incorporated.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Phase 2: Reconstruction pre-training with full captions.</span> This phase aligns identity learning with text-conditioned generation and lasts for an additional <math alttext="40\text{k}" class="ltx_Math" display="inline" id="S5.SS2.p3.m1" intent=":literal"><semantics><mrow><mn>40</mn><mo lspace="0em" rspace="0em">​</mo><mtext>k</mtext></mrow><annotation encoding="application/x-tex">40\text{k}</annotation></semantics></math> steps, during which the model reaches peak identity similarity.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Phase 3: Paired tuning.</span> To suppress trivial copy–paste behavior, we replace <math alttext="50\%" class="ltx_Math" display="inline" id="S5.SS2.p4.m1" intent=":literal"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">50\%</annotation></semantics></math> of the training samples with paired instances drawn from the <math alttext="500\text{k}" class="ltx_Math" display="inline" id="S5.SS2.p4.m2" intent=":literal"><semantics><mrow><mn>500</mn><mo lspace="0em" rspace="0em">​</mo><mtext>k</mtext></mrow><annotation encoding="application/x-tex">500\text{k}</annotation></semantics></math> labeled images in MultiID-2M. For each paired sample, instead of using the same image as both input and target, we randomly select one reference image from the identity’s reference set and another distinct image of the same identity as the target. This perturbation breaks the shortcut of direct duplication and compels the model to rely on high-level identity embeddings rather than low-level copying.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Phase 4: Quality tuning.</span> Finally, we fine-tune on a curated high-quality subset augmented with generated stylized variants to (i) enhance perceptual fidelity and (ii) improve style robustness and transferability. This phase refines texture, lighting, and stylistic adaptability while preserving the strong identity consistency established in earlier phases.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>
<figure class="ltx_figure" id="S6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1188" id="S6.F6.g1" src="x15.png" width="932"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Qualitative Results of Different Generation Methods.<span class="ltx_text ltx_font_medium"> The text prompt is extracted from the ground-truth image shown on the leftmost side.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p">In this section, we present a comprehensive evaluation of baselines and our WithAnyone model on the proposed MultiID-Bench.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Baselines.</span>
We evaluate two categories of baseline methods: general customization models and face customization methods. The general customization models include OmniGen <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib61" title="">61</a>]</cite>, OmniGen2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib54" title="">54</a>]</cite>, Qwen-Image-Edit <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib53" title="">53</a>]</cite>, FLUX.1 Kontext <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib2" title="">2</a>]</cite>, UNO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib56" title="">56</a>]</cite>, USO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib55" title="">55</a>]</cite>, UMO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib8" title="">8</a>]</cite>, and native GPT-4o-Image <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib32" title="">32</a>]</cite>. The face customization methods include UniPortrait <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib15" title="">15</a>]</cite>, ID-Patch <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib68" title="">68</a>]</cite>, PuLID <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib14" title="">14</a>]</cite> (referring to its FLUX <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib27" title="">27</a>]</cite> implementation throughout this paper), and InstantID <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib50" title="">50</a>]</cite>. All models were evaluated on the single-person subset of the benchmark, while only those supporting multi-ID generation were additionally tested on the multi-person subset.
Further implementation details are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A6.SS1" title="F.1 Implementation Details ‣ Appendix F Experimental Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">F.1</span></a>.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Quantitative Evaluation</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p">The quantitative results are reported in Tables <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S5.T1.st2" title="Table 1(b) ‣ Table 1 ‣ 5.1 Training Objectives ‣ 5 WithAnyone: Controllable and ID-Consistent Generation ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">1(b)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S5.T2.st2" title="Table 2(b) ‣ Table 2 ‣ 5.1 Training Objectives ‣ 5 WithAnyone: Controllable and ID-Consistent Generation ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">2(b)</span></a>. We observe a clear trade-off between face similarity and copy-paste artifacts. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S5.F5" title="Figure 5 ‣ 5.1 Training Objectives ‣ 5 WithAnyone: Controllable and ID-Consistent Generation ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">5</span></a>, most methods align closely with a regression curve, where higher face similarity generally coincides with stronger copy-paste. This indicates that many existing models boost measured similarity by directly replicating reference facial features rather than synthesizing the identity. In contrast, WithAnyone deviates substantially from this curve, achieving the highest face similarity with regard to GT while maintaining a markedly lower copy-paste score.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p">WithAnyone also achieves the highest score among ID-specific reference models on the OmniContext <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib54" title="">54</a>]</cite> benchmark. However, VLMs <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib32" title="">32</a>]</cite> exhibit limited ability to distinguish individual identities and instead emphasize non-identity attributes such as pose, expression, or background. Despite that general customization and editing models often outperform face customization models on OmniContext, WithAnyone still has best performance among face customization models.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Qualitative Comparison</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p">To complement the quantitative results, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.F6" title="Figure 6 ‣ 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">6</span></a> presents qualitative comparisons between our method, state-of-the-art general customization/editing models, and face customization generation models.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p">It shows that identity consistency remains a significant weakness of general customization or editing models, consistent with our quantitative findings. Many VAE-based approaches where references are encoded through a VAE, such as FLUX.1 Kontext and DreamO tend to produce faces that either exhibit copy-paste artifacts or deviate markedly from the target identity. A likely reason is that VAE embeddings emphasize low-level features, leaving high-level semantic understanding to the diffusion backbone, which may not have been pre-trained for this task.
ID-specific reference models also struggle with copy-paste artifacts. For example, they fail to make the subject smile when the reference image is neutral and often cannot adjust head pose or even eye gaze. In contrast, WithAnyone generates flexible, controllable faces while faithfully preserving identity.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Ablation and User Studies</h3>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation Study<span class="ltx_text ltx_font_medium">. <span class="ltx_rule" style="width:7.7pt;height:7.7pt;--ltx-bg-color:black;display:inline-block;"> </span> <span class="ltx_rule" style="width:7.7pt;height:7.7pt;--ltx-bg-color:black;display:inline-block;"> </span> <span class="ltx_rule" style="width:7.7pt;height:7.7pt;--ltx-bg-color:black;display:inline-block;"> </span> indicate the first, second, third performance respectively. We ablate paired data training (without stage 2, w/o s2), GT-Aligned landmark ID loss (Self-aligned, S.A.), extended negative samples in InfoNCE (w/o neg). And model trained on FFHQ is also compared.</span></span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:245.9pt;height:52pt;vertical-align:-24.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-82.0pt,17.4pt) scale(0.6,0.6) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span class="ltx_text ltx_font_bold">Ablation</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Identity Metrics</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Generation Quality</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t">Sim(G) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">Sim(R) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CP <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T3.m3" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">CLIP-I <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.m4" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">CLIP-T <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.m5" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">Aes <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.m6" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Phases</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">w/o Phase 3</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.406</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.625</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.239</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.755</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.307</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">4.955</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2">Loss</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">w/o GT-Align</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.385</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.549</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.175</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.763</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFF5F0;"><span class="ltx_text" style="--ltx-bg-color:#FFF5F0;">0.317</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">4.754</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">w/o Ext. Neg.</th>
<td class="ltx_td ltx_align_center">0.368</td>
<td class="ltx_td ltx_align_center">0.455</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.074</span></td>
<td class="ltx_td ltx_align_center">0.740</td>
<td class="ltx_td ltx_align_center">0.304</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">4.984</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Data</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">FFHQ only</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.224</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.246</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.027</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.658</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.330</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">5.039</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Ours</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Full Setting</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.405</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.551</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.161</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="--ltx-bg-color:#FCBBA1;"><span class="ltx_text" style="--ltx-bg-color:#FCBBA1;">0.770</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="--ltx-bg-color:#FEE0D2;"><span class="ltx_text" style="--ltx-bg-color:#FEE0D2;">0.321</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">4.883</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S6.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="182" id="S6.F7.g1" src="x16.png" width="419"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="180" id="S6.F7.g2" src="x17.png" width="361"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison of GT-aligned and Prediction-aligned landmarks.<span class="ltx_text ltx_font_medium"> </span></span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p">To better understand the contribution of each component in WithAnyone, we conduct ablation studies on the training strategy, the GT-aligned ID loss, the InfoNCE-based ID loss, and our dataset. Due to space constraints, we report the key results here, with additional analyses provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A7" title="Appendix G Ablation Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.T3" title="Table 3 ‣ 6.3 Ablation and User Studies ‣ 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">3</span></a>, the paired-data fine-tuning phase reduces copy-paste artifacts without diminishing similarity to the ground truth, while training on FFHQ performs significantly worse than on our curated dataset. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.F7" title="Figure 7 ‣ 6.3 Ablation and User Studies ‣ 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">7</span></a> further demonstrates that the GT-aligned ID loss lowers denoising error at low noise levels and yields higher-variance, more informative gradients at high noise, thereby strengthening identity learning. By ablating extended negatives, leaving only <math alttext="63" class="ltx_Math" display="inline" id="S6.SS3.p2.m1" intent=":literal"><semantics><mn>63</mn><annotation encoding="application/x-tex">63</annotation></semantics></math> negative samples from the batch (originally extended to <math alttext="4096" class="ltx_Math" display="inline" id="S6.SS3.p2.m2" intent=":literal"><semantics><mn>4096</mn><annotation encoding="application/x-tex">4096</annotation></semantics></math>), the effectiveness of ID contrastive loss is greatly reduced. More ablation results can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A7" title="Appendix G Ablation Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p">We conduct a user study to evaluate perceptual quality and identity preservation. Ten participants were recruited and asked to rank 230 groups of generated images according to four criteria: identity similarity, presence of copy-paste artifacts, prompt adherence, and aesthetics. The results, shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.F8" title="Figure 8 ‣ 6.3 Ablation and User Studies ‣ 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">8</span></a>, indicate that our method consistently achieves the highest average ranking across all dimensions, demonstrating both stronger identity preservation and superior visual quality. Moreover, the copy-paste metric exhibits a moderate positive correlation with human judgments, suggesting that it captures perceptually meaningful artifacts. Further details of the study design, ranking protocol, and statistical analysis are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A8" title="Appendix H User Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">H</span></a>.</p>
</div>
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="579" id="S6.SS3.g1" src="x18.png" width="830"/>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">User study.<span class="ltx_text ltx_font_medium"> Bigger bubbles indicate higher ranking.</span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p">Copy-paste artifacts are a common limitation of identity customization methods, and face-similarity metrics often exacerbate the issue by implicitly rewarding direct copying. In this work, we identify and formally quantify this failure mode through MultiID-Bench, and propose targeted solutions. We curate MultiID-2M and develop training strategies and loss functions that explicitly discourage trivial replication. Empirical evaluations demonstrate that WithAnyone significantly reduces copy-paste artifacts while maintaining and in many cases improving identity similarity, thereby breaking the long-standing trade-off between fidelity and copying. These results highlight a practical path toward more faithful, controllable, and robust identity customization.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin.

</span>
<span class="ltx_bibblock">Qwen2.5-vl technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2502.13923</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al.

</span>
<span class="ltx_bibblock">Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv e-prints</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, and Shanghang Zhang.

</span>
<span class="ltx_bibblock">Training-free regional prompting for diffusion transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2411.02395</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bowen Chen, Mengyi Zhao, Haomiao Sun, Li Chen, Xu Wang, Kang Du, and Xinglong Wu.

</span>
<span class="ltx_bibblock">Xverse: Consistent multi-subject control of identity and semantic attributes via dit modulation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2506.21416</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Weifeng Chen, Jiacheng Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, and Liang Lin.

</span>
<span class="ltx_bibblock">Id-aligner: Enhancing identity-preserving text-to-image generation with reward feedback learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2404.15449</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Wei Cheng, Ruixiang Chen, Siming Fan, Wanqi Yin, Keyu Chen, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, et al.

</span>
<span class="ltx_bibblock">Dna-rendering: A diverse neural actor repository for high-fidelity human-centric rendering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Wei Cheng, Su Xu, Jingtan Piao, Chen Qian, Wayne Wu, Kwan-Yee Lin, and Hongsheng Li.

</span>
<span class="ltx_bibblock">Generalizable neural performer: Learning robust radiance fields for human novel view synthesis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2204.11798</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, and Qian He.

</span>
<span class="ltx_bibblock">Umo: Scaling multi-identity consistency for image customization via matching reward.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2509.06818</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jiaming Chu, Lei Jin, Yinglei Teng, Jianshu Li, Yunchao Wei, Zheng Wang, Junliang Xing, Shuicheng Yan, and Jian Zhao.

</span>
<span class="ltx_bibblock">Uniparser: Multi-human parsing with unified correlation representation learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">TIP</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou.

</span>
<span class="ltx_bibblock">Retinaface: Single-shot multi-level face localisation in the wild.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">CVPR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.

</span>
<span class="ltx_bibblock">Arcface: Additive angular margin loss for deep face recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">CVPR</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
discus0434.

</span>
<span class="ltx_bibblock">aesthetic-predictor-v2-5.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/discus0434/aesthetic-predictor-v2-5" title="">https://github.com/discus0434/aesthetic-predictor-v2-5</a>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2025-05-12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al.

</span>
<span class="ltx_bibblock">Scaling rectified flow transformers for high-resolution image synthesis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">ICML</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, Peng Zhang, and Qian He.

</span>
<span class="ltx_bibblock">Pulid: Pure and lightning id customization via contrastive alignment.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">NeurIPS</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Junjie He, Yifeng Geng, and Liefeng Bo.

</span>
<span class="ltx_bibblock">Uniportrait: A unified framework for identity-preserving single-and multi-human image personalization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">ICCV</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">Prompt-to-prompt image editing with cross attention control.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">ICLR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">NeurIPS</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Xirui Hu, Jiahao Wang, Hao Chen, Weizhan Zhang, Benqi Wang, Yikun Li, and Haishun Nan.

</span>
<span class="ltx_bibblock">Dynamicid: Zero-shot multi-id image personalization with flexible facial editability.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">ICCV</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Yuqi Hu, Longguang Wang, Xian Liu, Ling-Hao Chen, Yuwei Guo, Yukai Shi, Ce Liu, Anyi Rao, Zeyu Wang, and Hui Xiong.

</span>
<span class="ltx_bibblock">Simulating the real world: A unified survey of multimodal generative models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2503.04641</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Junha Hyung, Jaeyo Shin, and Jaegul Choo.

</span>
<span class="ltx_bibblock">Magicapture: High-resolution multi-concept portrait customization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">AAAI</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, and Xin Lu.

</span>
<span class="ltx_bibblock">Infiniteyou: Flexible photo recrafting while preserving your identity.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">ICCV</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Qing Jiang, Lin Wu, Zhaoyang Zeng, Tianhe Ren, Yuda Xiong, Yihao Chen, Qin Liu, and Lei Zhang.

</span>
<span class="ltx_bibblock">Referring to any person.

</span>
<span class="ltx_bibblock">2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.

</span>
<span class="ltx_bibblock">Progressive growing of gans for improved quality, stability, and variation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">ICLR</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Tero Karras, Samuli Laine, and Timo Aila.

</span>
<span class="ltx_bibblock">A style-based generator architecture for generative adversarial networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">CVPR</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Chanran Kim, Jeongin Lee, Shichang Joung, Bongmo Kim, and Yeul-Min Baek.

</span>
<span class="ltx_bibblock">Instantfamily: Masked attention for zero-shot multi-id image generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2404.19427</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Minchul Kim, Anil K Jain, and Xiaoming Liu.

</span>
<span class="ltx_bibblock">Adaface: Quality adaptive margin for face recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Black Forest Labs.

</span>
<span class="ltx_bibblock">Flux.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/black-forest-labs/flux" title="">https://github.com/black-forest-labs/flux</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Black Forest Labs.

</span>
<span class="ltx_bibblock">Flux.1 krea.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev" title="">https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev</a>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.

</span>
<span class="ltx_bibblock">Deep learning face attributes in the wild.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">ICCV</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al.

</span>
<span class="ltx_bibblock">Dreamo: A unified framework for image customization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">SIGGRAPH Asia</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Aaron van den Oord, Yazhe Li, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">Representation learning with contrastive predictive coding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:1807.03748</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Addendum to gpt-4o system card: Native image generation, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.

</span>
<span class="ltx_bibblock">Dinov2: Learning robust visual features without supervision.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv:2304.07193</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Dongwei Pan, Long Zhuo, Jingtan Piao, Huiwen Luo, Wei Cheng, Yuxin Wang, Siming Fan, Shengqi Liu, Lei Yang, Bo Dai, et al.

</span>
<span class="ltx_bibblock">Renderme-360: A large digital asset library and benchmarks towards high-fidelity head avatars.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">NeurIPS</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos Moschoglou, Jiankang Deng, Bernhard Kainz, and Stefanos Zafeiriou.

</span>
<span class="ltx_bibblock">Arc2face: A foundation model for id-consistent human faces.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">ECCV</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu, Daniel Cohen-Or, and Kfir Aberman.

</span>
<span class="ltx_bibblock">Object-level visual prompts for compositional image generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2501.01424</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Or Patashnik, Rinon Gal, Daniil Ostashev, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">Nested attention: Semantic-aware attention values for concept personalization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">SIGGRAPH</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
William Peebles and Saining Xie.

</span>
<span class="ltx_bibblock">Scalable diffusion models with transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Xu Peng, Junwei Zhu, Boyuan Jiang, Ying Tai, Donghao Luo, Jiangning Zhang, Wei Lin, Taisong Jin, Chengjie Wang, and Rongrong Ji.

</span>
<span class="ltx_bibblock">Portraitbooth: A versatile portrait model for fast identity-preserved personalization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Guocheng Qian, Kuan-Chieh Wang, Or Patashnik, Negin Heravi, Daniil Ostashev, Sergey Tulyakov, Daniel Cohen-Or, and Kfir Aberman.

</span>
<span class="ltx_bibblock">Omni-id: Holistic identity representation designed for generative tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">CVPR</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">ICML</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Xingyu Ren, Alexandros Lattas, Baris Gecer, Jiankang Deng, Chao Ma, and Xiaokang Yang.

</span>
<span class="ltx_bibblock">Facial geometric detail recovery via implicit representation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">FG</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">MICCAI</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.

</span>
<span class="ltx_bibblock">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">CVPR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Florian Schroff, Dmitry Kalenichenko, and James Philbin.

</span>
<span class="ltx_bibblock">Facenet: A unified embedding for face recognition and clustering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">CVPR</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Erich Schubert, Jörg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu.

</span>
<span class="ltx_bibblock">Dbscan revisited, revisited: why and how you should (still) use dbscan.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">TODS</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Lorenzo Stacchio, Alessia Angeli, Giuseppe Lisanti, Daniela Calanca, and Gustavo Marfia.

</span>
<span class="ltx_bibblock">Imago: A family photo album dataset for a socio-historical analysis of the twentieth century.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.01955</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan.

</span>
<span class="ltx_bibblock">Face0: Instantaneously conditioning a text-to-image model on a face.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">SIGGRAPH Asia</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Qinghe Wang, Xu Jia, Xiaomin Li, Taiqing Li, Liqian Ma, Yunzhi Zhuge, and Huchuan Lu.

</span>
<span class="ltx_bibblock">Stableidentity: Inserting anybody into anywhere at first sight.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">TMM</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen.

</span>
<span class="ltx_bibblock">Instantid: Zero-shot identity-preserving generation in seconds.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2401.07519</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Shuhe Wang, Xiaoya Li, Jiwei Li, Guoyin Wang, Xiaofei Sun, Bob Zhu, Han Qiu, Mo Yu, Shengjie Shen, Tianwei Zhang, et al.

</span>
<span class="ltx_bibblock">Faceid-6m: A large-scale, open-source faceid customization dataset.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2503.07091</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Yibin Wang, Weizhong Zhang, Jianwei Zheng, and Cheng Jin.

</span>
<span class="ltx_bibblock">High-fidelity person-centric subject-to-image synthesis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu.

</span>
<span class="ltx_bibblock">Qwen-image technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2508.02324</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu.

</span>
<span class="ltx_bibblock">Omnigen2: Exploration to advanced multimodal generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2506.18871</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Shaojin Wu, Mengqi Huang, Yufeng Cheng, Wenxu Wu, Jiahe Tian, Yiming Luo, Fei Ding, and Qian He.

</span>
<span class="ltx_bibblock">Uso: Unified style and subject-driven generation via disentangled and reward learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2508.18966</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He.

</span>
<span class="ltx_bibblock">Less-to-more generalization: Unlocking more controllability by in-context generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">ICCV</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Tong Wu, Yinghao Xu, Ryan Po, Mengchen Zhang, Guandao Yang, Jiaqi Wang, Ziwei Liu, Dahua Lin, and Gordon Wetzstein.

</span>
<span class="ltx_bibblock">Fiva: Fine-grained visual attribute dataset for text-to-image diffusion models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">NeurIPS</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Yi Wu, Ziqiang Li, Heliang Zheng, Chaoyue Wang, and Bin Li.

</span>
<span class="ltx_bibblock">Infinite-id: Identity-preserved personalization via id-semantics decoupling paradigm.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">ECCV</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Chufeng Xiao and Hongbo Fu.

</span>
<span class="ltx_bibblock">Customsketching: Sketch concept extraction for sketch-based image synthesis and editing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Computer Graphics Forum</span>. Wiley Online Library, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Guangxuan Xiao, Tianwei Yin, William T Freeman, Frédo Durand, and Song Han.

</span>
<span class="ltx_bibblock">Fastcomposer: Tuning-free multi-subject image generation with localized attention.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">IJCV</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu.

</span>
<span class="ltx_bibblock">Omnigen: Unified image generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2409.11340</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Hengyuan Xu, Liyao Xiang, Hangyu Ye, Dixi Yao, Pengzhi Chu, and Baochun Li.

</span>
<span class="ltx_bibblock">Permutation equivariance of transformers and its applications.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">CVPR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Yuxuan Yan, Chi Zhang, Rui Wang, Yichao Zhou, Gege Zhang, Pei Cheng, Gang Yu, and Bin Fu.

</span>
<span class="ltx_bibblock">Facestudio: Put your face everywhere in seconds.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2312.02663</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang.

</span>
<span class="ltx_bibblock">Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arxiv:2308.06721</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.

</span>
<span class="ltx_bibblock">Sigmoid loss for language image pre-training.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image diffusion models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">ICCV</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Ning Zhang, Manohar Paluri, Yaniv Taigman, Rob Fergus, and Lubomir Bourdev.

</span>
<span class="ltx_bibblock">Beyond frontal faces: Improving person recognition using multiple cues.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">CVPR</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Yimeng Zhang, Tiancheng Zhi, Jing Liu, Shen Sang, Liming Jiang, Qing Yan, Sijia Liu, and Linjie Luo.

</span>
<span class="ltx_bibblock">Id-patch: Robust id association for group photo personalization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">CVPR</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al.

</span>
<span class="ltx_bibblock">Recognize anything: A strong image tagging model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.03514</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Yujie Zhong, Relja Arandjelovic, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Compact deep aggregation for set retrieval.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">ECCV</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Cailin Zhuang, Ailin Huang, Wei Cheng, Jingwei Wu, Yaoqi Hu, Jiaqi Liao, Hongyuan Wang, Xinyao Liao, Weiwei Cai, Hengyuan Xu, et al.

</span>
<span class="ltx_bibblock">Vistorybench: Comprehensive benchmark suite for story visualization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">arXiv preprint arXiv:2505.24862</span>, 2025.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold" style="font-size:173%;">Appendix</span></p>
</div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Family of WithAnyone</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p">FLUX.1 comprises a family of models, including FLUX.1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib27" title="">27</a>]</cite>, FLUX.1 Kontext <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib2" title="">2</a>]</cite> and FLUX.1 Krea <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib28" title="">28</a>]</cite>. Krea is a text-to-image model with improved real-person face generation, whereas Kontext is an image-editing model that excels at making targeted adjustments while preserving the rest of the image. However, as reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S5.T1.st2" title="Table 1(b) ‣ Table 1 ‣ 5.1 Training Objectives ‣ 5 WithAnyone: Controllable and ID-Consistent Generation ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">1(b)</span></a>, Kontext shows limited consistency with the reference face identity.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p">Our method, WithAnyone, can be seamlessly integrated into Kontext for the face customization downstream tasks like face editing. As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A1.F9" title="Figure 9 ‣ Appendix A Family of WithAnyone ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">9</span></a>, WithAnyone effectively injects identity information from the reference images into the target image.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p">The overall training pipeline follows the procedure described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S5" title="5 WithAnyone: Controllable and ID-Consistent Generation ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">5</span></a>, with a single modification: the input image provided to Kontext (whose tokens are concatenated with the noisy latent at each denoising step) is set to the target image with the face region blurred.</p>
</div>
<figure class="ltx_figure" id="A1.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="175" id="A1.F9.g1" src="x19.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Application of WithAnyone-Kontext.<span class="ltx_text ltx_font_medium"> Marrying editing models, WithAnyone is capable of face editing given customization references.</span></span></figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>MultiID-2M Construction Details</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p">To fill in the void left by the lack of publicly available multi-ID datasets, a data constraction pipeline is proposed to create a large-scale dataset of multi-person images with paired identity references for identities on the data record. Based on this pipeline, <math alttext="500" class="ltx_Math" display="inline" id="A2.p1.m1" intent=":literal"><semantics><mn>500</mn><annotation encoding="application/x-tex">500</annotation></semantics></math>k group photo images are collected, featuring <math alttext="3" class="ltx_Math" display="inline" id="A2.p1.m2" intent=":literal"><semantics><mn>3</mn><annotation encoding="application/x-tex">3</annotation></semantics></math>k identities, each with hundreds of single-ID reference images. Another <math alttext="1" class="ltx_Math" display="inline" id="A2.p1.m3" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>M images that cannot be identified are also included in the dataset for image reconstruction training purpose for image reconstruction training purpose.</p>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Dataset Construction Pipeline</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p">The pipeline contains four steps, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S2.F3" title="Figure 3 ‣ 2 Related Work ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">3</span></a>. The detailed pipeline are as follows.</p>
</div>
<figure class="ltx_figure" id="A2.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F10.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="620" id="A2.F10.sf1.g1" src="x20.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">ID Appearance.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F10.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="547" id="A2.F10.sf2.g1" src="x21.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Nationality Distribution.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F10.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="699" id="A2.F10.sf3.g1" src="x22.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(c)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Benchmark Distribution.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Overview of Dataset Distributions.<span class="ltx_text ltx_font_medium"> (a) ID appearance distribution for the subset of one nation: the x-axis represents celebrities, sorted by the number of images in which they appear. (b) Nationality distribution: celebrities in our dataset come from over 10 countries, with most data sourced from China and the USA. (c) Word cloud of the most frequent words in the captions.</span></span></figcaption>
</figure>
<div class="ltx_para" id="A2.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Single-ID images.</span> To construct a ID reference set, single-ID images were collected from the web using celebrity names as search queries on Google Images. For each image, facial features were extracted with ArcFace <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib42" title="">42</a>]</cite>, ensuring that only images containing exactly one face were retained. To remove outliers, DBSCAN <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib46" title="">46</a>]</cite> clustering was applied to the embeddings for each celebrity, resulting in a set of cluster centers and hundreds of reference images per identity. This process established a reliable reference set for each unique identity. Human review confirms the accuracy of the ID bank built in this step.</p>
</div>
<div class="ltx_para" id="A2.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Multi-ID images</span>. To achieve best searching efficiency, group photos were obtained using more complex queries that combined multiple celebrity names, keywords indicating the number of people (e.g., “two celebrities”), scene descriptors (e.g., “award ceremony”), and negative keywords to filter out irrelevant results. ArcFace embeddings were extracted for these images, yielding a large pool of candidate multi-ID images. At this stage, the dataset comprised more than 20 million images.</p>
</div>
<div class="ltx_para" id="A2.SS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Retrieval.</span> To provide ID reference for the multi-ID images, it is necessary to retrieve the IDs on it. All single-ID cluster centers were aggregated into an embedding matrix. For each detected face in every multi-ID image, its ArcFace embedding was compared to all single-ID cluster centers to determine identity. The similarity between two embeddings was calculated as:</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{sim}(id_{1},id_{2})=\cos(f(id_{1}),f(id_{2}))" class="ltx_Math" display="block" id="A2.E9.m1" intent=":literal"><semantics><mrow><mrow><mi>sim</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>d</mi><mn>1</mn></msub></mrow><mo>,</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>d</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>cos</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>d</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>d</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathrm{sim}(id_{1},id_{2})=\cos(f(id_{1}),f(id_{2}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="id_{1}" class="ltx_Math" display="inline" id="A2.SS1.p4.m1" intent=":literal"><semantics><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>d</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">id_{1}</annotation></semantics></math> and <math alttext="id_{2}" class="ltx_Math" display="inline" id="A2.SS1.p4.m2" intent=":literal"><semantics><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>d</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">id_{2}</annotation></semantics></math> denote two faces, and <math alttext="f" class="ltx_Math" display="inline" id="A2.SS1.p4.m3" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> is the ArcFace embedding network.</p>
</div>
<div class="ltx_para" id="A2.SS1.p5">
<p class="ltx_p">Each face in a multi-ID image was assigned the identity of the single-ID cluster center with the highest similarity, provided the similarity exceeded a predefined threshold (0.5). This approach enabled accurate and automated identity assignment in group images and facilitated retrieval of corresponding reference images.</p>
</div>
<div class="ltx_para" id="A2.SS1.p6">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Filtering and labelling.</span> To further improve dataset quality, a series of annotation and filtering steps were applied. The Recognize Anything model <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib69" title="">69</a>]</cite>, an aesthetic score predictor <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib12" title="">12</a>]</cite>, and other auxiliary tools were used for annotation. Images with low aesthetic scores or those identified as collages rather than genuine group photos were excluded. Optical Character Recognition (OCR) tools detected watermarks and logos, which were cropped out when possible; otherwise, the images were discarded. Finally, descriptive captions were generated for the images using a large language model, enriching the dataset with textual information.</p>
</div>
<div class="ltx_para" id="A2.SS1.p7">
<p class="ltx_p">So far, a dataset with three parts is obtained: (1) <math alttext="1" class="ltx_Math" display="inline" id="A2.SS1.p7.m1" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>M single-ID images as reference bank, or single-ID cross-paired training; (2) <math alttext="500" class="ltx_Math" display="inline" id="A2.SS1.p7.m2" intent=":literal"><semantics><mn>500</mn><annotation encoding="application/x-tex">500</annotation></semantics></math>k paired multi-ID images with identified persons; (3) <math alttext="1" class="ltx_Math" display="inline" id="A2.SS1.p7.m3" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>M unpaired multi-ID images, which can be used for training scenario without the need of references, such as reconstruction.</p>
</div>
<figure class="ltx_figure" id="A2.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F11.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="542" id="A2.F11.sf1.g1" src="x23.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Clothes &amp; Accessories Distribution.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F11.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="536" id="A2.F11.sf2.g1" src="x24.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Action Distribution.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Distribution of Clothes and Action Labels of Proposed Dataset.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Dataset Statistics</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p">Following prior arts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib34" title="">34</a>]</cite>, comprehensive statistics of the dataset are provided in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A2.F11" title="Figure 11 ‣ B.1 Dataset Construction Pipeline ‣ Appendix B MultiID-2M Construction Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">11</span></a>, including the distribution of nationalities, the count of appearances per identity, and a word cloud illustrating the most frequent terms in the generated image captions, offering insights into the diversity and richness of the dataset.
A long-tail distribution is observed in the count of appearances per identity in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A2.F11.sf1" title="Figure 11(a) ‣ Figure 11 ‣ B.1 Dataset Construction Pipeline ‣ Appendix B MultiID-2M Construction Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">11(a)</span></a>, with a few identities appearing frequently while many others are less common. This provide a diverse set of identities, as well as a perfect test dataset without identity interaction with the training set. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A2.F11.sf2" title="Figure 11(b) ‣ Figure 11 ‣ B.1 Dataset Construction Pipeline ‣ Appendix B MultiID-2M Construction Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">11(b)</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A2.F10.sf3" title="Figure 10(c) ‣ Figure 10 ‣ B.1 Dataset Construction Pipeline ‣ Appendix B MultiID-2M Construction Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">10(c)</span></a> illustrate MultiID-2M’s nationality distribution and action diversity respectively.
The comparison between the proposed dataset and existing multi-ID datasets are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A2.T4" title="Table 4 ‣ B.2 Dataset Statistics ‣ Appendix B MultiID-2M Construction Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">4</span></a>, highlighting MultiID-2M’s outstanding volume and paired references.</p>
</div>
<figure class="ltx_table" id="A2.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" style="font-size:129%;">Table 4</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:129%;">Statistic comparison for multi-identity group photo datasets<span class="ltx_text ltx_font_medium">. </span>#Img<span class="ltx_text ltx_font_medium"> refers to total scale of the dataset; </span>#Paired<span class="ltx_text ltx_font_medium"> refers to paired group image number; </span>#Img / ID<span class="ltx_text ltx_font_medium"> indicates number of reference image for each single ID; </span>#ID / Img<span class="ltx_text ltx_font_medium"> means number of IDs appears on group photos.</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:70%;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:70%;">#Img</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:70%;">#Paired</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:70%;">#Img / ID</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:70%;">#ID / Img</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span class="ltx_text" style="font-size:70%;">IMAGO </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib47" title="">47</a><span class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math alttext="80" class="ltx_Math" display="inline" id="A2.T4.m1" intent=":literal"><semantics><mn mathsize="0.700em">80</mn><annotation encoding="application/x-tex">80</annotation></semantics></math><span class="ltx_text" style="font-size:70%;">k</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math alttext="0" class="ltx_Math" display="inline" id="A2.T4.m2" intent=":literal"><mn mathsize="0.700em">0</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math alttext="0" class="ltx_Math" display="inline" id="A2.T4.m3" intent=":literal"><mn mathsize="0.700em">0</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">
<span class="ltx_text" style="font-size:70%;">MHP </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib9" title="">9</a><span class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r">
<math alttext="5" class="ltx_Math" display="inline" id="A2.T4.m4" intent=":literal"><semantics><mn mathsize="0.700em">5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math><span class="ltx_text" style="font-size:70%;">k</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r"><math alttext="0" class="ltx_Math" display="inline" id="A2.T4.m5" intent=":literal"><mn mathsize="0.700em">0</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math alttext="0" class="ltx_Math" display="inline" id="A2.T4.m6" intent=":literal"><mn mathsize="0.700em">0</mn></math></td>
<td class="ltx_td ltx_align_center"><math alttext="2-10" class="ltx_Math" display="inline" id="A2.T4.m7" intent=":literal"><semantics><mrow><mn mathsize="0.700em">2</mn><mo mathsize="0.700em">−</mo><mn mathsize="0.700em">10</mn></mrow><annotation encoding="application/x-tex">2-10</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">
<span class="ltx_text" style="font-size:70%;">PIPA </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib67" title="">67</a><span class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r">
<math alttext="40" class="ltx_Math" display="inline" id="A2.T4.m8" intent=":literal"><semantics><mn mathsize="0.700em">40</mn><annotation encoding="application/x-tex">40</annotation></semantics></math><span class="ltx_text" style="font-size:70%;">k</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r">
<math alttext="40" class="ltx_Math" display="inline" id="A2.T4.m9" intent=":literal"><semantics><mn mathsize="0.700em">40</mn><annotation encoding="application/x-tex">40</annotation></semantics></math><span class="ltx_text" style="font-size:70%;">k</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text" style="font-size:70%;">cross</span></td>
<td class="ltx_td ltx_align_center"><math alttext="1-10" class="ltx_Math" display="inline" id="A2.T4.m10" intent=":literal"><semantics><mrow><mn mathsize="0.700em">1</mn><mo mathsize="0.700em">−</mo><mn mathsize="0.700em">10</mn></mrow><annotation encoding="application/x-tex">1-10</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">
<span class="ltx_text" style="font-size:70%;">HumanRef </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib22" title="">22</a><span class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r">
<math alttext="36" class="ltx_Math" display="inline" id="A2.T4.m11" intent=":literal"><semantics><mn mathsize="0.700em">36</mn><annotation encoding="application/x-tex">36</annotation></semantics></math><span class="ltx_text" style="font-size:70%;">k</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r"><math alttext="36" class="ltx_Math" display="inline" id="A2.T4.m12" intent=":literal"><semantics><mn mathsize="0.700em">36</mn><annotation encoding="application/x-tex">36</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math alttext="1+" class="ltx_Math" display="inline" id="A2.T4.m13" intent=":literal"><semantics><mrow><mn mathsize="0.700em">1</mn><mo mathsize="0.700em">+</mo></mrow><annotation encoding="application/x-tex">1+</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="1-14+" class="ltx_Math" display="inline" id="A2.T4.m14" intent=":literal"><semantics><mrow><mn mathsize="0.700em">1</mn><mo mathsize="0.700em">−</mo><mrow><mn mathsize="0.700em">14</mn><mo mathsize="0.700em">+</mo></mrow></mrow><annotation encoding="application/x-tex">1-14+</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">
<span class="ltx_text" style="font-size:70%;">Celebrity Together </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib70" title="">70</a><span class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r">
<math alttext="194" class="ltx_Math" display="inline" id="A2.T4.m15" intent=":literal"><semantics><mn mathsize="0.700em">194</mn><annotation encoding="application/x-tex">194</annotation></semantics></math><span class="ltx_text" style="font-size:70%;">k</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r"><math alttext="0" class="ltx_Math" display="inline" id="A2.T4.m16" intent=":literal"><mn mathsize="0.700em">0</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math alttext="0" class="ltx_Math" display="inline" id="A2.T4.m17" intent=":literal"><mn mathsize="0.700em">0</mn></math></td>
<td class="ltx_td ltx_align_center"><math alttext="1-5" class="ltx_Math" display="inline" id="A2.T4.m18" intent=":literal"><semantics><mrow><mn mathsize="0.700em">1</mn><mo mathsize="0.700em">−</mo><mn mathsize="0.700em">5</mn></mrow><annotation encoding="application/x-tex">1-5</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:70%;">MultiID-2M</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">
<math alttext="1.5" class="ltx_Math" display="inline" id="A2.T4.m19" intent=":literal"><semantics><mn mathsize="0.700em">1.5</mn><annotation encoding="application/x-tex">1.5</annotation></semantics></math><span class="ltx_text ltx_font_bold" style="font-size:70%;">M</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">
<math alttext="500" class="ltx_Math" display="inline" id="A2.T4.m20" intent=":literal"><semantics><mn mathsize="0.700em">500</mn><annotation encoding="application/x-tex">500</annotation></semantics></math><span class="ltx_text ltx_font_bold" style="font-size:70%;">k</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><math alttext="100+" class="ltx_Math" display="inline" id="A2.T4.m21" intent=":literal"><semantics><mrow><mn mathsize="0.700em">100</mn><mo mathsize="0.700em">+</mo></mrow><annotation encoding="application/x-tex">100+</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><math alttext="1-5" class="ltx_Math" display="inline" id="A2.T4.m22" intent=":literal"><semantics><mrow><mn mathsize="0.700em">1</mn><mo mathsize="0.700em">−</mo><mn mathsize="0.700em">5</mn></mrow><annotation encoding="application/x-tex">1-5</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Benchmark and Metrics Details</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p">Most existing methods are evaluated on privately curated test sets that are seldom released, and even when datasets are shared, the accompanying evaluation protocols vary widely. For example, ID-Patch <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib68" title="">68</a>]</cite> and UniPortrait <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib15" title="">15</a>]</cite> measure identity similarity using ArcFace embeddings, whereas UNO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib56" title="">56</a>]</cite> relies on DINO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib33" title="">33</a>]</cite> and CLIP similarity scores. This heterogeneity together with the common practice of reporting only the cosine similarity between matched ArcFace embeddings fails to capture more nuanced insights and can even encourage degenerate behavior in which models produce images that are effectively “copy‑pastes” of the reference photos.</p>
</div>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p">In this work, MultiID-Bench  is introduced as a unified and extensible evaluation framework for group photo (multi-ID) generation. It standardizes assessment along two complementary axes: (i) identity fidelity (preserving each target identity without unintended copying and blending), and (ii) generation quality (semantic faithfulness to the prompt/ground truth and overall aesthetic quality).</p>
</div>
<div class="ltx_para" id="A3.p3">
<p class="ltx_p">The data used in MultiID-Bench  are drawn from the long-tail portion of MultiID-2M. We first select the least frequent identities and gather all images containing them. To prevent information leakage, the training split is filtered to ensure zero identity overlap with the benchmark set. The final benchmark contains 435 samples; each sample provides 1–4 reference identities (with their images), a corresponding ground-truth image, and a text prompt describing that ground-truth scene.</p>
</div>
<div class="ltx_para" id="A3.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Identity Blending.</span> In the similarity matrix, the off-diagonal elements correspond to the similarity between different identities. The average of the diagonal elements is used as the metric for identity fidelity, and the average of the off-diagonal elements serves as the metric for identity blending, as in Eq. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A3.E10" title="Equation 10 ‣ Appendix C Benchmark and Metrics Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<div class="ltx_para" id="A3.p5">
<table class="ltx_equation ltx_eqn_table" id="A3.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{M_{Bld}}(x^{g},x^{t})=\frac{1}{N^{2}-N}\sum_{i=1}^{N}\sum_{j=1,j\neq i}^{N}\cos(g_{i},t_{j})" class="ltx_Math" display="block" id="A3.E10.m1" intent=":literal"><semantics><mrow><mrow><msub><mi mathvariant="normal">M</mi><mi>Bld</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>g</mi></msup><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>−</mo><mi>N</mi></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false" rspace="0em">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></mrow><mi>N</mi></munderover><mrow><mi>cos</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>g</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathrm{M_{Bld}}(x^{g},x^{t})=\frac{1}{N^{2}-N}\sum_{i=1}^{N}\sum_{j=1,j\neq i}^{N}\cos(g_{i},t_{j})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="g_{i}" class="ltx_Math" display="inline" id="A3.p5.m1" intent=":literal"><semantics><msub><mi>g</mi><mi>i</mi></msub><annotation encoding="application/x-tex">g_{i}</annotation></semantics></math> is the embedding of the <math alttext="i" class="ltx_Math" display="inline" id="A3.p5.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th face in the generated image <math alttext="x^{g}" class="ltx_Math" display="inline" id="A3.p5.m3" intent=":literal"><semantics><msup><mi>x</mi><mi>g</mi></msup><annotation encoding="application/x-tex">x^{g}</annotation></semantics></math>, and <math alttext="t_{j}" class="ltx_Math" display="inline" id="A3.p5.m4" intent=":literal"><semantics><msub><mi>t</mi><mi>j</mi></msub><annotation encoding="application/x-tex">t_{j}</annotation></semantics></math> is the embedding of the <math alttext="j" class="ltx_Math" display="inline" id="A3.p5.m5" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th face in the ground-truth image <math alttext="x^{t}" class="ltx_Math" display="inline" id="A3.p5.m6" intent=":literal"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding="application/x-tex">x^{t}</annotation></semantics></math>. A lower value indicates less unintended blending between different identities, which is desirable.</p>
</div>
<div class="ltx_para" id="A3.p6">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Generation quality.</span> The overall generation quality is evaluated based on CLIP-I and CLIP-T, which are the de facto standards for evaluating the prompt-following capability <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib41" title="">41</a>]</cite>, are employed to measure the cosine similarity in the CLIP embedding space between the generated image and the ground truth image or caption. Additionally, an aesthetic score model <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib12" title="">12</a>]</cite> is used to assess the aesthetic quality of the generated images.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Galleries of WithAnyone</h2>
<figure class="ltx_figure" id="A4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1188" id="A4.F12.g1" src="x25.png" width="856"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Galleries of Single-ID Generation<span class="ltx_text ltx_font_medium">.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A4.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1190" id="A4.F13.g1" src="x26.png" width="713"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Galleries of 2-person Generation<span class="ltx_text ltx_font_medium">.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A4.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1185" id="A4.F14.g1" src="x27.png" width="713"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Galleries of 3-to-4-person Generation<span class="ltx_text ltx_font_medium">.</span></span></figcaption>
</figure>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p">We show more results of WithAnyone in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A4.F12" title="Figure 12 ‣ Appendix D Galleries of WithAnyone ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">12</span></a>, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A4.F13" title="Figure 13 ‣ Appendix D Galleries of WithAnyone ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">13</span></a>, and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A4.F14" title="Figure 14 ‣ Appendix D Galleries of WithAnyone ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Model Framework Details</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p">We follow prior work <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib14" title="">14</a>]</cite> and integrate a lightweight identity adapter into the diffusion backbone. Identity embeddings are injected by cross-attention so that the base generative prior is preserved while controllable identity signals are added.</p>
</div>
<div class="ltx_para" id="A5.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Face embedding.</span> Each reference face is first encoded by ArcFace, producing a <math alttext="1\times 512" class="ltx_Math" display="inline" id="A5.p2.m1" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">1\times 512</annotation></semantics></math> identity embedding. To match the tokenized latent space of the DiT backbone, this vector is projected with a multi-layer perceptron (MLP) into <math alttext="8" class="ltx_Math" display="inline" id="A5.p2.m2" intent=":literal"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation></semantics></math> tokens of dimension <math alttext="3072" class="ltx_Math" display="inline" id="A5.p2.m3" intent=":literal"><semantics><mn>3072</mn><annotation encoding="application/x-tex">3072</annotation></semantics></math> (i.e., an <math alttext="8\times 3072" class="ltx_Math" display="inline" id="A5.p2.m4" intent=":literal"><semantics><mrow><mn>8</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3072</mn></mrow><annotation encoding="application/x-tex">8\times 3072</annotation></semantics></math> tensor). This tokenization provides sufficient capacity for the cross-attention layers to integrate identity cues without overwhelming the generative context.</p>
</div>
<div class="ltx_para" id="A5.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Controllable attribute retention.</span> Completely suppressing copy-like behavior is not always desirable: users sometimes expect certain mid-level appearance attributes (e.g., hairstyle, accessories) to be preserved. ArcFace focuses on high-level, identity-discriminative geometry and texture cues but omits many mid-level semantic factors. To expose controllable retention of such attributes when needed, we optionally incorporate SigLIP <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib65" title="">65</a>]</cite> as a secondary encoder. SigLIP provides more semantically entangled representations, enabling selective transfer of style-relevant traits while ArcFace anchors identity fidelity.</p>
</div>
<div class="ltx_para" id="A5.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Attention mask and location control.</span> To further improve identity disentanglement and precise localization in the generated images, an attention mask and location control mechanism are incorporated <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib62" title="">62</a>]</cite>. Specifically, ground-truth facial bounding boxes are extracted from the training data and used to generate binary attention masks. These masks are applied to the attention layers of the backbone model, ensuring that each reference token only attends to its corresponding face region in the image, providing location control at the same time.</p>
</div>
<div class="ltx_para" id="A5.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Feature injection.</span> After each transformer block of the DiT backbone, we inject face features through a cross-attention modulation:</p>
<table class="ltx_equation ltx_eqn_table" id="A5.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H^{\prime}=H+\lambda_{\text{id}}\,\text{softmax}\!\left(\frac{(HW_{Q})(EW_{K})^{\top}}{\sqrt{d}}+M\right)(EW_{V})," class="ltx_Math" display="block" id="A5.E11.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>H</mi><mo>′</mo></msup><mo>=</mo><mrow><mi>H</mi><mo>+</mo><mrow><msub><mi>λ</mi><mtext>id</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mpadded style="width:3.720em;" width="3.720em"><mtext>softmax</mtext></mpadded><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mfrac><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>W</mi><mi>Q</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>W</mi><mi>K</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo>+</mo><mi>M</mi></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>W</mi><mi>V</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">H^{\prime}=H+\lambda_{\text{id}}\,\text{softmax}\!\left(\frac{(HW_{Q})(EW_{K})^{\top}}{\sqrt{d}}+M\right)(EW_{V}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="H" class="ltx_Math" display="inline" id="A5.p5.m1" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> denotes the current hidden tokens, <math alttext="E" class="ltx_Math" display="inline" id="A5.p5.m2" intent=":literal"><semantics><mi>E</mi><annotation encoding="application/x-tex">E</annotation></semantics></math> the stacked face-embedding tokens, and <math alttext="W_{Q},W_{K},W_{V}" class="ltx_Math" display="inline" id="A5.p5.m3" intent=":literal"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>K</mi></msub><mo>,</mo><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_{Q},W_{K},W_{V}</annotation></semantics></math> the projection matrices; <math alttext="d" class="ltx_Math" display="inline" id="A5.p5.m4" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> is the query/key dimension, and <math alttext="\lambda_{\text{id}}=1.0" class="ltx_Math" display="inline" id="A5.p5.m5" intent=":literal"><semantics><mrow><msub><mi>λ</mi><mtext>id</mtext></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding="application/x-tex">\lambda_{\text{id}}=1.0</annotation></semantics></math> during training. When SigLIP is enabled, its tokens are processed by a parallel cross-attention with an independent scaling coefficient.</p>
</div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Experimental Details</h2>
<section class="ltx_subsection" id="A6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.1 </span>Implementation Details</h3>
<div class="ltx_para" id="A6.SS1.p1">
<p class="ltx_p">WithAnyone is trained on 8 NVIDIA H100 GPUs, with a batch size of <math alttext="4" class="ltx_Math" display="inline" id="A6.SS1.p1.m1" intent=":literal"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math> on each GPU. The learning rate is set to <math alttext="1e^{-4}" class="ltx_Math" display="inline" id="A6.SS1.p1.m2" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1e^{-4}</annotation></semantics></math>, and the AdamW optimizer is employed with a weight decay of <math alttext="0.01" class="ltx_Math" display="inline" id="A6.SS1.p1.m3" intent=":literal"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation></semantics></math>.
The pre-training phase runs for <math alttext="60k" class="ltx_Math" display="inline" id="A6.SS1.p1.m4" intent=":literal"><semantics><mrow><mn>60</mn><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">60k</annotation></semantics></math> steps, with a fixed prompt used during the first <math alttext="20k" class="ltx_Math" display="inline" id="A6.SS1.p1.m5" intent=":literal"><semantics><mrow><mn>20</mn><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">20k</annotation></semantics></math> steps. The subsequent paired-tuning phase lasts <math alttext="30k" class="ltx_Math" display="inline" id="A6.SS1.p1.m6" intent=":literal"><semantics><mrow><mn>30</mn><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">30k</annotation></semantics></math> steps: <math alttext="50\%" class="ltx_Math" display="inline" id="A6.SS1.p1.m7" intent=":literal"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">50\%</annotation></semantics></math> of the samples use paired (reference, ground-truth) data, while the remaining <math alttext="50\%" class="ltx_Math" display="inline" id="A6.SS1.p1.m8" intent=":literal"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">50\%</annotation></semantics></math> continue reconstruction training. Finally, a quality/style tuning stage of <math alttext="10k" class="ltx_Math" display="inline" id="A6.SS1.p1.m9" intent=":literal"><semantics><mrow><mn>10</mn><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">10k</annotation></semantics></math> steps is performed with a reduced learning rate of <math alttext="1\times 10^{-5}" class="ltx_Math" display="inline" id="A6.SS1.p1.m10" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-5}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A6.SS1.p2">
<p class="ltx_p">For the extended ID contrastive loss, the target is used as the positve sample, while other IDs from samples in the same batch serve as negative samples. With the global batch size of <math alttext="32" class="ltx_Math" display="inline" id="A6.SS1.p2.m1" intent=":literal"><semantics><mn>32</mn><annotation encoding="application/x-tex">32</annotation></semantics></math>, this yields less than a hundred negative samples. Extended negative samples are drawn from reference bank. If this ID is identified as one of the <math alttext="3" class="ltx_Math" display="inline" id="A6.SS1.p2.m2" intent=":literal"><semantics><mn>3</mn><annotation encoding="application/x-tex">3</annotation></semantics></math>k ID in the reference bank, we simply omit its own ID and draw the from other IDs. If this ID is not identified, then it makes things easier – all the IDs in the reference bank can be used as negative samples.</p>
</div>
<div class="ltx_para" id="A6.SS1.p3">
<p class="ltx_p">For other baseline methods, official implementations and checkpoints (or API) are used with default settings. Methods are tested on MultiID-Bench and real-human subset of OmniContext <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib54" title="">54</a>]</cite>. OmniContext uses Vision-Language Models (VLMs) to evaluate the prompt-following (PF) and subject-consistency (SC) of generated images. For reproducibility, the VLM is fixed to Qwen2.5-VL <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib1" title="">1</a>]</cite>. ID-Patch <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib68" title="">68</a>]</cite> requires pose condition, and we use the ground-truth pose for it.</p>
</div>
<div class="ltx_para" id="A6.SS1.p4">
<p class="ltx_p">Single face embedding model may induce biased evaluation on ID similarity, thus we average three de-facto face recognition models’ consine similarity to compute the overall ID similarity metric, namely ArcFace <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib11" title="">11</a>]</cite>, FaceNet <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib45" title="">45</a>]</cite>, and AdaFace <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib26" title="">26</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="A6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.2 </span>More Discussion on the Quantitative Results</h3>
<div class="ltx_para" id="A6.SS2.p1">
<p class="ltx_p">The performance of GPT on our 3‑and‑4‑people subset offers a useful validation of our copy‑paste metric, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S5.T2.st2" title="Table 2(b) ‣ Table 2 ‣ 5.1 Training Objectives ‣ 5 WithAnyone: Controllable and ID-Consistent Generation ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">2(b)</span></a>. This subset largely comprises group photographs from TV series that GPT may have encountered during pretraining, so GPT attains unusually high identity‑similarity scores both to the ground truth (GT) and to the reference images. Actually, in one case GPT even generates an ID from the TV series that is not present in the reference images. This behaviour approximates an idealized scenario in which a model fully understands and faithfully reproduces the target identity: similarity to GT and to references are both high, and the copy‑paste measure the difference between distances to GT and to references approaches zero. These observations are consistent with our metric design and support its ability to distinguish true identity understanding from trivial copy‑and‑paste replication.</p>
</div>
<div class="ltx_para" id="A6.SS2.p2">
<p class="ltx_p">We report the experimental limit in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S5.T1.st2" title="Table 1(b) ‣ Table 1 ‣ 5.1 Training Objectives ‣ 5 WithAnyone: Controllable and ID-Consistent Generation ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">1(b)</span></a>. If one model completely copy the reference image, <math alttext="\mathrm{Sim_{GT}}=0.521" class="ltx_Math" display="inline" id="A6.SS2.p2.m1" intent=":literal"><semantics><mrow><msub><mi>Sim</mi><mi>GT</mi></msub><mo>=</mo><mn>0.521</mn></mrow><annotation encoding="application/x-tex">\mathrm{Sim_{GT}}=0.521</annotation></semantics></math>, <math alttext="\mathrm{Sim_{Ref}}=1.0" class="ltx_Math" display="inline" id="A6.SS2.p2.m2" intent=":literal"><semantics><mrow><msub><mi>Sim</mi><mi>Ref</mi></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding="application/x-tex">\mathrm{Sim_{Ref}}=1.0</annotation></semantics></math>, and copy-paste is <math alttext="0.999" class="ltx_Math" display="inline" id="A6.SS2.p2.m3" intent=":literal"><semantics><mn>0.999</mn><annotation encoding="application/x-tex">0.999</annotation></semantics></math>, which aligns with the theoretical limit <math alttext="1.0" class="ltx_Math" display="inline" id="A6.SS2.p2.m4" intent=":literal"><semantics><mn>1.0</mn><annotation encoding="application/x-tex">1.0</annotation></semantics></math> of copy-paste.</p>
</div>
<div class="ltx_para" id="A6.SS2.p3">
<p class="ltx_p">The prompt-following ability is measured by CLIP-I and CLIP-T in our benchmark, and is judged by VLM in OmniContext. WithAnyonegains state-of-the-art performance in both metrics, and is ranked the highest in our user study. However, the credibility of CLIP scores and the aesthetic scores may be debated, as they are not always consistent with human perception.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Ablation Study Details</h2>
<figure class="ltx_figure" id="A7.F16">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A7.F16.fig1" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="A7.F16.g1" src="x28.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">ID Loss Curves with <math alttext="\lambda\times" class="ltx_math_unparsed" display="inline" id="A7.F16.m6" intent=":literal"><semantics><mrow><mi>λ</mi><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">\lambda\times</annotation></semantics></math> InfoNCE Loss.<span class="ltx_text ltx_font_medium"> <math alttext="0.1" class="ltx_Math" display="inline" id="A7.F16.m7" intent=":literal"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math> is <math alttext="0.1\times" class="ltx_math_unparsed" display="inline" id="A7.F16.m8" intent=":literal"><semantics><mrow><mn>0.1</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">0.1\times</annotation></semantics></math> InfoNCE Loss without extended negative samples, and <math alttext="0.1+" class="ltx_Math" display="inline" id="A7.F16.m9" intent=":literal"><semantics><mrow><mn>0.1</mn><mo>+</mo></mrow><annotation encoding="application/x-tex">0.1+</annotation></semantics></math> is <math alttext="0.1\times" class="ltx_math_unparsed" display="inline" id="A7.F16.m10" intent=":literal"><semantics><mrow><mn>0.1</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">0.1\times</annotation></semantics></math> InfoNCE Loss with extended negative samples.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A7.F16.fig2" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="405" id="A7.F16.g2" src="x29.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">Trade-off Curves<span class="ltx_text ltx_font_medium"> with <math alttext="\lambda\times" class="ltx_math_unparsed" display="inline" id="A7.F16.m13" intent=":literal"><semantics><mrow><mi>λ</mi><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">\lambda\times</annotation></semantics></math> Siglip and <math alttext="(1-\lambda)\times" class="ltx_math_unparsed" display="inline" id="A7.F16.m14" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>λ</mi><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">(1-\lambda)\times</annotation></semantics></math> ArcFace signal.</span></span></figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p">In this section, we systematically evaluate the impact of training strategy, GT-aligned ID-Loss, InfoNCE ID Loss, and our dataset construction. User study is also conducted to validate the consistency of the proposed metrics with human perception, as well as evaluate the human preference on different methods.</p>
</div>
<div class="ltx_para" id="A7.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">SigLIP signal.</span> SigLIP <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib65" title="">65</a>]</cite> signal is introduced to retain copy-paste effect when user tend to retain the features from reference images like hairstyle, accessories, etc. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A7.F16" title="Figure 16 ‣ Appendix G Ablation Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">16</span></a>, increasing the SigLIP signal weight effectively amplifies the copy-paste effect while simultaneously boosting ID similarity to the reference images exactly as expected, since stronger SigLIP guidance enforces tighter semantic alignment and transfers more fine-grained appearance cues (e.g., hairstyle, accessories, local textures).</p>
</div>
<div class="ltx_para" id="A7.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Training strategy.</span> We evaluate the effect of a paired-data fine-tuning stage. After an initial reconstruction training phase, we either continue training with paired (reference, ground-truth) data or keep training under the reconstruction objective for 10k steps. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.T3" title="Table 3 ‣ 6.3 Ablation and User Studies ‣ 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">3</span></a>, continuing with paired data effectively reduces the copy-paste effect without compromising similarity to the ground truth.</p>
</div>
<div class="ltx_para" id="A7.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Dataset construction.</span> To validate the effectiveness of our dataset, we trained a model on FFHQ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#bib.bib24" title="">24</a>]</cite> using reconstruction training for the same number of steps. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.T3" title="Table 3 ‣ 6.3 Ablation and User Studies ‣ 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">3</span></a>, the FFHQ-trained model performs poorly across all metrics. This likely stems from FFHQ’s limited diversity and size, as it contains only  70k face-only portrait images.</p>
</div>
<div class="ltx_para" id="A7.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">GT-aligned ID-Loss.</span> We validate the GT-aligned ID-Loss with a simple experiment that visualizes predicted faces at different denoising time steps during training. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.F7" title="Figure 7 ‣ 6.3 Ablation and User Studies ‣ 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">7</span></a>, at low noise levels the GT-aligned ID-Loss is substantially lower than the loss computed using prediction-aligned landmarks, indicating that aligning faces to ground-truth landmarks reduces denoising error and yields a more accurate identity assessment. At high noise levels the GT-aligned ID-Loss shows greater variance, producing stronger and more informative gradients that help the model learn identity features.</p>
</div>
<div class="ltx_para" id="A7.p6">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">InfoNCE Loss.</span> The InfoNCE loss with extended negative samples is crucial for the convergence in the early training stage. We conduct a toy experiment with 1000 training samples, and record ID Loss curves with no InfoNCE loss, <math alttext="0.1\times" class="ltx_math_unparsed" display="inline" id="A7.p6.m1" intent=":literal"><semantics><mrow><mn>0.1</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">0.1\times</annotation></semantics></math> InfoNCE loss without extended negatives, and <math alttext="0.1\times" class="ltx_math_unparsed" display="inline" id="A7.p6.m2" intent=":literal"><semantics><mrow><mn>0.1</mn><mo lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex">0.1\times</annotation></semantics></math> InfoNCE loss with extended negatives. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A7.F16" title="Figure 16 ‣ Appendix G Ablation Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">16</span></a>, ID loss fits a lot faster with InfoNCE loss with extended negatives, demonstrating its effectiveness in accelerating training convergence. It also largely increases the ID similarity score, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#S6.T3" title="Table 3 ‣ 6.3 Ablation and User Studies ‣ 6 Experiments ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_table" id="A7.T5">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span><span class="ltx_text ltx_font_bold">Correlation Statistics Between Machine Ranking and Human Ranking.</span> Reported values include Pearson’s <math alttext="r" class="ltx_Math" display="inline" id="A7.T5.m5" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>, Spearman’s <math alttext="\rho" class="ltx_Math" display="inline" id="A7.T5.m6" intent=":literal"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math>, and Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="A7.T5.m7" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math> with corresponding <math alttext="p" class="ltx_Math" display="inline" id="A7.T5.m8" intent=":literal"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Dimension (N)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Pearson <math alttext="r" class="ltx_Math" display="inline" id="A7.T5.m9" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> (p)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Spearman <math alttext="\rho" class="ltx_Math" display="inline" id="A7.T5.m10" intent=":literal"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math> (p)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Kendall <math alttext="\tau" class="ltx_Math" display="inline" id="A7.T5.m11" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math> (p)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">Copy-Paste</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math alttext="0.4417" class="ltx_Math" display="inline" id="A7.T5.m12" intent=":literal"><semantics><mn mathsize="0.900em">0.4417</mn><annotation encoding="application/x-tex">0.4417</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (</span><math alttext="7.98e{-48}" class="ltx_Math" display="inline" id="A7.T5.m13" intent=":literal"><semantics><mrow><mrow><mn mathsize="0.900em">7.98</mn><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">e</mi></mrow><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">48</mn></mrow><annotation encoding="application/x-tex">7.98e{-48}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math alttext="0.4535" class="ltx_Math" display="inline" id="A7.T5.m14" intent=":literal"><semantics><mn mathsize="0.900em">0.4535</mn><annotation encoding="application/x-tex">0.4535</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (</span><math alttext="1.26e{-50}" class="ltx_Math" display="inline" id="A7.T5.m15" intent=":literal"><semantics><mrow><mrow><mn mathsize="0.900em">1.26</mn><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">e</mi></mrow><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">50</mn></mrow><annotation encoding="application/x-tex">1.26e{-50}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">
<math alttext="0.3405" class="ltx_Math" display="inline" id="A7.T5.m16" intent=":literal"><semantics><mn mathsize="0.900em">0.3405</mn><annotation encoding="application/x-tex">0.3405</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (</span><math alttext="1.10e{-46}" class="ltx_Math" display="inline" id="A7.T5.m17" intent=":literal"><semantics><mrow><mrow><mn mathsize="0.900em">1.10</mn><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">e</mi></mrow><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">46</mn></mrow><annotation encoding="application/x-tex">1.10e{-46}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">)</span>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span class="ltx_text" style="font-size:90%;">ID Sim</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">
<math alttext="0.3254" class="ltx_Math" display="inline" id="A7.T5.m18" intent=":literal"><semantics><mn mathsize="0.900em">0.3254</mn><annotation encoding="application/x-tex">0.3254</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (</span><math alttext="1.54e{-26}" class="ltx_Math" display="inline" id="A7.T5.m19" intent=":literal"><semantics><mrow><mrow><mn mathsize="0.900em">1.54</mn><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">e</mi></mrow><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">26</mn></mrow><annotation encoding="application/x-tex">1.54e{-26}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">
<math alttext="0.3237" class="ltx_Math" display="inline" id="A7.T5.m20" intent=":literal"><semantics><mn mathsize="0.900em">0.3237</mn><annotation encoding="application/x-tex">0.3237</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (</span><math alttext="2.91e{-26}" class="ltx_Math" display="inline" id="A7.T5.m21" intent=":literal"><semantics><mrow><mrow><mn mathsize="0.900em">2.91</mn><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">e</mi></mrow><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">26</mn></mrow><annotation encoding="application/x-tex">2.91e{-26}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb">
<math alttext="0.2423" class="ltx_Math" display="inline" id="A7.T5.m22" intent=":literal"><semantics><mn mathsize="0.900em">0.2423</mn><annotation encoding="application/x-tex">0.2423</annotation></semantics></math><span class="ltx_text" style="font-size:90%;"> (</span><math alttext="1.11e{-25}" class="ltx_Math" display="inline" id="A7.T5.m23" intent=":literal"><semantics><mrow><mrow><mn mathsize="0.900em">1.11</mn><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">e</mi></mrow><mo mathsize="0.900em">−</mo><mn mathsize="0.900em">25</mn></mrow><annotation encoding="application/x-tex">1.11e{-25}</annotation></semantics></math><span class="ltx_text" style="font-size:90%;">)</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>User Study Details</h2>
<figure class="ltx_figure" id="A8.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="412" id="A8.F17.g1" src="files/user_study_interface.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span class="ltx_text ltx_font_bold" style="font-size:90%;">User Study Interface<span class="ltx_text ltx_font_medium">. </span></span></figcaption>
</figure>
<div class="ltx_para" id="A8.p1">
<p class="ltx_p">Our user study is conducted with the same data samples and generated results in our quantitative experiments. Due to a tight financial budget, we randomly select <math alttext="100" class="ltx_Math" display="inline" id="A8.p1.m1" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> samples from single-person subset, <math alttext="100" class="ltx_Math" display="inline" id="A8.p1.m2" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> samples from 2-people subset, and all samples from 3-and-4 people subset. 10 participants are recruited for the study, all of whom are trained with a brief tutorial to understand the task and evaluation criteria.</p>
</div>
<div class="ltx_para" id="A8.p2">
<p class="ltx_p">We illustrate the interface used in our user study in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A8.F17" title="Figure 17 ‣ Appendix H User Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">17</span></a>.</p>
</div>
<section class="ltx_subsection" id="A8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">H.1 </span>Correlation Analysis</h3>
<div class="ltx_para" id="A8.SS1.p1">
<p class="ltx_p">We analyze the correlation between our proposed metrics and user study results. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14975v1#A7.T5" title="Table 5 ‣ Appendix G Ablation Study Details ‣ WithAnyone: Towards Controllable and ID Consistent Image Generation"><span class="ltx_text ltx_ref_tag">5</span></a>, our copy-paste metric shows a moderate positve correlation with user ratings on copy-paste effect.</p>
</div>
</section>
<section class="ltx_subsection" id="A8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">H.2 </span>Participant Instructions</h3>
<div class="ltx_para" id="A8.SS2.p1">
<p class="ltx_p">We provide the instructions for training the participants in the following table.</p>
</div>
<figure class="ltx_table" id="A8.SS2.tab1"><span class="ltx_inline-block"><svg class="ltx_picture ltx_centering" height="30161.55" id="A8.SS2.pic1" overflow="visible" version="1.1" viewbox="0 0 600 30161.55" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,30161.55) matrix(1 0 0 -1 0 0)"><g fill="#6D99FF" fill-opacity="1.0" style="--ltx-fill-color:#6D99FF;"><path d="M 0 5.91 L 0 30155.64 C 0 30158.91 2.64 30161.55 5.91 30161.55 L 594.09 30161.55 C 597.36 30161.55 600 30158.91 600 30155.64 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F9FAFF" fill-opacity="1.0" style="--ltx-fill-color:#F9FAFF;"><path d="M 1.97 5.91 L 1.97 30137.44 L 598.03 30137.44 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 22660.93)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" style="--ltx-fg-color:#FFFFFF;--fo_width :40.23em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Participant Instructions and Evaluation Procedure</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 -3539.27)"><foreignobject color="#000000" height="30111.85" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:1894.18em;--fo_depth :282em;" transform="matrix(1 0 0 -1 0 26209.81)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_inline-sectional-block ltx_minipage">
<span class="ltx_para" id="A8.SS2.pic1.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Data source and task overview.</span></span>
<span class="ltx_p">Five different methods generated images under the following conditions:</span>
<span class="ltx_itemize" id="A8.I1">
<span class="ltx_item" id="A8.I1.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I1.i1.p1">
<span class="ltx_p">A single prompt that describes the “ground truth image.”</span>
</span></span>
<span class="ltx_item" id="A8.I1.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I1.i2.p1">
<span class="ltx_p">Between 1 and 4 people in the scene (most examples contain 1–2 people).</span>
</span></span>
</span>
<span class="ltx_p">For each trial you will be shown the ground truth image, input images, and a generation instruction. Then you will observe five generated group-photo results (one per method) and rank them according to several evaluation dimensions. Use a 5‑star scale where 5 stars = best and 1 star = worst. Please read the input image(s) and the editing instruction carefully before inspecting the generated results.</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Evaluation procedure (per-image ranking).</span></span>
<span class="ltx_p">Rank each generated image individually on the following criteria.</span>
</span>
<section class="ltx_paragraph" id="A8.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Identity similarity</h4>
<span class="ltx_para" id="A8.SS2.SSS0.Px1.p1">
<span class="ltx_itemize" id="A8.I2">
<span class="ltx_item" id="A8.I2.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I2.i1.p1">
<span class="ltx_p">How well do the person(s) in the generated image resemble the person(s) in the <em class="ltx_emph ltx_font_italic">ground truth image</em>?</span>
</span></span>
<span class="ltx_item" id="A8.I2.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I2.i2.p1">
<span class="ltx_p">Rank images by their resemblance to the ground truth image: the more the generated person(s) look like the original reference, the higher the rating.</span>
</span></span>
<span class="ltx_item" id="A8.I2.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I2.i3.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Important:</span> When judging identity similarity, ignore factors such as image quality, rendering artifacts, or general aesthetics. Focus only on how much the person(s) resemble the original reference(s). Also, try to assess resemblance to the ground truth image as a whole, rather than comparing to any single separate “reference person n.”</span>
</span></span>
</span>
</span>
</section>
<section class="ltx_paragraph" id="A8.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Copy-and-paste effect (excessive mimicry of the reference)</h4>
<span class="ltx_para" id="A8.SS2.SSS0.Px2.p1">
<span class="ltx_itemize" id="A8.I3">
<span class="ltx_item" id="A8.I3.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I3.i1.p1">
<span class="ltx_p">Generated images should resemble the original reference but should not be direct copies of an individual reference photo.</span>
</span></span>
<span class="ltx_item" id="A8.I3.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I3.i2.p1">
<span class="ltx_p">Evaluate whether the generated person appears to be directly copied from one of the reference images. Consider changes (or lack thereof) in <span class="ltx_text ltx_font_bold">expression, head pose and orientation, facial expression/demeanor, and lighting/shading</span>.</span>
</span></span>
<span class="ltx_item" id="A8.I3.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I3.i3.p1">
<span class="ltx_p">The lower the degree of direct copying (i.e., the less it looks like a pasted replica), the better. Rank according to the amount of change observed in the person(s): more natural variation (less copy-paste) should be ranked higher.</span>
</span></span>
</span>
</span>
</section>
<section class="ltx_paragraph" id="A8.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Prompt following</h4>
<span class="ltx_para" id="A8.SS2.SSS0.Px3.p1">
<span class="ltx_itemize" id="A8.I4">
<span class="ltx_item" id="A8.I4.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I4.i1.p1">
<span class="ltx_p">Does the generated image reflect the content and constraints specified by the prompt/instruction?</span>
</span></span>
<span class="ltx_item" id="A8.I4.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I4.i2.p1">
<span class="ltx_p">Rank images by prompt fidelity: the more faithfully the image follows the prompt, the higher the ranking.</span>
</span></span>
</span>
</span>
</section>
<section class="ltx_paragraph" id="A8.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Aesthetics</h4>
<span class="ltx_para" id="A8.SS2.SSS0.Px4.p1">
<span class="ltx_itemize" id="A8.I5">
<span class="ltx_item" id="A8.I5.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I5.i1.p1">
<span class="ltx_p">Judge the overall visual quality and pleasantness of the generated image (e.g., smoothness of rendering, harmonious body poses and composition).</span>
</span></span>
<span class="ltx_item" id="A8.I5.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A8.I5.i2.p1">
<span class="ltx_p">Rank images by aesthetic quality: higher perceived visual quality receives higher ratings.</span>
</span></span>
</span>
</span>
</section></span></span></span></foreignobject></g></g></svg></span>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A9">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Prompts for Language Models</h2>
<div class="ltx_para" id="A9.p1">
<p class="ltx_p">Large language models (LLMs) and vision-language models (VLMs) are used in various stages of our work, including dataset captioning and OmniContext evaluation.</p>
</div>
<section class="ltx_subsection" id="A9.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">I.1 </span>Dataset Captioning</h3>
<div class="ltx_para" id="A9.SS1.p1">
<p class="ltx_p">Besides the system prompt, we design 6 different prompts to generate diverse captions for each image. 1 prompt is randomly selected for each image during captioning.</p>
</div>
<figure class="ltx_table" id="A9.SS1.tab1"><span class="ltx_inline-block"><svg class="ltx_picture ltx_centering" height="32546.2" id="A9.SS1.pic1" overflow="visible" version="1.1" viewbox="0 0 600 32546.2" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,32546.2) matrix(1 0 0 -1 0 0)"><g fill="#6D99FF" fill-opacity="1.0" style="--ltx-fill-color:#6D99FF;"><path d="M 0 5.91 L 0 32540.3 C 0 32543.56 2.64 32546.2 5.91 32546.2 L 594.09 32546.2 C 597.36 32546.2 600 32543.56 600 32540.3 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F9FAFF" fill-opacity="1.0" style="--ltx-fill-color:#F9FAFF;"><path d="M 1.97 5.91 L 1.97 32520.56 L 598.03 32520.56 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 22660.16)"><foreignobject color="#FFFFFF" height="13.84" overflow="visible" style="--ltx-fg-color:#FFFFFF;--fo_width :40.23em;--fo_height:0.75em;--fo_depth :0.25em;" transform="matrix(1 0 0 -1 0 10.38)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Full Prompts for Dataset Captioning (6 variants)</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 -5756.35)"><foreignobject color="#000000" height="32494.96" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:2054.41em;--fo_depth :294em;" transform="matrix(1 0 0 -1 0 28426.89)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">System Prompt</span>: You are an advanced vision-language model tasked with generating accurate and comprehensive captions for images.</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt 1:</span> Please provide a brief description of the image based on these guidelines:</span>
<span class="ltx_enumerate" id="A9.I1">
<span class="ltx_item" id="A9.I1.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="A9.I1.i1.p1">
<span class="ltx_p">Describe the clothing, accessories, or jewelry worn by the people in detail.</span>
</span></span>
<span class="ltx_item" id="A9.I1.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="A9.I1.i2.p1">
<span class="ltx_p">Describe the genders, actions, and posture of the individual in detail, focusing on what they are doing.</span>
</span></span>
<span class="ltx_item" id="A9.I1.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="A9.I1.i3.p1">
<span class="ltx_p">The description should be concise, with a maximum of 77 words.</span>
</span></span>
<span class="ltx_item" id="A9.I1.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="A9.I1.i4.p1">
<span class="ltx_p">Start with ‘This image shows’</span>
</span></span>
</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt 2:</span> Offer a short description of the image according to these rules:</span>
<span class="ltx_enumerate" id="A9.I2">
<span class="ltx_item" id="A9.I2.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="A9.I2.i1.p1">
<span class="ltx_p">Focus on details about clothing, accessories, or jewelry.</span>
</span></span>
<span class="ltx_item" id="A9.I2.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="A9.I2.i2.p1">
<span class="ltx_p">Focus on the gender, activity, and pose, and explain what the people is doing.</span>
</span></span>
<span class="ltx_item" id="A9.I2.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="A9.I2.i3.p1">
<span class="ltx_p">Keep the description within 77 words.</span>
</span></span>
<span class="ltx_item" id="A9.I2.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="A9.I2.i4.p1">
<span class="ltx_p">Begin the description with ‘This image shows’</span>
</span></span>
</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt 3:</span> Please describe the image briefly, following these instructions:</span>
<span class="ltx_enumerate" id="A9.I3">
<span class="ltx_item" id="A9.I3.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="A9.I3.i1.p1">
<span class="ltx_p">Provide a detailed description of the clothing or jewelry the person may be wearing.</span>
</span></span>
<span class="ltx_item" id="A9.I3.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="A9.I3.i2.p1">
<span class="ltx_p">Provide a detailed description of the two persons’ gender, actions, and body position.</span>
</span></span>
<span class="ltx_item" id="A9.I3.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="A9.I3.i3.p1">
<span class="ltx_p">Limit the description to no more than 77 words.</span>
</span></span>
<span class="ltx_item" id="A9.I3.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="A9.I3.i4.p1">
<span class="ltx_p">Begin your description with ‘This image shows’</span>
</span></span>
</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt 4:</span> Describe the picture briefly according to these rules:</span>
<span class="ltx_enumerate" id="A9.I4">
<span class="ltx_item" id="A9.I4.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="A9.I4.i1.p1">
<span class="ltx_p">Provide a detailed description of the clothing, jewelry, or accessories of the individuals.</span>
</span></span>
<span class="ltx_item" id="A9.I4.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="A9.I4.i2.p1">
<span class="ltx_p">Focus on the two persons’ gender, what they are doing, and their posture.</span>
</span></span>
<span class="ltx_item" id="A9.I4.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="A9.I4.i3.p1">
<span class="ltx_p">Keep the description concise, within a limit of 77 words.</span>
</span></span>
<span class="ltx_item" id="A9.I4.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="A9.I4.i4.p1">
<span class="ltx_p">Start your description with ‘This image shows’</span>
</span></span>
</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt 5:</span> Provide a short and precise description of the image based on the following guidelines:</span>
<span class="ltx_enumerate" id="A9.I5">
<span class="ltx_item" id="A9.I5.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="A9.I5.i1.p1">
<span class="ltx_p">Describe what the person is wearing or any accessories.</span>
</span></span>
<span class="ltx_item" id="A9.I5.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="A9.I5.i2.p1">
<span class="ltx_p">Focus on the gender, activities, and body posture of the person.</span>
</span></span>
<span class="ltx_item" id="A9.I5.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="A9.I5.i3.p1">
<span class="ltx_p">Ensure the description is no longer than 77 words.</span>
</span></span>
<span class="ltx_item" id="A9.I5.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="A9.I5.i4.p1">
<span class="ltx_p">Begin with ‘This image shows’</span>
</span></span>
</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt 6:</span> Briefly describe the image according to these instructions:</span>
<span class="ltx_enumerate" id="A9.I6">
<span class="ltx_item" id="A9.I6.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">1.</span>
<span class="ltx_para" id="A9.I6.i1.p1">
<span class="ltx_p">Provide a precise description of the clothing, jewelry, or other adornments of the people.</span>
</span></span>
<span class="ltx_item" id="A9.I6.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">2.</span>
<span class="ltx_para" id="A9.I6.i2.p1">
<span class="ltx_p">Focus on the person’s gender, what they are doing, and their posture.</span>
</span></span>
<span class="ltx_item" id="A9.I6.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">3.</span>
<span class="ltx_para" id="A9.I6.i3.p1">
<span class="ltx_p">The description should not exceed 77 words.</span>
</span></span>
<span class="ltx_item" id="A9.I6.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">4.</span>
<span class="ltx_para" id="A9.I6.i4.p1">
<span class="ltx_p">Start with the phrase ‘This image shows’</span>
</span></span>
</span>
</span></span></span></foreignobject></g></g></svg></span>
</figure>
<figure class="ltx_table" id="A9.SS1.tab2"><span class="ltx_inline-block"><svg class="ltx_picture ltx_centering" height="21349.61" id="A9.SS1.pic2" overflow="visible" version="1.1" viewbox="0 0 600 21349.61" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,21349.61) matrix(1 0 0 -1 0 0)"><g fill="#6D99FF" fill-opacity="1.0" style="--ltx-fill-color:#6D99FF;"><path d="M 0 5.91 L 0 21343.71 C 0 21346.97 2.64 21349.61 5.91 21349.61 L 594.09 21349.61 C 597.36 21349.61 600 21346.97 600 21343.71 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F9FAFF" fill-opacity="1.0" style="--ltx-fill-color:#F9FAFF;"><path d="M 1.97 5.91 L 1.97 21323.97 L 598.03 21323.97 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 21333.33)"><foreignobject color="#FFFFFF" height="13.84" overflow="visible" style="--ltx-fg-color:#FFFFFF;--fo_width :40.23em;--fo_height:0.75em;--fo_depth :0.25em;" transform="matrix(1 0 0 -1 0 10.38)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Modified Prompt for OmniContext Evaluation (Face Identity Focus)</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 14.32)"><foreignobject color="#000000" height="21298.37" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:1539.19em;--fo_depth :0.04em;" transform="matrix(1 0 0 -1 0 21297.83)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p">Rate from 0 to 10:</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Task:</span> Evaluate how well the facial features in the final image match those of the individuals in the original reference images, as described in the instruction. Focus strictly on facial identity similarity; ignore hairstyle, clothing, body shape, background, and pose.</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Scoring Criteria</span></span>
<span class="ltx_itemize" id="A9.I7">
<span class="ltx_item" id="A9.I7.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I7.i1.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">0:</span> The facial features are <em class="ltx_emph ltx_font_italic">completely different</em> from those in the reference images.</span>
</span></span>
<span class="ltx_item" id="A9.I7.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I7.i2.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">1–3:</span> The facial features have <em class="ltx_emph ltx_font_italic">minimal similarity</em> with only one or two matching elements.</span>
</span></span>
<span class="ltx_item" id="A9.I7.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I7.i3.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">4–6:</span> The facial features have <em class="ltx_emph ltx_font_italic">moderate similarity</em> but several important differences remain.</span>
</span></span>
<span class="ltx_item" id="A9.I7.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I7.i4.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">7–9:</span> The facial features are <em class="ltx_emph ltx_font_italic">highly similar</em> with only minor discrepancies.</span>
</span></span>
<span class="ltx_item" id="A9.I7.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I7.i5.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">10:</span> The facial features are <em class="ltx_emph ltx_font_italic">perfectly matched</em> to those in the reference images.</span>
</span></span>
</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Pay detailed attention to these facial elements:</span></span>
<span class="ltx_itemize" id="A9.I8">
<span class="ltx_item" id="A9.I8.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I8.i1.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Eyes:</span> Shape, size, spacing, color, and distinctive characteristics of the eyes and eyebrows.</span>
</span></span>
<span class="ltx_item" id="A9.I8.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I8.i2.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Nose:</span> Shape, size, width, bridge height, and nostril appearance.</span>
</span></span>
<span class="ltx_item" id="A9.I8.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I8.i3.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Mouth:</span> Lip shape, fullness, width, and distinctive smile characteristics.</span>
</span></span>
<span class="ltx_item" id="A9.I8.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I8.i4.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Facial structure:</span> Cheekbone prominence, jawline definition, chin shape, and forehead structure.</span>
</span></span>
<span class="ltx_item" id="A9.I8.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I8.i5.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Skin features:</span> Distinctive marks like moles, freckles, wrinkles, and overall facial texture.</span>
</span></span>
<span class="ltx_item" id="A9.I8.i6" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I8.i6.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Proportions:</span> Overall facial symmetry and proportional relationships between features.</span>
</span></span>
</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Example:</span> If the instruction requests combining the face from one image onto another pose, the final image should clearly show the <em class="ltx_emph ltx_font_italic">same</em> facial features from the source image.</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Important:</span></span>
<span class="ltx_itemize" id="A9.I9">
<span class="ltx_item" id="A9.I9.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I9.i1.p1">
<span class="ltx_p">For each significant facial feature difference, deduct at least one point.</span>
</span></span>
<span class="ltx_item" id="A9.I9.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I9.i2.p1">
<span class="ltx_p"><em class="ltx_emph ltx_font_italic">Ignore</em> hairstyle, body shape, clothing, background, pose, or other non-facial elements.</span>
</span></span>
<span class="ltx_item" id="A9.I9.i3" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I9.i3.p1">
<span class="ltx_p">Focus <em class="ltx_emph ltx_font_italic">only</em> on facial similarity, not whether the overall instruction was followed.</span>
</span></span>
<span class="ltx_item" id="A9.I9.i4" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I9.i4.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Scoring should be strict</span> high scores should only be given for very close facial matches.</span>
</span></span>
<span class="ltx_item" id="A9.I9.i5" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A9.I9.i5.p1">
<span class="ltx_p">Consider the level of detail visible in the images when making your assessment.</span>
</span></span>
</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Editing instruction:</span> <span class="ltx_text ltx_font_typewriter">&lt;instruction&gt;</span></span>
</span></span></span></foreignobject></g></g></svg></span>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 16 14:35:36 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
