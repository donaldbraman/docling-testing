<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>LaSeR: Reinforcement Learning with Last-Token Self-Rewarding</title>
<!--Generated on Thu Oct 16 17:56:58 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2510.14943v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S1" title="In LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S2" title="In LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3" title="In LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.SS1" title="In 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.SS2" title="In 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>LaSeR: Reinforcement Learning with Last-Token Self-Rewarding</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.SS2.SSS1" title="In 3.2 LaSeR: Reinforcement Learning with Last-Token Self-Rewarding ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Formal Formulation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.SS3" title="In 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Other Techniques</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.SS4" title="In 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Brief Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S4" title="In LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S4.SS1" title="In 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S4.SS2" title="In 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results and Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S4.SS3" title="In 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Inference-Time Scaling Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S5" title="In Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S5.SS1" title="In 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>The Impact of Simplifying The Reference Log-Probabilities to A Constant</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S5.SS2" title="In 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>The Generalizability of LaSeR to General Reasoning Domain</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S5.SS3" title="In Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Further Reduction or Increase of Self-Rewarding Cost</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S6" title="In 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A1" title="In 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>The Length Bias in Implicit Reward</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A2" title="In 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Statistics of <math alttext="\log\pi_{ref}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log\pi_{ref}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A3" title="In 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Detailed Training Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A4" title="In 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Ablation Studies on Self-Rewarding Hyper-Parameters</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A5" title="In Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A6" title="In Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A7" title="In Appendix F Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss ‣ Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Detailed Self-Verification Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A8" title="In Appendix G Detailed Self-Verification Results ‣ Appendix F Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss ‣ Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>Training and Evaluation Settings in General Reasoning Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A9" title="In Appendix H Training and Evaluation Settings in General Reasoning Experiments ‣ Appendix G Detailed Self-Verification Results ‣ Appendix F Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss ‣ Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span>Prompt Templates</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">LaSeR: Reinforcement Learning with Last-Token Self-Rewarding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Wenkai Yang<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1,</span></sup>  , Weijie Liu<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>, Ruobing Xie<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>, Yiju Guo<sup class="ltx_sup">1</sup>,
<br class="ltx_break"/>Lulu Wu<sup class="ltx_sup">2</sup>, Saiyong Yang<sup class="ltx_sup">2</sup>, Yankai Lin<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1,</span></sup>
<br class="ltx_break"/><sup class="ltx_sup">1</sup><span class="ltx_text ltx_font_bold">Gaoling School of Artificial Intelligence, Renmin University of China</span>
<br class="ltx_break"/><sup class="ltx_sup">2</sup><span class="ltx_text ltx_font_bold">LLM Department, Tencent
<br class="ltx_break"/></span>🖂 {wenkaiyang,yankailin}@ruc.edu.cn
<br class="ltx_break"/>
</span><span class="ltx_author_notes"> Work done during an internship at Tencent. Corresponding author.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Reinforcement Learning with Verifiable Rewards (RLVR)
has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs).
To address the lack of verification signals at test time, prior studies incorporate the training of model’s self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency.
In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: <span class="ltx_text ltx_font_bold">the true reasoning reward of a solution is equal to its <span class="ltx_text ltx_font_italic">last-token self-rewarding score</span></span>, which is computed as the difference between the policy model’s next-token log-probability assigned to any pre-specified token at the solution’s last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose <span class="ltx_text ltx_font_bold">LaSeR</span> (Reinforcement Learning with <span class="ltx_text ltx_framed ltx_framed_underline">La</span>st-Token <span class="ltx_text ltx_framed ltx_framed_underline">Se</span>lf-<span class="ltx_text ltx_framed ltx_framed_underline">R</span>ewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs.
The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance.
Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference.
Experiments show that our method not only improves the model’s reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance. Code and models are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/RUCBM/LaSeR" title="">https://github.com/RUCBM/LaSeR</a>.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="335" id="S0.F1.g1" src="x1.png" width="647"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The full illustration of our method <span class="ltx_text ltx_font_bold">LaSeR</span>. During training, our approach augments the standard RLVR process with an additional MSE loss between the verifier-based rewards (<math alttext="r_{v}" class="ltx_Math" display="inline" id="S0.F1.m6" intent=":literal"><semantics><msub><mi>r</mi><mi>v</mi></msub><annotation encoding="application/x-tex">r_{v}</annotation></semantics></math>) and the last-token self-rewarding scores (<math alttext="r_{s}" class="ltx_Math" display="inline" id="S0.F1.m7" intent=":literal"><semantics><msub><mi>r</mi><mi>s</mi></msub><annotation encoding="application/x-tex">r_{s}</annotation></semantics></math>), where <math alttext="r_{s}" class="ltx_Math" display="inline" id="S0.F1.m8" intent=":literal"><semantics><msub><mi>r</mi><mi>s</mi></msub><annotation encoding="application/x-tex">r_{s}</annotation></semantics></math> is the difference between the policy model’s next-token log-probabilities of a pre-specified special token at the final response token and a pre-calculated constant <math alttext="c_{ref}" class="ltx_Math" display="inline" id="S0.F1.m9" intent=":literal"><semantics><msub><mi>c</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><annotation encoding="application/x-tex">c_{ref}</annotation></semantics></math>, scaled by the KL coefficient <math alttext="\beta_{v}" class="ltx_Math" display="inline" id="S0.F1.m10" intent=":literal"><semantics><msub><mi>β</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\beta_{v}</annotation></semantics></math>. The optimized self-rewarding scores can serve as auxiliary reward signals in both training and testing stages to enhance model performance.
</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p">In the past few years, Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib1" title="">2023</a>; MetaAI, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib31" title="">2024a</a>; Qwen Team, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib37" title="">2024</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib24" title="">2024a</a>)</cite> have advanced significantly, excelling in various domains <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib21" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib50" title="">2024b</a>)</cite>. However, they still face limitations in complex reasoning tasks <cite class="ltx_cite ltx_citemacro_citep">(AI-MO, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib3" title="">2024a</a>; OpenCompass, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib35" title="">2025</a>; Rein et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib39" title="">2024</a>; Jain et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib19" title="">2025</a>)</cite>. Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has shown great promise in enhancing the complex reasoning abilities of LLMs, as demonstrated by OpenAI o1 <cite class="ltx_cite ltx_citemacro_citep">(Jaech et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib18" title="">2024</a>)</cite> and DeepSeek-R1 <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib11" title="">2025</a>)</cite>. By rewarding reasoning paths based on the consistency between final outcomes and ground-truth answers through a deterministic verifier, RLVR incentivizes LLMs to produce more deliberate reasoning chains while effectively mitigating the risk of reward hacking <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib10" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p">Despite its effectiveness, a limitation of standard RLVR is its inability to continue providing verification signals for model outputs in scenarios where ground truth answers are unavailable, such as during test-time inference <cite class="ltx_cite ltx_citemacro_citep">(Zuo et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib68" title="">2025</a>)</cite>. To address this, existing works either train an external verifier <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib23" title="">2023</a>; Snell et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib45" title="">2024</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib63" title="">2024</a>; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib9" title="">2024</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib56" title="">2025b</a>)</cite> for evaluating candidate solutions or jointly optimize the self-verification and reasoning capabilities of the same policy model during RLVR <cite class="ltx_cite ltx_citemacro_citep">(Sareen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib40" title="">2025</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib27" title="">2025a</a>; Zha et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib62" title="">2025</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib20" title="">2025</a>)</cite>. However, we argue that <span class="ltx_text ltx_font_bold">these methods have a major issue of inefficiency</span>: the external verifier requires additional training on a separate LLM during or after reinforcement learning (RL); while joint optimization involves generating both solutions and self-verifications sequentially under two separate prompt templates, which doubles the per-sample inference cost and reduces generation efficiency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p">In this work, we propose <span class="ltx_text ltx_font_bold">LaSeR</span> (Reinforcement Learning with <span class="ltx_text ltx_framed ltx_framed_underline">La</span>st-Token <span class="ltx_text ltx_framed ltx_framed_underline">Se</span>lf-<span class="ltx_text ltx_framed ltx_framed_underline">R</span>ewarding), a lightweight and highly effective algorithm that achieves this goal, jointly optimizing reasoning and self-verification capabilities at nearly zero additional cost. Our core insight is that a model’s assessment in its own solution can be captured in the last token’s predicted probability distribution.
We first show theoretically that the RL objective of self-verification has a closed-form solution, where the true reasoning reward from the verifier is equal to the next-token log-probability ratio between the policy and reference models for a pre-specified special token (an unused token like “<span class="ltx_text ltx_font_typewriter">&lt;|vision_start|&gt;</span>” that serves as the pre-defined ground truth for verifications on correct candidate solutions) at the last response token, scaled by the KL coefficient. We refer to this scaled log-probability ratio as the <span class="ltx_text ltx_font_italic">last-token self-rewarding score</span>.
Furthermore, we observe that for a randomly chosen special token, its predicted log-probability under the reference model is practically a constant, small value across all problems and solutions (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A1.F11" title="Figure 11 ‣ Appendix A The Length Bias in Implicit Reward ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">11</span></a>). This enables us to simplify the self-rewarding score into a remarkably simple form that depends only on the policy model’s outputs and a pre-calculated constant, making it exceptionally efficient to compute.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p">Building on above analysis, we replace the explicit RL optimization for self-verification with a simple Mean Squared Error (MSE) loss. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S0.F1" title="Figure 1 ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">1</span></a>, we train the model to align its last-token self-rewarding score with the true reasoning reward from the verifier. In specific, after the policy model generates the solution for each problem, we calculate the last-token self-rewarding score based on its last token’s next-token log-probability for the pre-specified special token, and construct the corresponding MSE loss. This MSE objective is added directly to the standard RLVR loss, allowing for seamless joint optimization for both the reasoning and self-rewarding capabilities of the policy model. At both training and testing time, our method generates each candidate solution and computes the self-rewarding score in a single forward pass, incurring the cost of at most one additional token inference with no extra generation required. This is significantly more efficient than prior approaches, which require a separate inference step. The optimized self-rewarding scores can not only complement the original reasoning rewards during RLVR to further enhance training performance, but also be used at test time to rank and weight solutions for more accurate answer aggregation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p">We conduct experiments on both LLaMA <cite class="ltx_cite ltx_citemacro_citep">(MetaAI, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib32" title="">2024b</a>)</cite> and Qwen <cite class="ltx_cite ltx_citemacro_citep">(Qwen Team, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib37" title="">2024</a>)</cite> architectures, including pre-trained, mid-trained and reinforced variants, to demonstrate the effectiveness of our method in broader math reasoning tasks. Experimental results show that our methods not only effectively improve the reasoning performance of the policy model, but also allows its self-rewarding accuracy to reach a high level, thereby equipping the model with better confidence calibration of its own outputs and improving its inference-time scaling performance.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">RLVR for LLM Reasoning</span> Reinforcement Learning with Verifiable Rewards (RLVR), which sorely calculates binary rewards based on the final answers, has been shown to be highly effective in enhancing the reasoning capabilities of LLMs <cite class="ltx_cite ltx_citemacro_citep">(Jaech et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib18" title="">2024</a>; Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib11" title="">2025</a>; Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib48" title="">2025b</a>)</cite>. Current studies can be categorized into several directions, including but not limited to (1) designing more efficient and effective RLVR algorithms <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib41" title="">2017</a>; Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib42" title="">2024</a>; Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib57" title="">2025a</a>; Yue et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib61" title="">2025b</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib28" title="">2025b</a>; Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib66" title="">2025</a>)</cite>, (2) extending RLVR to general reasoning domain <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib30" title="">2025</a>; Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib67" title="">2025</a>; Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib58" title="">2025b</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib22" title="">2025</a>)</cite> and agent scenarios <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib52" title="">2025b</a>; Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib47" title="">2025a</a>; Dong et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib7" title="">2025</a>)</cite>, (3) collecting diverse verifiable datasets <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib16" title="">2025</a>; He et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib14" title="">2025</a>; Liu &amp; Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib26" title="">2025</a>; Ma et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib30" title="">2025</a>; Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib8" title="">2025</a>)</cite>, and (4) analyzing the mechanisms of RLVR <cite class="ltx_cite ltx_citemacro_citep">(Mukherjee et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib34" title="">2025</a>; Yue et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib60" title="">2025a</a>; Wen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib53" title="">2025</a>; Huan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib17" title="">2025</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">External Verifiers for LLM Reasoning</span> Training external verifiers to identify the correctness of the LLM-generated solutions is an effective way to enhance the reasoning performance of LLMs in the inference time. External verifiers usually fall into two categories: (1) <span class="ltx_text ltx_font_bold">Scalar Reward Models</span>: Outcome-supervised Reward Models (ORMs) <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib5" title="">2021</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib54" title="">2024</a>)</cite> and Process-supervised Reward Models (PRMs) <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib23" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib49" title="">2024a</a>; Skywork-o1, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib44" title="">2024</a>; Yuan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib59" title="">2024</a>)</cite> are two representative approaches. ORMs provide supervision by evaluating the final answer, while PRMs offer more fine-grained feedback by assessing the intermediate reasoning steps.
(2) <span class="ltx_text ltx_font_bold">Generative Verifiers</span>: Recent studies have explored the potential of training LLMs to perform natural language critiques of reasoning solutions generated by the LLM generators, and then to judge their final outcomes <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib63" title="">2024</a>; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib9" title="">2024</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib56" title="">2025b</a>; Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib65" title="">2025</a>)</cite>. This paradigm has demonstrated stronger verification performance than scalar reward models, as it enables the LLM verifier to conduct deliberate chain-of-thought reasoning before arriving at the final judgment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Self-Verification for LLM Reasoning</span>
Several recent studies <cite class="ltx_cite ltx_citemacro_citep">(Sareen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib40" title="">2025</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib27" title="">2025a</a>; Zha et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib62" title="">2025</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib20" title="">2025</a>; Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib29" title="">2025</a>)</cite> aim to unify the roles of generator and verifier by equipping a single policy model with self-verification capability. The trained self-verification capability can be used in both the RL training and inference-time scaling stages to enhance the model performance. However, these approaches require generating solutions and self-verifications sequentially during training and inference. In contrast, our method derives the self-rewarding signal directly from the next-token probability distribution of the final token of the generated sequence, achieving a more efficient and effective unification of generation and self-verification.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminaries</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">RL Objective</span> We denote <math alttext="\pi_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m1" intent=":literal"><semantics><msub><mi>π</mi><mi>𝜽</mi></msub><annotation encoding="application/x-tex">\pi_{\bm{\theta}}</annotation></semantics></math> as the target policy model to be optimized, and <math alttext="\pi_{\text{ref}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m2" intent=":literal"><semantics><msub><mi>π</mi><mtext>ref</mtext></msub><annotation encoding="application/x-tex">\pi_{\text{ref}}</annotation></semantics></math> as the reference model from which <math alttext="\pi_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m3" intent=":literal"><semantics><msub><mi>π</mi><mi>𝜽</mi></msub><annotation encoding="application/x-tex">\pi_{\bm{\theta}}</annotation></semantics></math> is initialized. <math alttext="D" class="ltx_Math" display="inline" id="S3.SS1.p1.m4" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> is the query set, <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.m5" intent=":literal"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation></semantics></math> is an input and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.m6" intent=":literal"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation></semantics></math> is the generated response to <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.m7" intent=":literal"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation></semantics></math>. The standard optimization objective of RL is formalized as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{O}_{\pi_{\bm{\theta}}}=\max_{\pi_{\bm{\theta}}}\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{\bm{\theta}}(\cdot|x)}\left[r(\bm{x},\bm{y})-\beta\mathcal{D}_{\text{KL}}(\pi_{\bm{\theta}}||\pi_{ref})\right]," class="ltx_math_unparsed" display="inline" id="S3.E1X.m2" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒪</mi><msub><mi>π</mi><mi>𝜽</mi></msub></msub><mo>=</mo><munder><mi>max</mi><msub><mi>π</mi><mi>𝜽</mi></msub></munder><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>∼</mo><mi>D</mi><mo>,</mo><mi>𝒚</mi><mo>∼</mo><msub><mi>π</mi><mi>𝜽</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mrow><mo>[</mo><mi>r</mi><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><mo>−</mo><mi>β</mi><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>KL</mtext></msub><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mi>𝜽</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{O}_{\pi_{\bm{\theta}}}=\max_{\pi_{\bm{\theta}}}\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{\bm{\theta}}(\cdot|x)}\left[r(\bm{x},\bm{y})-\beta\mathcal{D}_{\text{KL}}(\pi_{\bm{\theta}}||\pi_{ref})\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="r(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS1.p1.m8" intent=":literal"><semantics><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">r(\bm{x},\bm{y})</annotation></semantics></math> represents a reward function to score the response <math alttext="y" class="ltx_Math" display="inline" id="S3.SS1.p1.m9" intent=":literal"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> given <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p1.m10" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, <math alttext="\mathcal{D}_{\text{KL}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m11" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>KL</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{KL}}</annotation></semantics></math> is the Kullback–Leibler (KL) divergence loss regularizing the distance between two model distributions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">RLVR</span> Recently, RLVR <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib11" title="">2025</a>; Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib16" title="">2025</a>)</cite> has emerged as a widely adopted and effective paradigm for enhancing the reasoning capabilities of LLMs. In RLVR, the reward function <math alttext="r" class="ltx_Math" display="inline" id="S3.SS1.p2.m1" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> is typically chosen as a deterministic verifier <math alttext="r_{v}" class="ltx_Math" display="inline" id="S3.SS1.p2.m2" intent=":literal"><semantics><msub><mi>r</mi><mi>v</mi></msub><annotation encoding="application/x-tex">r_{v}</annotation></semantics></math>, such as a rule-based verifier, to evaluate whether the final extracted answer <math alttext="\bm{a}\subset\bm{y}" class="ltx_Math" display="inline" id="S3.SS1.p2.m3" intent=":literal"><semantics><mrow><mi>𝒂</mi><mo>⊂</mo><mi>𝒚</mi></mrow><annotation encoding="application/x-tex">\bm{a}\subset\bm{y}</annotation></semantics></math> matches the ground-truth answer <math alttext="\bm{a}^{*}" class="ltx_Math" display="inline" id="S3.SS1.p2.m4" intent=":literal"><semantics><msup><mi>𝒂</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">\bm{a}^{*}</annotation></semantics></math>, and to produce binary feedback (e.g., {0,1}). That is,</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle r_{v}(\bm{x},\bm{y})=\mathds{1}_{\{\bm{a}\equiv\bm{a}^{*}\}}=" class="ltx_Math" display="inline" id="S3.E2X.m2" intent=":literal"><semantics><mrow><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msub><mn>𝟙</mn><mrow><mo stretchy="false">{</mo><mrow><mi>𝒂</mi><mo>≡</mo><msup><mi>𝒂</mi><mo>∗</mo></msup></mrow><mo stretchy="false">}</mo></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mn>1</mn></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mtext>if </mtext><mi>𝒂</mi><mtext> is semantically equivalent to </mtext><msup><mi>𝒂</mi><mo>∗</mo></msup></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mn>0</mn></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtext>otherwise</mtext><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\displaystyle r_{v}(\bm{x},\bm{y})=\mathds{1}_{\{\bm{a}\equiv\bm{a}^{*}\}}=</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Policy Gradient Method</span> Policy Gradient <cite class="ltx_cite ltx_citemacro_citep">(Sutton et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib46" title="">1998</a>)</cite> is a widely adopted algorithm to optimize the objective of Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.E1" title="Equation 1 ‣ 3.1 Preliminaries ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">1</span></a>), which updates the policy model via the estimated gradient</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E3">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E3X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\nabla_{\bm{\theta}}\mathcal{O}_{\pi_{\bm{\theta}}}=\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{\bm{\theta}}(\cdot|x)}\left[\sum_{t=1}^{T}A_{t}\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(y_{t}|\bm{x},\bm{y}_{&lt;t})\right]," class="ltx_math_unparsed" display="inline" id="S3.E3X.m2" intent=":literal"><semantics><mrow><mrow><mrow><msub><mo>∇</mo><mi>𝜽</mi></msub><msub><mi class="ltx_font_mathcaligraphic">𝒪</mi><msub><mi>π</mi><mi>𝜽</mi></msub></msub></mrow><mo>=</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>∼</mo><mi>D</mi><mo>,</mo><mi>𝒚</mi><mo>∼</mo><msub><mi>π</mi><mi>𝜽</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover></mstyle><mrow><msub><mi>A</mi><mi>t</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>𝜽</mi></msub><mi>log</mi></mrow><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mi>𝜽</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒚</mi><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\nabla_{\bm{\theta}}\mathcal{O}_{\pi_{\bm{\theta}}}=\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{\bm{\theta}}(\cdot|x)}\left[\sum_{t=1}^{T}A_{t}\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(y_{t}|\bm{x},\bm{y}_{&lt;t})\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="A_{t}" class="ltx_Math" display="inline" id="S3.SS1.p3.m1" intent=":literal"><semantics><msub><mi>A</mi><mi>t</mi></msub><annotation encoding="application/x-tex">A_{t}</annotation></semantics></math> is the <span class="ltx_text ltx_font_italic">advantage function</span> measuring the relative value of the action <math alttext="a_{t}" class="ltx_Math" display="inline" id="S3.SS1.p3.m2" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> (i.e., token <math alttext="y_{t}" class="ltx_Math" display="inline" id="S3.SS1.p3.m3" intent=":literal"><semantics><msub><mi>y</mi><mi>t</mi></msub><annotation encoding="application/x-tex">y_{t}</annotation></semantics></math>) compared to the baseline value under state <math alttext="s_{t}" class="ltx_Math" display="inline" id="S3.SS1.p3.m4" intent=":literal"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_{t}</annotation></semantics></math> (i.e., sequence <math alttext="(\bm{x},\bm{y}_{&lt;t})" class="ltx_Math" display="inline" id="S3.SS1.p3.m5" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><msub><mi>𝒚</mi><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y}_{&lt;t})</annotation></semantics></math>). In practice, <math alttext="A_{t}" class="ltx_Math" display="inline" id="S3.SS1.p3.m6" intent=":literal"><semantics><msub><mi>A</mi><mi>t</mi></msub><annotation encoding="application/x-tex">A_{t}</annotation></semantics></math> can be estimated in various ways <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib41" title="">2017</a>; Ahmadian et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib2" title="">2024</a>)</cite>. For example,
Group Relative Policy Optimization (GRPO) <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib42" title="">2024</a>)</cite> estimates the baseline value as the average reward within a sampled group <math alttext="\{\bm{y}^{1},\cdots,\bm{y}^{K}\}" class="ltx_Math" display="inline" id="S3.SS1.p3.m7" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msup><mi>𝒚</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msup><mi>𝒚</mi><mi>K</mi></msup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\bm{y}^{1},\cdots,\bm{y}^{K}\}</annotation></semantics></math> for the same problem, and computes the relative advantage for each token <math alttext="y_{t}^{i}" class="ltx_Math" display="inline" id="S3.SS1.p3.m8" intent=":literal"><semantics><msubsup><mi>y</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding="application/x-tex">y_{t}^{i}</annotation></semantics></math> in sequence <math alttext="\bm{y}^{i}" class="ltx_Math" display="inline" id="S3.SS1.p3.m9" intent=":literal"><semantics><msup><mi>𝒚</mi><mi>i</mi></msup><annotation encoding="application/x-tex">\bm{y}^{i}</annotation></semantics></math> as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E4">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E4X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle A_{t}^{i}=(r_{v}^{i}-\text{mean}(r_{v}^{1},\cdots,r_{v}^{K}))/\text{std}(r_{v}^{1},\cdots,r_{v}^{K}),\quad r_{v}^{i}=r_{v}(\bm{x},\bm{y}^{i})." class="ltx_Math" display="inline" id="S3.E4X.m2" intent=":literal"><semantics><mrow><mrow><mrow><msubsup><mi>A</mi><mi>t</mi><mi>i</mi></msubsup><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>r</mi><mi>v</mi><mi>i</mi></msubsup><mo>−</mo><mrow><mtext>mean</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>r</mi><mi>v</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msubsup><mi>r</mi><mi>v</mi><mi>K</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>/</mo><mtext>std</mtext></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>r</mi><mi>v</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msubsup><mi>r</mi><mi>v</mi><mi>K</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msubsup><mi>r</mi><mi>v</mi><mi>i</mi></msubsup><mo>=</mo><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><msup><mi>𝒚</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle A_{t}^{i}=(r_{v}^{i}-\text{mean}(r_{v}^{1},\cdots,r_{v}^{K}))/\text{std}(r_{v}^{1},\cdots,r_{v}^{K}),\quad r_{v}^{i}=r_{v}(\bm{x},\bm{y}^{i}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Implicit Reward</span> Previous studies <cite class="ltx_cite ltx_citemacro_citep">(Rafailov et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib38" title="">2023</a>; Peters &amp; Schaal, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib36" title="">2007</a>)</cite> have identified that the optimal solution to the objective Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.E1" title="Equation 1 ‣ 3.1 Preliminaries ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">1</span></a>) satisfies that</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E5">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E5X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle r_{v}(\bm{x},\bm{y})=\beta\log[\pi_{\bm{\theta}}(\bm{y}|\bm{x})/\pi_{ref}(\bm{y}|\bm{x})]+\beta\log Z(\bm{x})," class="ltx_Math" display="inline" id="S3.E5X.m2" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>β</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo fence="false">|</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo fence="false">|</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>β</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>Z</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle r_{v}(\bm{x},\bm{y})=\beta\log[\pi_{\bm{\theta}}(\bm{y}|\bm{x})/\pi_{ref}(\bm{y}|\bm{x})]+\beta\log Z(\bm{x}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(5)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="Z(\bm{x})=\sum_{\bm{y}}\pi_{ref}(\bm{y}|\bm{x})\exp(\frac{1}{\beta}r_{v}(\bm{x},\bm{y}))" class="ltx_Math" display="inline" id="S3.SS1.p4.m1" intent=":literal"><semantics><mrow><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><msub><mo>∑</mo><mi>𝒚</mi></msub><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo fence="false">|</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mfrac><mn>1</mn><mi>β</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">Z(\bm{x})=\sum_{\bm{y}}\pi_{ref}(\bm{y}|\bm{x})\exp(\frac{1}{\beta}r_{v}(\bm{x},\bm{y}))</annotation></semantics></math> is a partition function. <math alttext="\beta\log\frac{\pi_{\bm{\theta}}(\bm{y}|\bm{x})}{\pi_{ref}(\bm{y}|\bm{x})}" class="ltx_Math" display="inline" id="S3.SS1.p4.m2" intent=":literal"><semantics><mrow><mi>β</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo fence="false">|</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo fence="false">|</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow><annotation encoding="application/x-tex">\beta\log\frac{\pi_{\bm{\theta}}(\bm{y}|\bm{x})}{\pi_{ref}(\bm{y}|\bm{x})}</annotation></semantics></math> is termed as the <span class="ltx_text ltx_font_italic">implicit reward</span>, which has been used in prior works <cite class="ltx_cite ltx_citemacro_citep">(Mitchell et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib33" title="">2024</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib25" title="">2024b</a>)</cite> to analyze the behavioral shift induced by the alignment process.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>LaSeR: Reinforcement Learning with Last-Token Self-Rewarding</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Formal Formulation</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p1">
<p class="ltx_p">In training, ground-truth answers can be reliably used to determine the correctness of solutions. At test time, however, when ground-truth answers are unavailable, the use of verifiers becomes crucial for evaluating solution quality and providing feedback signals. To address this problem, in this work, we explore the promising paradigm of jointly optimizing the self-verification and reasoning capabilities of LLMs within the RLVR framework, thereby enabling them not only to produce high-quality reasoning paths but also to evaluate their own outputs at test time.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p2">
<p class="ltx_p">According to Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.E5" title="Equation 5 ‣ 3.1 Preliminaries ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">5</span></a>), as <math alttext="Z(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.m1" intent=":literal"><semantics><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Z(\bm{x})</annotation></semantics></math> remains the same for all <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.m2" intent=":literal"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation></semantics></math>, a straight-forward idea is to utilize the implicit reward <math alttext="\beta\log\frac{\pi_{\bm{\theta}}(\bm{y}|\bm{x})}{\pi_{ref}(\bm{y}|\bm{x})}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.m3" intent=":literal"><semantics><mrow><mi>β</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo fence="false">|</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo fence="false">|</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow><annotation encoding="application/x-tex">\beta\log\frac{\pi_{\bm{\theta}}(\bm{y}|\bm{x})}{\pi_{ref}(\bm{y}|\bm{x})}</annotation></semantics></math> as the indicator to rank different generations at test time. However, this approach has a critical drawback: the absolute value of the implicit reward is <span class="ltx_text ltx_font_bold">length-biased</span>, since the absolute value of <math alttext="\beta\log\frac{\pi_{\bm{\theta}}(\bm{y}|\bm{x})}{\pi_{ref}(\bm{y}|\bm{x})}=\beta\sum_{i}\log\frac{\pi_{\bm{\theta}}(y_{i}|\bm{x},\bm{y}_{&lt;i})}{\pi_{ref}(y_{i}|\bm{x},,\bm{y}_{&lt;i})}" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.m4" intent=":literal"><semantics><mrow><mrow><mi>β</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo fence="false">|</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒚</mi><mo fence="false">|</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow><mo>=</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><mrow><msub><mo>∑</mo><mi>i</mi></msub><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒚</mi><mrow><mi></mi><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>𝒙</mi><mo>,</mo><mo>,</mo><msub><mi>𝒚</mi><mrow><mi></mi><mo>&lt;</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\beta\log\frac{\pi_{\bm{\theta}}(\bm{y}|\bm{x})}{\pi_{ref}(\bm{y}|\bm{x})}=\beta\sum_{i}\log\frac{\pi_{\bm{\theta}}(y_{i}|\bm{x},\bm{y}_{&lt;i})}{\pi_{ref}(y_{i}|\bm{x},,\bm{y}_{&lt;i})}</annotation></semantics></math> increases proportionally with the response length. In reasoning tasks, the incorrect solutions are usually longer than the correct solutions <cite class="ltx_cite ltx_citemacro_citep">(Hassid et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib12" title="">2025</a>)</cite>, making the implicit reward unreliable in evaluating solution correctness (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A1" title="Appendix A The Length Bias in Implicit Reward ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">A</span></a>). Furthermore, disregarding <math alttext="Z(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.m5" intent=":literal"><semantics><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Z(\bm{x})</annotation></semantics></math> and directly aligning the implicit reward with the true reasoning reward during training degrades the policy model’s generation ability <cite class="ltx_cite ltx_citemacro_citep">(Cui et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib6" title="">2025</a>)</cite>, since a fundamental gap (i.e., <math alttext="\beta\log Z(\bm{x})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.m6" intent=":literal"><semantics><mrow><mi>β</mi><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>Z</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\beta\log Z(\bm{x})</annotation></semantics></math>) exists between the solution to RLVR and that to reward modeling.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p3">
<p class="ltx_p">In this work, we begin by formulating our approach from the RL objective of verification. Given a problem <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.m1" intent=":literal"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation></semantics></math>, and a candidate solution <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.m2" intent=":literal"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation></semantics></math>, the model is required to produce a verification <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.m3" intent=":literal"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation></semantics></math> to identify the correctness of the solution: <math alttext="\bm{z}\sim\pi_{\bm{\theta}}(\cdot|\bm{x},\bm{y})" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p3.m4" intent=":literal"><semantics><mrow><mi>𝒛</mi><mo>∼</mo><msub><mi>π</mi><mi>𝜽</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{z}\sim\pi_{\bm{\theta}}(\cdot|\bm{x},\bm{y})</annotation></semantics></math>. Thus, the RL objective of verification can be written as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E6">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E6X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{V}_{\pi_{\bm{\theta}}}=\max_{\pi_{\bm{\theta}}}\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{g}(\cdot|x),\bm{z}\sim\pi_{\bm{\theta}}(\cdot|\bm{x},\bm{y})}\left[\hat{r}(\bm{x},\bm{y},\bm{z})-\beta_{v}\mathcal{D}_{\text{KL}}(\pi_{\bm{\theta}}||\pi_{ref})\right]," class="ltx_math_unparsed" display="inline" id="S3.E6X.m2" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒱</mi><msub><mi>π</mi><mi>𝜽</mi></msub></msub><mo>=</mo><munder><mi>max</mi><msub><mi>π</mi><mi>𝜽</mi></msub></munder><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>∼</mo><mi>D</mi><mo>,</mo><mi>𝒚</mi><mo>∼</mo><msub><mi>π</mi><mi>g</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>𝒛</mi><mo>∼</mo><msub><mi>π</mi><mi>𝜽</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mrow><mo>[</mo><mover accent="true"><mi>r</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow><mo>−</mo><msub><mi>β</mi><mi>v</mi></msub><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>KL</mtext></msub><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mi>𝜽</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{V}_{\pi_{\bm{\theta}}}=\max_{\pi_{\bm{\theta}}}\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{g}(\cdot|x),\bm{z}\sim\pi_{\bm{\theta}}(\cdot|\bm{x},\bm{y})}\left[\hat{r}(\bm{x},\bm{y},\bm{z})-\beta_{v}\mathcal{D}_{\text{KL}}(\pi_{\bm{\theta}}||\pi_{ref})\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(6)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="\pi_{g}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.m5" intent=":literal"><semantics><msub><mi>π</mi><mi>g</mi></msub><annotation encoding="application/x-tex">\pi_{g}</annotation></semantics></math> is the generator to solve the problem (can also be the target model <math alttext="\pi_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.m6" intent=":literal"><semantics><msub><mi>π</mi><mi>𝜽</mi></msub><annotation encoding="application/x-tex">\pi_{\bm{\theta}}</annotation></semantics></math> itself in the self-verification setting), <math alttext="\hat{r}(\bm{x},\bm{y},\bm{z})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.m7" intent=":literal"><semantics><mrow><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{r}(\bm{x},\bm{y},\bm{z})</annotation></semantics></math> is the verification reward that measures the consistency between the true correctness of <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.m8" intent=":literal"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation></semantics></math> and the verification result of <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.m9" intent=":literal"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E7">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E7X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{r}(\bm{x},\bm{y},\bm{z})=" class="ltx_Math" display="inline" id="S3.E7X.m2" intent=":literal"><semantics><mrow><mrow><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mn>1</mn></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mtext>if verification result of </mtext><mi>𝒛</mi><mtext> matches the true correctness of </mtext><mi>𝒚</mi></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mn>0</mn></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtext>otherwise</mtext><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\hat{r}(\bm{x},\bm{y},\bm{z})=</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(7)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">In practice, <math alttext="\bm{z}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.m10" intent=":literal"><semantics><mi>𝒛</mi><annotation encoding="application/x-tex">\bm{z}</annotation></semantics></math> can be either a single token—for instance, “<span class="ltx_text ltx_font_typewriter">Yes</span>” or “<span class="ltx_text ltx_font_typewriter">No</span>” to directly indicate whether the solution is verified as correct or incorrect—or a sequence that includes both a chain of thought and the final judgment. In this work, we focus on the former setting and simplify the ground-truth label space to two single tokens <math alttext="z_{c}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.m11" intent=":literal"><semantics><msub><mi>z</mi><mi>c</mi></msub><annotation encoding="application/x-tex">z_{c}</annotation></semantics></math> (e.g., “<span class="ltx_text ltx_font_typewriter">Yes</span>”) and <math alttext="z_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p3.m12" intent=":literal"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_{i}</annotation></semantics></math> (e.g., “<span class="ltx_text ltx_font_typewriter">No</span>”). That is, the verification reward is simplified to</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E8">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E8X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{r}(\bm{x},\bm{y},\bm{z})=" class="ltx_Math" display="inline" id="S3.E8X.m2" intent=":literal"><semantics><mrow><mrow><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mn>1</mn></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>=</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo lspace="0em" rspace="0em">​</mo><mtext> and </mtext><mo lspace="0em" rspace="0em">​</mo><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mtext> or </mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo>=</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mtext> and </mtext><mo lspace="0em" rspace="0em">​</mo><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mn>0</mn></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtext>otherwise</mtext><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\hat{r}(\bm{x},\bm{y},\bm{z})=</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(8)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p4">
<p class="ltx_p">Similarly, following from Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.E5" title="Equation 5 ‣ 3.1 Preliminaries ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">5</span></a>), the close-form solution to Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.E6" title="Equation 6 ‣ 3.2.1 Formal Formulation ‣ 3.2 LaSeR: Reinforcement Learning with Last-Token Self-Rewarding ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">6</span></a>) can be written as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E9">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E9X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{r}(\bm{x},\bm{y},\bm{z})=\beta_{v}\log\frac{\pi_{\bm{\theta}}(\bm{z}|\bm{x},\bm{y})}{\pi_{ref}(\bm{z}|\bm{x},\bm{y})}+\beta_{v}\log Z(\bm{x},\bm{y}),\quad Z(\bm{x},\bm{y})=\sum_{\bm{z}}\pi_{ref}(\bm{z}|\bm{x},\bm{y})\exp(\frac{1}{\beta_{v}}\hat{r}(\bm{x},\bm{y},\bm{z}))." class="ltx_Math" display="inline" id="S3.E9X.m2" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>+</mo><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>Z</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>𝒛</mi></munder></mstyle><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msub><mi>β</mi><mi>v</mi></msub></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\hat{r}(\bm{x},\bm{y},\bm{z})=\beta_{v}\log\frac{\pi_{\bm{\theta}}(\bm{z}|\bm{x},\bm{y})}{\pi_{ref}(\bm{z}|\bm{x},\bm{y})}+\beta_{v}\log Z(\bm{x},\bm{y}),\quad Z(\bm{x},\bm{y})=\sum_{\bm{z}}\pi_{ref}(\bm{z}|\bm{x},\bm{y})\exp(\frac{1}{\beta_{v}}\hat{r}(\bm{x},\bm{y},\bm{z})).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(9)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p5">
<p class="ltx_p">Now, let’s take a closer look at <math alttext="Z(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.m1" intent=":literal"><semantics><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Z(\bm{x},\bm{y})</annotation></semantics></math>. First, <span class="ltx_text ltx_font_bold">for <math alttext="\bm{z}\in\{z_{c},z_{i}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.m2" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><msub><mi>z</mi><mi>c</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{z}\in\{z_{c},z_{i}\}</annotation></semantics></math>, <math alttext="\pi_{ref}(\bm{z}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.m3" intent=":literal"><semantics><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>z</mi><mo fence="false">|</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{ref}(\bm{z}|\bm{x},\bm{y})</annotation></semantics></math> is a extremely small positive value for any problem-solution pair <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.m4" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation></semantics></math>, i.e., <math alttext="\pi_{ref}(\bm{z}|\bm{x},\bm{y})\approx 0" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.m5" intent=":literal"><semantics><mrow><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>z</mi><mo fence="false">|</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\pi_{ref}(\bm{z}|\bm{x},\bm{y})\approx 0</annotation></semantics></math>, for <math alttext="\bm{z}\in\{z_{c},z_{i}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.m6" intent=":literal"><semantics><mrow><mi>z</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><msub><mi>z</mi><mi>c</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\bm{z}\in\{z_{c},z_{i}\}</annotation></semantics></math></span>. The reason is that the model is not specifically optimized for predicting the next token once it completes the generation and produces the final token (typically the “<span class="ltx_text ltx_font_typewriter">&lt;EOS&gt;</span>” token). We present a numerical analysis to validate this claim in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A1.F11" title="Figure 11 ‣ Appendix A The Length Bias in Implicit Reward ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">11</span></a>, and we can see <span class="ltx_text ltx_font_bold">the value of <math alttext="\pi_{ref}(z|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.m7" intent=":literal"><semantics><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>z</mi><mo fence="false">|</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{ref}(z|\bm{x},\bm{y})</annotation></semantics></math> is less than <math alttext="e^{-9}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.m8" intent=":literal"><semantics><msup><mi>e</mi><mrow><mo>−</mo><mn>9</mn></mrow></msup><annotation encoding="application/x-tex">e^{-9}</annotation></semantics></math> for common tokens and even less than <math alttext="e^{-20}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.m9" intent=":literal"><semantics><msup><mi>e</mi><mrow><mo>−</mo><mn>20</mn></mrow></msup><annotation encoding="application/x-tex">e^{-20}</annotation></semantics></math> for unused special tokens</span>. Then, we can get that</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E10">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E10X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle Z(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.E10X.m2" intent=":literal"><semantics><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle Z(\bm{x},\bm{y})</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum_{\bm{z}}\pi_{ref}(\bm{z}|\bm{x},\bm{y})\exp(\frac{1}{\beta_{v}}\hat{r}(\bm{x},\bm{y},\bm{z}))=\sum_{\bm{z}\notin\{z_{c},z_{i}\}}\pi_{ref}(\bm{z}|\bm{x},\bm{y})\exp(\frac{1}{\beta_{v}}\hat{r}(\bm{x},\bm{y},\bm{z}))" class="ltx_Math" display="inline" id="S3.E10X.m3" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>𝒛</mi></munder></mstyle><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msub><mi>β</mi><mi>v</mi></msub></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mrow><mi>𝒛</mi><mo>∉</mo><mrow><mo stretchy="false">{</mo><msub><mi>z</mi><mi>c</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow></mrow></munder></mstyle><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msub><mi>β</mi><mi>v</mi></msub></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\sum_{\bm{z}}\pi_{ref}(\bm{z}|\bm{x},\bm{y})\exp(\frac{1}{\beta_{v}}\hat{r}(\bm{x},\bm{y},\bm{z}))=\sum_{\bm{z}\notin\{z_{c},z_{i}\}}\pi_{ref}(\bm{z}|\bm{x},\bm{y})\exp(\frac{1}{\beta_{v}}\hat{r}(\bm{x},\bm{y},\bm{z}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="4"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(10)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E10Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle+\pi_{ref}(z_{c}|\bm{x},\bm{y})\exp(\frac{1}{\beta_{v}}\hat{r}(\bm{x},\bm{y},z_{c}))+\pi_{ref}(z_{i}|\bm{x},\bm{y})\exp(\frac{1}{\beta_{v}}\hat{r}(\bm{x},\bm{y},z_{i}))" class="ltx_Math" display="inline" id="S3.E10Xa.m2" intent=":literal"><semantics><mrow><mrow><mo>+</mo><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msub><mi>β</mi><mi>v</mi></msub></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><msub><mi>z</mi><mi>c</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>+</mo><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><msub><mi>β</mi><mi>v</mi></msub></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle+\pi_{ref}(z_{c}|\bm{x},\bm{y})\exp(\frac{1}{\beta_{v}}\hat{r}(\bm{x},\bm{y},z_{c}))+\pi_{ref}(z_{i}|\bm{x},\bm{y})\exp(\frac{1}{\beta_{v}}\hat{r}(\bm{x},\bm{y},z_{i}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E10Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(1-\pi_{ref}(z_{c}|\bm{x},\bm{y})-\pi_{ref}(z_{i}|\bm{x},\bm{y}))\exp(0)+(\pi_{ref}(z_{c}|\bm{x},\bm{y})+\pi_{ref}(z_{i}|\bm{x},\bm{y}))\exp(\frac{1}{\beta_{v}})" class="ltx_Math" display="inline" id="S3.E10Xb.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><msub><mi>β</mi><mi>v</mi></msub></mfrac></mstyle><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=(1-\pi_{ref}(z_{c}|\bm{x},\bm{y})-\pi_{ref}(z_{i}|\bm{x},\bm{y}))\exp(0)+(\pi_{ref}(z_{c}|\bm{x},\bm{y})+\pi_{ref}(z_{i}|\bm{x},\bm{y}))\exp(\frac{1}{\beta_{v}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E10Xc">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\approx 1\times 1+0\times\exp(\frac{1}{\beta_{v}})=1\implies\log Z(\bm{x},\bm{y})\approx 0." class="ltx_Math" display="inline" id="S3.E10Xc.m2" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>≈</mo><mrow><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow><mo>+</mo><mrow><mn>0</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><msub><mi>β</mi><mi>v</mi></msub></mfrac></mstyle><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>=</mo><mn>1</mn><mo stretchy="false">⟹</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>Z</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><mn>0</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\approx 1\times 1+0\times\exp(\frac{1}{\beta_{v}})=1\implies\log Z(\bm{x},\bm{y})\approx 0.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">The above analysis reveals that, under our formulation, the partition function that cannot be ignored by previous studies <cite class="ltx_cite ltx_citemacro_citep">(Cui et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib6" title="">2025</a>)</cite> can be now naturally discarded. Consequently, the optimal solution to Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.E6" title="Equation 6 ‣ 3.2.1 Formal Formulation ‣ 3.2 LaSeR: Reinforcement Learning with Last-Token Self-Rewarding ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">6</span></a>) can be <span class="ltx_text ltx_font_bold">approximately</span> reduced to:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E11">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E11X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{r}(\bm{x},\bm{y},\bm{z})=\beta_{v}\log[\pi_{\bm{\theta}}(\bm{z}|\bm{x},\bm{y})/\pi_{ref}(\bm{z}|\bm{x},\bm{y})]." class="ltx_Math" display="inline" id="S3.E11X.m2" intent=":literal"><semantics><mrow><mrow><mrow><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><mi>𝒛</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>𝒛</mi><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\hat{r}(\bm{x},\bm{y},\bm{z})=\beta_{v}\log[\pi_{\bm{\theta}}(\bm{z}|\bm{x},\bm{y})/\pi_{ref}(\bm{z}|\bm{x},\bm{y})].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(11)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">In particular, the true verification reward when the model verifies a solution as correct is:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E12">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E12X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{r}(\bm{x},\bm{y},z_{c})=r_{v}(\bm{x},\bm{y})=\beta_{v}\log[\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})/\pi_{ref}(z_{c}|\bm{x},\bm{y})]." class="ltx_Math" display="inline" id="S3.E12X.m2" intent=":literal"><semantics><mrow><mrow><mrow><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><msub><mi>z</mi><mi>c</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\hat{r}(\bm{x},\bm{y},z_{c})=r_{v}(\bm{x},\bm{y})=\beta_{v}\log[\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})/\pi_{ref}(z_{c}|\bm{x},\bm{y})].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(12)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">The first equation is derived from the definition in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.E8" title="Equation 8 ‣ 3.2.1 Formal Formulation ‣ 3.2 LaSeR: Reinforcement Learning with Last-Token Self-Rewarding ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">8</span></a>). The second equation reveals that <span class="ltx_text ltx_font_bold">the true reasoning reward is equal to log-probability ratio of the policy model to the reference model at
<math alttext="z_{c}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p5.m10" intent=":literal"><semantics><msub><mi>z</mi><mi>c</mi></msub><annotation encoding="application/x-tex">z_{c}</annotation></semantics></math>, scaled by the KL coefficient</span>. Thus, to optimize the model’s verification capability, we do not need to explicitly perform a RLVR procedure. Instead, we can directly optimize the following MSE loss:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E13">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E13X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle L=\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{g}(\cdot|x)}\left(\beta_{v}\log[\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})/\pi_{ref}(z_{c}|\bm{x},\bm{y})]-r_{v}(\bm{x},\bm{y})\right)^{2}." class="ltx_math_unparsed" display="inline" id="S3.E13X.m2" intent=":literal"><semantics><mrow><mrow><mi>L</mi><mo>=</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>∼</mo><mi>D</mi><mo>,</mo><mi>𝒚</mi><mo>∼</mo><msub><mi>π</mi><mi>g</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>(</mo><mrow><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>/</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><mo>−</mo><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle L=\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{g}(\cdot|x)}\left(\beta_{v}\log[\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})/\pi_{ref}(z_{c}|\bm{x},\bm{y})]-r_{v}(\bm{x},\bm{y})\right)^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(13)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p6">
<p class="ltx_p">Thus, in the self-verification setting where <math alttext="\pi_{g}=\pi_{\bm{\theta}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.m1" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>g</mi></msub><mo>=</mo><msub><mi>π</mi><mi>𝜽</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{g}=\pi_{\bm{\theta}}</annotation></semantics></math>, we can directly adds the above loss into the original RLVR loss to jointly optimize the reasoning and self-verification capabilities of the policy model:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E14">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E14X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{S}_{\pi_{\bm{\theta}}}=\max_{\pi_{\bm{\theta}}}\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{\bm{\theta}}(\cdot|x)}\left\{r_{v}(\bm{x},\bm{y})-\beta\mathcal{D}_{\text{KL}}(\pi_{\bm{\theta}}(\bm{y}|\bm{x})||\pi_{ref}(\bm{y}|\bm{x}))-\alpha\left[\beta_{v}\log\frac{\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})}{\pi_{ref}(z_{c}|\bm{x},\bm{y})}-r_{v}(\bm{x},\bm{y})\right]^{2}\right\}," class="ltx_math_unparsed" display="inline" id="S3.E14X.m2" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒮</mi><msub><mi>π</mi><mi>𝜽</mi></msub></msub><mo>=</mo><munder><mi>max</mi><msub><mi>π</mi><mi>𝜽</mi></msub></munder><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>∼</mo><mi>D</mi><mo>,</mo><mi>𝒚</mi><mo>∼</mo><msub><mi>π</mi><mi>𝜽</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mrow><mo>{</mo><msub><mi>r</mi><mi>v</mi></msub><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><mo>−</mo><mi>β</mi><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>KL</mtext></msub><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mi>𝜽</mi></msub><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mi>𝒚</mi><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>𝒙</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo>−</mo><mi>α</mi><msup><mrow><mo>[</mo><msub><mi>β</mi><mi>v</mi></msub><mi>log</mi><mstyle displaystyle="true"><mfrac><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>−</mo><msub><mi>r</mi><mi>v</mi></msub><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow><mn>2</mn></msup><mo>}</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{S}_{\pi_{\bm{\theta}}}=\max_{\pi_{\bm{\theta}}}\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{\bm{\theta}}(\cdot|x)}\left\{r_{v}(\bm{x},\bm{y})-\beta\mathcal{D}_{\text{KL}}(\pi_{\bm{\theta}}(\bm{y}|\bm{x})||\pi_{ref}(\bm{y}|\bm{x}))-\alpha\left[\beta_{v}\log\frac{\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})}{\pi_{ref}(z_{c}|\bm{x},\bm{y})}-r_{v}(\bm{x},\bm{y})\right]^{2}\right\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(14)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.m2" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> is a loss balancing coefficient. We refer the term <math alttext="r_{s}=\beta_{v}\log\frac{\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})}{\pi_{ref}(z_{c}|\bm{x},\bm{y})}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.m3" intent=":literal"><semantics><mrow><msub><mi>r</mi><mi>s</mi></msub><mo>=</mo><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow><annotation encoding="application/x-tex">r_{s}=\beta_{v}\log\frac{\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})}{\pi_{ref}(z_{c}|\bm{x},\bm{y})}</annotation></semantics></math> to the <span class="ltx_text ltx_font_bold">last-token self-rewarding score</span>, since it depends on the log-probability distributions of the last token in <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.m4" intent=":literal"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Other Techniques</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p">Here, we discuss several practical techniques to further simplify and improve the efficiency and effectiveness of the self-rewarding MSE loss introduced above.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Simplification of the Log-Probability in the Reference Model</span>
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A1.F11" title="Figure 11 ‣ Appendix A The Length Bias in Implicit Reward ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">11</span></a>, the quantity
<math alttext="\log\pi_{ref}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS3.p2.m1" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log\pi_{ref}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> remains almost constant, exhibiting only a negligible standard deviation across all <math alttext="\bm{x}" class="ltx_Math" display="inline" id="S3.SS3.p2.m2" intent=":literal"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS3.p2.m3" intent=":literal"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation></semantics></math>. Therefore, we can regard it as a pre-calculated constant <math alttext="c_{ref}" class="ltx_Math" display="inline" id="S3.SS3.p2.m4" intent=":literal"><semantics><msub><mi>c</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><annotation encoding="application/x-tex">c_{ref}</annotation></semantics></math> in calculating the last-token self-rewarding score during both training and inference. This eliminates the need for forwarding
<math alttext="\bm{y}" class="ltx_Math" display="inline" id="S3.SS3.p2.m5" intent=":literal"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation></semantics></math> through the reference model and thus further enhances efficiency. In specific, <math alttext="c_{ref}" class="ltx_Math" display="inline" id="S3.SS3.p2.m6" intent=":literal"><semantics><msub><mi>c</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><annotation encoding="application/x-tex">c_{ref}</annotation></semantics></math> is the mean value of <math alttext="\log\pi_{ref}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS3.p2.m7" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log\pi_{ref}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> on a small set of pre-generated set of <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS3.p2.m8" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation></semantics></math>. Furthermore, based on the findings in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A1.F11" title="Figure 11 ‣ Appendix A The Length Bias in Implicit Reward ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">11</span></a>, we select an unused special token as <math alttext="z_{c}" class="ltx_Math" display="inline" id="S3.SS3.p2.m9" intent=":literal"><semantics><msub><mi>z</mi><mi>c</mi></msub><annotation encoding="application/x-tex">z_{c}</annotation></semantics></math> to make <math alttext="\pi_{ref}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS3.p2.m10" intent=":literal"><semantics><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{ref}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> closer to 0 and to further minimize its impact on the approximation of <math alttext="Z(\bm{x},\bm{y})=1" class="ltx_Math" display="inline" id="S3.SS3.p2.m11" intent=":literal"><semantics><mrow><mrow><mi>Z</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">Z(\bm{x},\bm{y})=1</annotation></semantics></math> and the stability of training.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Self-Rewarding Loss Re-Weighting</span> During training, the numbers of correct and incorrect solutions are imbalanced, and their ratio dynamically changes. To prevent the last-token self-rewarding score from being biased toward the class with more samples, we apply a class-level loss re-weighting strategy within each optimization step. In each step, we calculate the total numbers of correct and incorrect solutions (identified by the deterministic verifier) for all problems in the current batch as <math alttext="N_{c}" class="ltx_Math" display="inline" id="S3.SS3.p3.m1" intent=":literal"><semantics><msub><mi>N</mi><mi>c</mi></msub><annotation encoding="application/x-tex">N_{c}</annotation></semantics></math> and <math alttext="N_{i}" class="ltx_Math" display="inline" id="S3.SS3.p3.m2" intent=":literal"><semantics><msub><mi>N</mi><mi>i</mi></msub><annotation encoding="application/x-tex">N_{i}</annotation></semantics></math>. Then, we apply the loss re-weighting as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E15">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E15X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle l=\frac{1}{N_{c}+N_{i}}\sum_{\bm{x}}\sum_{\bm{y}}\left[w_{c}\mathds{1}_{\{r_{v}(\bm{x},\bm{y})=1\}}+w_{i}\mathds{1}_{\{r_{v}(\bm{x},\bm{y})=0\}}\right]\left[\beta_{v}\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})-\beta_{v}c_{ref}-r_{v}(\bm{x},\bm{y})\right]^{2}," class="ltx_Math" display="inline" id="S3.E15X.m2" intent=":literal"><semantics><mrow><mrow><mi>l</mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><msub><mi>N</mi><mi>c</mi></msub><mo>+</mo><msub><mi>N</mi><mi>i</mi></msub></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>𝒙</mi></munder></mstyle><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>𝒚</mi></munder></mstyle><mrow><mrow><mo>[</mo><mrow><mrow><msub><mi>w</mi><mi>c</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mn>𝟙</mn><mrow><mo stretchy="false">{</mo><mrow><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><mo stretchy="false">}</mo></mrow></msub></mrow><mo>+</mo><mrow><msub><mi>w</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mn>𝟙</mn><mrow><mo stretchy="false">{</mo><mrow><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow><mo stretchy="false">}</mo></mrow></msub></mrow></mrow><mo>]</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msup><mrow><mo>[</mo><mrow><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mi>𝜽</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>c</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo>−</mo><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>]</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle l=\frac{1}{N_{c}+N_{i}}\sum_{\bm{x}}\sum_{\bm{y}}\left[w_{c}\mathds{1}_{\{r_{v}(\bm{x},\bm{y})=1\}}+w_{i}\mathds{1}_{\{r_{v}(\bm{x},\bm{y})=0\}}\right]\left[\beta_{v}\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})-\beta_{v}c_{ref}-r_{v}(\bm{x},\bm{y})\right]^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(15)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="w_{c}=\frac{N_{c}+N_{i}}{2\times N_{c}}" class="ltx_Math" display="inline" id="S3.SS3.p3.m3" intent=":literal"><semantics><mrow><msub><mi>w</mi><mi>c</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>N</mi><mi>c</mi></msub><mo>+</mo><msub><mi>N</mi><mi>i</mi></msub></mrow><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>N</mi><mi>c</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">w_{c}=\frac{N_{c}+N_{i}}{2\times N_{c}}</annotation></semantics></math> and <math alttext="w_{i}=\frac{N_{c}+N_{i}}{2\times N_{i}}" class="ltx_Math" display="inline" id="S3.SS3.p3.m4" intent=":literal"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>N</mi><mi>c</mi></msub><mo>+</mo><msub><mi>N</mi><mi>i</mi></msub></mrow><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msub><mi>N</mi><mi>i</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">w_{i}=\frac{N_{c}+N_{i}}{2\times N_{i}}</annotation></semantics></math> are re-weighting factors.
This practice achieves a more balanced self-verification capability. We provide empirical validations on this in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A5" title="Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">E</span></a>. Future work can explore more effective ways to address the issue of imbalanced distribution of solutions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Integration of Verifier-based and Self-Rewarding-based Advantages</span> The last-token self-rewarding scores can not only be used at test time, but also facilitate the training process through the integration of verifier-based and self-rewarding-based advantages. We believe such practice can help mitigate the issue of misjudgments by rule-based verifiers, which often occur when the format of ground-truth answer is overly complex, and produce more fine-grained rewards. For example, in GRPO, the final advantage can be calculated as:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E16">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E16X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{A}_{t}^{i}" class="ltx_Math" display="inline" id="S3.E16X.m2" intent=":literal"><semantics><msubsup><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>t</mi><mi>i</mi></msubsup><annotation encoding="application/x-tex">\displaystyle\hat{A}_{t}^{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(1-\tau)\frac{r_{v}^{i}-\text{mean}(r_{v}^{1},\cdots,r_{v}^{K})}{\text{std}(r_{v}^{1},\cdots,r_{v}^{K})}+\tau\frac{r_{s}^{i}-\text{mean}(r_{s}^{1},\cdots,r_{s}^{K})}{\text{std}(r_{s}^{1},\cdots,r_{s}^{K})}," class="ltx_Math" display="inline" id="S3.E16X.m3" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>τ</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mstyle displaystyle="true"><mfrac><mrow><msubsup><mi>r</mi><mi>v</mi><mi>i</mi></msubsup><mo>−</mo><mrow><mtext>mean</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>r</mi><mi>v</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msubsup><mi>r</mi><mi>v</mi><mi>K</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mrow><mtext>std</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>r</mi><mi>v</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msubsup><mi>r</mi><mi>v</mi><mi>K</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo>+</mo><mrow><mi>τ</mi><mo lspace="0em" rspace="0em">​</mo><mstyle displaystyle="true"><mfrac><mrow><msubsup><mi>r</mi><mi>s</mi><mi>i</mi></msubsup><mo>−</mo><mrow><mtext>mean</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>r</mi><mi>s</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msubsup><mi>r</mi><mi>s</mi><mi>K</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mrow><mtext>std</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>r</mi><mi>s</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msubsup><mi>r</mi><mi>s</mi><mi>K</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=(1-\tau)\frac{r_{v}^{i}-\text{mean}(r_{v}^{1},\cdots,r_{v}^{K})}{\text{std}(r_{v}^{1},\cdots,r_{v}^{K})}+\tau\frac{r_{s}^{i}-\text{mean}(r_{s}^{1},\cdots,r_{s}^{K})}{\text{std}(r_{s}^{1},\cdots,r_{s}^{K})},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(16)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E16Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\text{where }r_{v}^{i}=r_{v}(\bm{x},\bm{y}^{i})\text{ and }r_{s}^{i}=\beta_{v}\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y}^{i})-\beta_{v}c_{ref}." class="ltx_Math" display="inline" id="S3.E16Xa.m2" intent=":literal"><semantics><mrow><mrow><mrow><mtext>where </mtext><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>r</mi><mi>v</mi><mi>i</mi></msubsup></mrow><mo>=</mo><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><msup><mi>𝒚</mi><mi>i</mi></msup><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mtext> and </mtext><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>r</mi><mi>s</mi><mi>i</mi></msubsup></mrow><mo>=</mo><mrow><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mi>𝜽</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><msup><mi>𝒚</mi><mi>i</mi></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>c</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\text{where }r_{v}^{i}=r_{v}(\bm{x},\bm{y}^{i})\text{ and }r_{s}^{i}=\beta_{v}\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y}^{i})-\beta_{v}c_{ref}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">To stabilize training,
we adopt a filtering strategy that sets <math alttext="\tau=0" class="ltx_Math" display="inline" id="S3.SS3.p4.m1" intent=":literal"><semantics><mrow><mi>τ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\tau=0</annotation></semantics></math> for any group whenever the standard deviation <math alttext="\text{std}(r_{s}^{1},\cdots,r_{s}^{K})" class="ltx_Math" display="inline" id="S3.SS3.p4.m2" intent=":literal"><semantics><mrow><mtext>std</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>r</mi><mi>s</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msubsup><mi>r</mi><mi>s</mi><mi>K</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{std}(r_{s}^{1},\cdots,r_{s}^{K})</annotation></semantics></math>
within this group falls below a threshold <math alttext="T" class="ltx_Math" display="inline" id="S3.SS3.p4.m3" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>, which is set to 0.1.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Separate Warm-Up of Reasoning and Self-Rewarding Capabilities</span>
During the initial phase of training, we optimize only the last-token self-rewarding score, without integrating self-rewarding-based advantages into the learning process. After a certain steps when the last-token self-rewarding loss is sufficiently small, we proceed to integrate verifier-based and self-rewarding-based advantages.
Moreover, when training from base (i.e., pre-trained) models, we first perform standard RLVR without incorporating the last-token self-rewarding loss in order to warm up the model’s reasoning capability, followed by a warm-up phase for the self-rewarding capability before the complete integration of verifier-based and self-rewarding-based advantages.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p6">
<p class="ltx_p">By combining all the aforementioned techniques, our full algorithm <span class="ltx_text ltx_font_bold">Reinforcement Learning with <span class="ltx_text ltx_framed ltx_framed_underline">La</span>st-Token <span class="ltx_text ltx_framed ltx_framed_underline">Se</span>lf-<span class="ltx_text ltx_framed ltx_framed_underline">R</span>ewarding</span> (<span class="ltx_text ltx_font_bold">LaSeR</span>), is summarized in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#algorithm1" title="Algorithm 1 ‣ 3.3 Other Techniques ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">1</span></a> and illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S0.F1" title="Figure 1 ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">1</span></a>. During the testing phase, once the model generates a solution, we compute the last-token self-rewarding score based on <math alttext="r_{s}=\beta_{v}\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})-\beta_{v}c_{ref}" class="ltx_Math" display="inline" id="S3.SS3.p6.m1" intent=":literal"><semantics><mrow><msub><mi>r</mi><mi>s</mi></msub><mo>=</mo><mrow><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mi>𝜽</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>c</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">r_{s}=\beta_{v}\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})-\beta_{v}c_{ref}</annotation></semantics></math>. The comparison between this score and 0.5 determines the self-verification outcome of the solution, or the score itself can be further used to perform weighted majority voting.</p>
</div>
<figure class="ltx_float ltx_algorithm" id="algorithm1">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div class="ltx_listingline">
<span class="ltx_text ltx_font_bold">Input:</span> Initial policy model <math alttext="\pi_{\bm{\theta}}" class="ltx_Math" display="inline" id="algorithm1.m1" intent=":literal"><semantics><msub><mi>π</mi><mi>𝜽</mi></msub><annotation encoding="application/x-tex">\pi_{\bm{\theta}}</annotation></semantics></math>, prompts <math alttext="D" class="ltx_Math" display="inline" id="algorithm1.m2" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>, verifier <math alttext="r_{v}" class="ltx_Math" display="inline" id="algorithm1.m3" intent=":literal"><semantics><msub><mi>r</mi><mi>v</mi></msub><annotation encoding="application/x-tex">r_{v}</annotation></semantics></math>, warm-up hyper-parameters <math alttext="w_{r}" class="ltx_Math" display="inline" id="algorithm1.m4" intent=":literal"><semantics><msub><mi>w</mi><mi>r</mi></msub><annotation encoding="application/x-tex">w_{r}</annotation></semantics></math> and <math alttext="w_{sr}" class="ltx_Math" display="inline" id="algorithm1.m5" intent=":literal"><semantics><msub><mi>w</mi><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>r</mi></mrow></msub><annotation encoding="application/x-tex">w_{sr}</annotation></semantics></math>, coefficient <math alttext="\beta_{v}" class="ltx_Math" display="inline" id="algorithm1.m6" intent=":literal"><semantics><msub><mi>β</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\beta_{v}</annotation></semantics></math>, pre-specified token <math alttext="z_{c}" class="ltx_Math" display="inline" id="algorithm1.m7" intent=":literal"><semantics><msub><mi>z</mi><mi>c</mi></msub><annotation encoding="application/x-tex">z_{c}</annotation></semantics></math>, pre-calculated <math alttext="c_{ref}=\mathbb{E}_{(\bm{x},\bm{y})}[\log\pi_{ref}(z_{c}|\bm{x},\bm{y})]" class="ltx_Math" display="inline" id="algorithm1.m8" intent=":literal"><semantics><mrow><msub><mi>c</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo>=</mo><mrow><msub><mi>𝔼</mi><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">c_{ref}=\mathbb{E}_{(\bm{x},\bm{y})}[\log\pi_{ref}(z_{c}|\bm{x},\bm{y})]</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_text ltx_font_bold">for</span> <em class="ltx_emph ltx_font_italic">Step <math alttext="s=1,\cdots,S" class="ltx_Math" display="inline" id="algorithm1.m9" intent=":literal"><semantics><mrow><mi>s</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><mi>S</mi></mrow></mrow><annotation encoding="application/x-tex">s=1,\cdots,S</annotation></semantics></math></em> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline"> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 
1. Set <math alttext="\pi_{old}\leftarrow\pi_{\bm{\theta}}" class="ltx_Math" display="inline" id="algorithm1.m10" intent=":literal"><semantics><mrow><msub><mi>π</mi><mrow><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msub><mo stretchy="false">←</mo><msub><mi>π</mi><mi>𝜽</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{old}\leftarrow\pi_{\bm{\theta}}</annotation></semantics></math>;

</div>
<div class="ltx_listingline"> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 2. Sample batch prompts <math alttext="D_{s}" class="ltx_Math" display="inline" id="algorithm1.m11" intent=":literal"><semantics><msub><mi>D</mi><mi>s</mi></msub><annotation encoding="application/x-tex">D_{s}</annotation></semantics></math> from <math alttext="D" class="ltx_Math" display="inline" id="algorithm1.m12" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>;

</div>
<div class="ltx_listingline"> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 3. Generate solutions <math alttext="\{\bm{y}^{i}\}_{i=1}^{K}" class="ltx_Math" display="inline" id="algorithm1.m13" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msup><mi>𝒚</mi><mi>i</mi></msup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding="application/x-tex">\{\bm{y}^{i}\}_{i=1}^{K}</annotation></semantics></math> for each <math alttext="\bm{x}\in D_{s}" class="ltx_Math" display="inline" id="algorithm1.m14" intent=":literal"><semantics><mrow><mi>𝒙</mi><mo>∈</mo><msub><mi>D</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">\bm{x}\in D_{s}</annotation></semantics></math>;

</div>
<div class="ltx_listingline"> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 4. Calculate verifier-based rewards and advantages (e.g., Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.E4" title="Equation 4 ‣ 3.1 Preliminaries ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">4</span></a>)), calculate RL loss;

</div>
<div class="ltx_listingline"> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 5. If <math alttext="s\geq w_{r}" class="ltx_Math" display="inline" id="algorithm1.m15" intent=":literal"><semantics><mrow><mi>s</mi><mo>≥</mo><msub><mi>w</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">s\geq w_{r}</annotation></semantics></math>, calculate last-token self-rewarding loss based on Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.E15" title="Equation 15 ‣ 3.3 Other Techniques ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">15</span></a>) and add it to RL loss;

</div>
<div class="ltx_listingline"> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 6. If <math alttext="s\geq w_{sr}" class="ltx_Math" display="inline" id="algorithm1.m16" intent=":literal"><semantics><mrow><mi>s</mi><mo>≥</mo><msub><mi>w</mi><mrow><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">s\geq w_{sr}</annotation></semantics></math>, calculate self-rewarding-based advantages and perform advantage integration based on Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.E16" title="Equation 16 ‣ 3.3 Other Techniques ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">16</span></a>);

</div>
<div class="ltx_listingline"> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 7. Update the policy model <math alttext="\pi_{\bm{\theta}}" class="ltx_Math" display="inline" id="algorithm1.m17" intent=":literal"><semantics><msub><mi>π</mi><mi>𝜽</mi></msub><annotation encoding="application/x-tex">\pi_{\bm{\theta}}</annotation></semantics></math> using any RL algorithm with integrated loss and advantages;

</div>
<div class="ltx_listingline"> end for
</div>
<div class="ltx_listingline">
<span class="ltx_text ltx_font_bold">Output:</span> <math alttext="\pi_{\bm{\theta}}" class="ltx_Math" display="inline" id="algorithm1.m18" intent=":literal"><semantics><msub><mi>π</mi><mi>𝜽</mi></msub><annotation encoding="application/x-tex">\pi_{\bm{\theta}}</annotation></semantics></math>
</div>
<div class="ltx_listingline">
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 1</span> </span><span class="ltx_text ltx_font_bold">LaSeR</span>: Reinforcement Learning with <span class="ltx_text ltx_framed ltx_framed_underline">La</span>st-Token <span class="ltx_text ltx_framed ltx_framed_underline">Se</span>lf-<span class="ltx_text ltx_framed ltx_framed_underline">R</span>ewarding</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Brief Discussion</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Comparison Between LaSeR and Prior Approaches</span> Compared with previous methods <cite class="ltx_cite ltx_citemacro_citep">(Sareen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib40" title="">2025</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib27" title="">2025a</a>; Zha et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib62" title="">2025</a>)</cite> that requires the policy model to perform separate generations for solutions and verifications, our method directly derives the self-rewarding result from the next-token log-probability of the final solution token. In the RL process, the computation of token log-probabilities is typically carried out after all the generations are completed <cite class="ltx_cite ltx_citemacro_citep">(Sheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib43" title="">2024</a>)</cite>. Therefore, we can directly replace the token id of the first padding token with the token id of the pre-specified token before computing the log-probabilities of the sequences, thereby <span class="ltx_text ltx_font_bold">incurring no additional computation cost during training</span>. <span class="ltx_text ltx_font_bold">During inference, our method requires only one more token inference after the solution is completed</span>, which substantially reduces the computational cost compared to previous methods. We also discuss the potential way to <span class="ltx_text ltx_font_bold">further reduce the self-rewarding cost by avoiding any extra token inference</span> in Section <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S5.SS3" title="5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">5.3</span></a>, which can be an interesting future work.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Difference Between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss</span> An alternative to train the self-verification capability is to optimize the following supervised fine-tuning (SFT) loss by maximizing the next-token probability of the token <math alttext="z_{c}" class="ltx_Math" display="inline" id="S3.SS4.p2.m1" intent=":literal"><semantics><msub><mi>z</mi><mi>c</mi></msub><annotation encoding="application/x-tex">z_{c}</annotation></semantics></math> or <math alttext="z_{i}" class="ltx_Math" display="inline" id="S3.SS4.p2.m2" intent=":literal"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_{i}</annotation></semantics></math> based on the context <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS4.p2.m3" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E17">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E17X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle L_{SFT}=-\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{g}(\cdot|x)}\left[r_{v}(\bm{x},\bm{y})\cdot\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})+(1-r_{v}(\bm{x},\bm{y}))\cdot\log\pi_{\bm{\theta}}(z_{i}|\bm{x},\bm{y})\right]." class="ltx_math_unparsed" display="inline" id="S3.E17X.m2" intent=":literal"><semantics><mrow><mrow><msub><mi>L</mi><mrow><mi>S</mi><mo lspace="0em" rspace="0em">​</mo><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msub><mo>=</mo><mrow><mo>−</mo><mrow><msub><mi>𝔼</mi><mrow><mi>𝒙</mi><mo>∼</mo><mi>D</mi><mo>,</mo><mi>𝒚</mi><mo>∼</mo><msub><mi>π</mi><mi>g</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mrow><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⋅</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mi>𝜽</mi></msub></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">⋅</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mi>𝜽</mi></msub></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>i</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle L_{SFT}=-\mathbb{E}_{\bm{x}\sim D,\bm{y}\sim\pi_{g}(\cdot|x)}\left[r_{v}(\bm{x},\bm{y})\cdot\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})+(1-r_{v}(\bm{x},\bm{y}))\cdot\log\pi_{\bm{\theta}}(z_{i}|\bm{x},\bm{y})\right].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(17)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">The major difference between SFT loss and our last-token self-rewarding loss in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.E13" title="Equation 13 ‣ 3.2.1 Formal Formulation ‣ 3.2 LaSeR: Reinforcement Learning with Last-Token Self-Rewarding ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">13</span></a>) is that the SFT loss drives
<math alttext="\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS4.p2.m4" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> to fit <math alttext="1" class="ltx_Math" display="inline" id="S3.SS4.p2.m5" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> when <math alttext="r_{v}(\bm{x},\bm{y})=1" class="ltx_Math" display="inline" id="S3.SS4.p2.m6" intent=":literal"><semantics><mrow><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_{v}(\bm{x},\bm{y})=1</annotation></semantics></math>, which may lead to strong interference with the optimization of reasoning capability.
In contrast, our loss drives
<math alttext="\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS4.p2.m7" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> toward
<math alttext="\exp(1/\beta_{v})\cdot\pi_{\text{ref}}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS4.p2.m8" intent=":literal"><semantics><mrow><mrow><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><msub><mi>β</mi><mi>v</mi></msub></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⋅</mo><msub><mi>π</mi><mtext>ref</mtext></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\exp(1/\beta_{v})\cdot\pi_{\text{ref}}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> for <math alttext="r_{v}(\bm{x},\bm{y})=1.0" class="ltx_Math" display="inline" id="S3.SS4.p2.m9" intent=":literal"><semantics><mrow><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1.0</mn></mrow><annotation encoding="application/x-tex">r_{v}(\bm{x},\bm{y})=1.0</annotation></semantics></math>.
When <math alttext="\beta_{v}" class="ltx_Math" display="inline" id="S3.SS4.p2.m10" intent=":literal"><semantics><msub><mi>β</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\beta_{v}</annotation></semantics></math> is relatively large,
<math alttext="\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S3.SS4.p2.m11" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> remains still very small, thereby exerting only a negligible influence on the original RLVR optimization
(e.g., <math alttext="\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})=e^{-13}" class="ltx_Math" display="inline" id="S3.SS4.p2.m12" intent=":literal"><semantics><mrow><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>13</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})=e^{-13}</annotation></semantics></math> when
<math alttext="\pi_{\text{ref}}(z_{c}|\bm{x},\bm{y})=e^{-23}" class="ltx_Math" display="inline" id="S3.SS4.p2.m13" intent=":literal"><semantics><mrow><mrow><msub><mi>π</mi><mtext>ref</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>23</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\pi_{\text{ref}}(z_{c}|\bm{x},\bm{y})=e^{-23}</annotation></semantics></math> and <math alttext="\beta_{v}=0.1" class="ltx_Math" display="inline" id="S3.SS4.p2.m14" intent=":literal"><semantics><mrow><msub><mi>β</mi><mi>v</mi></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\beta_{v}=0.1</annotation></semantics></math>).
We provide the empirical comparison in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A6" title="Appendix F Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss ‣ Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">F</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Settings</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Base Models and Baselines</span>
We primarily conduct empirical validations on both LLaMA3.2 <cite class="ltx_cite ltx_citemacro_cite">MetaAI (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib32" title="">2024b</a>)</cite> and Qwen2.5 <cite class="ltx_cite ltx_citemacro_citep">(Qwen Team, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib37" title="">2024</a>)</cite> architectures, including three base models: OctoThinker-3B-Short-Base <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib51" title="">2025a</a>)</cite> (mid-trained version of LLaMA3.2-3B-Base), Qwen2.5-7B-Base <cite class="ltx_cite ltx_citemacro_citep">(Qwen Team, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib37" title="">2024</a>)</cite> (pre-trained model) and Open-Reasoner-Zero-7B <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib16" title="">2025</a>)</cite> (reinforced version of Qwen2.5-7B-Base). In principle, our method can be seamlessly integrated into any RLVR framework, as it only introduces an additional MSE loss term. In this work, we adopt the widely used GRPO <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib42" title="">2024</a>)</cite> as the base algorithm and primarily investigate the effectiveness of applying our method within GRPO, while leaving the exploration on other RL algorithms in the future work.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Training and Evaluation Datasets</span>
We adopt DeepMath-103K <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib14" title="">2025</a>)</cite>, a large-scale and high-quality mathematical reasoning dataset, for our RL training data. In testing, we evaluate both the reasoning and self-verification performance of each model on five typical math reasoning benchmarks: MATH500 <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib15" title="">2021</a>)</cite>, AMC23 <cite class="ltx_cite ltx_citemacro_citep">(AI-MO, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib4" title="">2024b</a>)</cite>, AIME24 <cite class="ltx_cite ltx_citemacro_citep">(AI-MO, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib3" title="">2024a</a>)</cite>, AIME25 <cite class="ltx_cite ltx_citemacro_citep">(OpenCompass, <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib35" title="">2025</a>)</cite>, and OlympiadBench <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib13" title="">2024</a>)</cite>. We also explore the effectiveness of our method in general reasoning tasks beyond math reasoning in Section <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S5.SS2" title="5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Training Settings</span> The detailed training hyper-parameters of GRPO are put in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A3" title="Appendix C Detailed Training Settings ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">C</span></a>. The prompt template for each model is in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A9" title="Appendix I Prompt Templates ‣ Appendix H Training and Evaluation Settings in General Reasoning Experiments ‣ Appendix G Detailed Self-Verification Results ‣ Appendix F Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss ‣ Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">I</span></a>. When applying our method, we set the hyper-parameters <math alttext="(\beta_{v},\alpha,\tau)=(0.1,0.1,0.1)" class="ltx_Math" display="inline" id="S4.SS1.p3.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>β</mi><mi>v</mi></msub><mo>,</mo><mi>α</mi><mo>,</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><mo stretchy="false">(</mo><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(\beta_{v},\alpha,\tau)=(0.1,0.1,0.1)</annotation></semantics></math>, which are empirically determined based on the observations in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A4" title="Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">D</span></a>. <math alttext="z_{c}" class="ltx_Math" display="inline" id="S4.SS1.p3.m2" intent=":literal"><semantics><msub><mi>z</mi><mi>c</mi></msub><annotation encoding="application/x-tex">z_{c}</annotation></semantics></math> is selected as “<span class="ltx_text ltx_font_typewriter">&lt;vision_start&gt;</span>” for Qwen2.5-7B-Base and Open-Reasoner-Zero-7B, and “<span class="ltx_text ltx_font_typewriter">&lt;reserved_special_token_0&gt;</span>” for OctoThinker-3B-Short-Base. The simplified constant of the reference log-probability, <math alttext="c_{\text{ref}}" class="ltx_Math" display="inline" id="S4.SS1.p3.m3" intent=":literal"><semantics><msub><mi>c</mi><mtext>ref</mtext></msub><annotation encoding="application/x-tex">c_{\text{ref}}</annotation></semantics></math>, is <math alttext="-23.0" class="ltx_Math" display="inline" id="S4.SS1.p3.m4" intent=":literal"><semantics><mrow><mo>−</mo><mn>23.0</mn></mrow><annotation encoding="application/x-tex">-23.0</annotation></semantics></math> for Qwen2.5-7B-Base and Open-Reasoner-Zero-7B, and <math alttext="-25.0" class="ltx_Math" display="inline" id="S4.SS1.p3.m5" intent=":literal"><semantics><mrow><mo>−</mo><mn>25.0</mn></mrow><annotation encoding="application/x-tex">-25.0</annotation></semantics></math> for OctoThinker-3B-Short-Base, as estimated from the results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A1.F11" title="Figure 11 ‣ Appendix A The Length Bias in Implicit Reward ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">11</span></a>. The number of reasoning warm-up steps is set to 200 for both Qwen2.5-7B-Base and OctoThinker-3B-Short-Base, and the number of self-rewarding warm-up steps is 200 across all models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Evaluation Settings</span> During generation, we set both the <span class="ltx_text ltx_font_typewriter">temperature</span> and <span class="ltx_text ltx_font_typewriter">top_p</span> to <math alttext="1.0" class="ltx_Math" display="inline" id="S4.SS1.p4.m1" intent=":literal"><semantics><mn>1.0</mn><annotation encoding="application/x-tex">1.0</annotation></semantics></math> for all models. The <span class="ltx_text ltx_font_typewriter">max_generation_len</span> is 8192. On MATH500 and OlympiadBench, we sample 2 solutions for each problem; whereas on AMC23, AIME24, and AIME25, we sample 32 solutions per problem. We then report the average Pass@1 accuracy of each model on each benchmark. We also evaluate the self-verification performance of each model by computing the self-verification F1 score, defined as the harmonic mean of self-verification accuracy on self-generated correct and incorrect solutions. Baselines perform self-verification based on the prompt in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A9" title="Appendix I Prompt Templates ‣ Appendix H Training and Evaluation Settings in General Reasoning Experiments ‣ Appendix G Detailed Self-Verification Results ‣ Appendix F Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss ‣ Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">I</span></a>. Any solution without a final answer is automatically treated as incorrect and excluded from the verification accuracy calculation. Detailed self-verification accuracy results are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A7" title="Appendix G Detailed Self-Verification Results ‣ Appendix F Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss ‣ Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results and Analysis</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Reasoning and self-verification performance of each model on five mathematical reasoning benchmarks. We do not report the results of OctoThinker-based models on AIME24-25, as the number of correct solutions is quite insufficient for a reliable evaluation.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">Method</td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" style="padding-left:4.0pt;padding-right:4.0pt;">Reasoning Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" style="padding-left:4.0pt;padding-right:4.0pt;">Self-Verification F1 Score</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">MATH-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">500</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AMC-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">23</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AIME-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">24</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AIME-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">25</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Olym.-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Bench</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Avg.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">MATH-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">500</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AMC-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">23</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AIME-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">24</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">AIME-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">25</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Olym.-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Bench</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">Avg.</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="13" style="padding-left:4.0pt;padding-right:4.0pt;"><em class="ltx_emph ltx_font_italic"> <span class="ltx_text ltx_font_bold">OctoThinker-3B-Short-Base</span></em></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">Base</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>3.7</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>1.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>1.0</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>2.0</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">22.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">11.2</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">13.7</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">15.7</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">GRPO</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">49.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">25.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">17.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">30.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">56.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">47.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">48.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">51.0</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">LaSeR</span></th>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">53.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">27.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">18.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">32.8</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">73.6</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">70.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">73.6</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">72.5</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">- <span class="ltx_text ltx_font_italic">SWA</span></span></th>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">52.9</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">26.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">18.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">32.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">80.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">70.9</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">66.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">72.4</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="13" style="padding-left:4.0pt;padding-right:4.0pt;"><em class="ltx_emph ltx_font_italic"> <span class="ltx_text ltx_font_bold">Qwen2.5-7B-Base</span></em></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">Base</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">35.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">20.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>3.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_text ltx_phantom"><span style="visibility:hidden">0</span></span>1.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">12.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">14.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">36.4</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">30.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">27.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">32.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">36.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">32.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">GRPO</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">79.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">55.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">16.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">13.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">43.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">41.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">54.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">59.7</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">36.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">41.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">53.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">49.2</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">LaSeR</span></th>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">80.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">58.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">15.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">15.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">44.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">42.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">83.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">82.5</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">79.6</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">74.3</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">78.3</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">79.6</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">- <span class="ltx_text ltx_font_italic">SWA</span></span></th>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">78.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">58.3</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">15.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">12.3</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">41.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">41.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">79.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">80.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">81.3</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">74.9</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">83.3</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">79.9</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="13" style="padding-left:4.0pt;padding-right:4.0pt;"><em class="ltx_emph ltx_font_italic"> <span class="ltx_text ltx_font_bold">Open-Reasoner-Zero-7B</span></em></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">Base</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">81.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">60.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">15.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold">15.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">46.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">44.0</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">26.7</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">51.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">45.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">55.2</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">37.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">43.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">GRPO</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">83.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">61.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">18.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">15.0</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">47.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">45.0</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">57.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">44.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">14.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">28.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">49.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">38.8</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">LaSeR</span></th>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">82.8</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">62.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">19.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">15.1</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">47.8</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">45.5</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">87.2</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">79.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">64.6</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">77.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">78.7</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">77.6</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">- <span class="ltx_text ltx_font_italic">SWA</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">83.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">62.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">19.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">14.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">47.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">45.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">87.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">77.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">63.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">77.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">77.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">76.7</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of verification F1 scores between LaSeR (self-rewarding) and external reward models (Qwen2.5-Math-7B-PRM800K, Qwen2.5-Math-PRM-7B, and Qwen2.5-Math-RM-72B) on responses generated by different policy models.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_left">Method</td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt">MATH500</td>
<td class="ltx_td ltx_align_center ltx_border_tt">AMC23</td>
<td class="ltx_td ltx_align_center ltx_border_tt">AIME24</td>
<td class="ltx_td ltx_align_center ltx_border_tt">AIME25</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Olym.</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Avg.</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="7"><em class="ltx_emph ltx_font_italic"> <span class="ltx_text ltx_font_bold">Generator: OctoThinker-3B-Short-LaSeR</span></em></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen2.5-Math-7B-PRM800K (7B RM)</th>
<td class="ltx_td ltx_align_center">77.0</td>
<td class="ltx_td ltx_align_center">68.9</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">68.5</td>
<td class="ltx_td ltx_align_center">71.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen2.5-Math-PRM-7B (7B RM)</th>
<td class="ltx_td ltx_align_center">80.9</td>
<td class="ltx_td ltx_align_center">63.5</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">64.1</td>
<td class="ltx_td ltx_align_center">69.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen2.5-Math-RM-72B (72B RM)</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">89.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">71.7</span></td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">72.9</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">77.9</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">LaSeR (3B Self-Rewarding)</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">73.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">70.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">73.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">72.5</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="7"><em class="ltx_emph ltx_font_italic"> <span class="ltx_text ltx_font_bold">Generator: Qwen2.5-7B-Laser</span></em></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen2.5-Math-7B-PRM800K (7B RM)</th>
<td class="ltx_td ltx_align_center">59.4</td>
<td class="ltx_td ltx_align_center">52.7</td>
<td class="ltx_td ltx_align_center">58.8</td>
<td class="ltx_td ltx_align_center">53.8</td>
<td class="ltx_td ltx_align_center">52.0</td>
<td class="ltx_td ltx_align_center">55.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen2.5-Math-PRM-7B (7B RM)</th>
<td class="ltx_td ltx_align_center">82.5</td>
<td class="ltx_td ltx_align_center">79.2</td>
<td class="ltx_td ltx_align_center">75.1</td>
<td class="ltx_td ltx_align_center">72.3</td>
<td class="ltx_td ltx_align_center">77.8</td>
<td class="ltx_td ltx_align_center">77.4</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen2.5-Math-RM-72B (72B RM)</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">87.8</span></td>
<td class="ltx_td ltx_align_center">80.7</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">81.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">74.8</span></td>
<td class="ltx_td ltx_align_center">75.4</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">80.0</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">LaSeR (7B Self-Rewarding)</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">83.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">82.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">79.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">74.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">78.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">79.6</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="7"><em class="ltx_emph ltx_font_italic"> <span class="ltx_text ltx_font_bold">Generator: Open-Reasoner-Zero-7B-LaSeR</span></em></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen2.5-Math-7B-PRM800K (7B RM)</th>
<td class="ltx_td ltx_align_center">56.3</td>
<td class="ltx_td ltx_align_center">42.5</td>
<td class="ltx_td ltx_align_center">51.4</td>
<td class="ltx_td ltx_align_center">50.8</td>
<td class="ltx_td ltx_align_center">38.5</td>
<td class="ltx_td ltx_align_center">47.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen2.5-Math-PRM-7B (7B RM)</th>
<td class="ltx_td ltx_align_center">86.0</td>
<td class="ltx_td ltx_align_center">79.6</td>
<td class="ltx_td ltx_align_center">70.8</td>
<td class="ltx_td ltx_align_center">67.3</td>
<td class="ltx_td ltx_align_center">76.0</td>
<td class="ltx_td ltx_align_center">75.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen2.5-Math-RM-72B (72B RM)</th>
<td class="ltx_td ltx_align_center">86.8</td>
<td class="ltx_td ltx_align_center">79.4</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">71.0</span></td>
<td class="ltx_td ltx_align_center">71.4</td>
<td class="ltx_td ltx_align_center">75.5</td>
<td class="ltx_td ltx_align_center">76.8</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">LaSeR (7B Self-Rewarding)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">87.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">79.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">64.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">77.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">78.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#E6FFFF;">77.6</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p">We put the main results in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S4.T1" title="Table 1 ‣ 4.2 Main Results and Analysis ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">1</span></a>. The key conclusion is that, <span class="ltx_text ltx_font_bold">across different model variants, our method not only yields better reasoning performance for the policy model compared with the baseline, but also substantially enhances its self-verification capability by enabling the self-rewarding scores to achieve remarkably high F1 scores</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p">Regarding reasoning performance, applying our algorithm leads to higher accuracy in most settings and consistently yields higher average accuracy on the three base models. We think there are two main reasons for this improvement: (1) First, our method encourages the model to encode its assessment of the overall solution in the final response token, which leads to better confidence calibration. Improved calibration itself can have a positive impact on the model’s learning. (2) Second, by integrating self-rewarding-based advantages into verifier-based advantages, our approach enables more fine-grained advantage estimation, which in turn facilitates more effective learning. For comparison, we report the results without self-rewarding-based advantages (<span class="ltx_text ltx_font_italic">-SWA</span>) in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S4.T1" title="Table 1 ‣ 4.2 Main Results and Analysis ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p">Regarding self-rewarding performance, applying a simple last-token self-rewarding MSE loss substantially enhances the self-rewarding capability of the models, achieving around 80% self-verification F1 scores. This demonstrates strong self-verification accuracy on both correct and incorrect solutions. To further highlight the self-rewarding capabilities, we display the comparison results of verification F1 scores between LaSeR and three advanced external reward models (Qwen2.5-Math-7B-PRM800K <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib64" title="">2025</a>)</cite>, Qwen2.5-Math-PRM-7B <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib64" title="">2025</a>)</cite>, and Qwen2.5-Math-RM-72B <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib54" title="">2024</a>)</cite>) on evaluating the solutions generated by the different reinforced models, i.e., OctoThinker-3B-Short-LaSeR, Qwen2.5-7B-LaSeR, and Open-Reasoner-Zero-7B-LaSeR. The results in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S4.T2" title="Table 2 ‣ 4.2 Main Results and Analysis ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">2</span></a> show that LaSeR outperforms equally sized state-of-the-art external verifiers in assessing the model’s own solutions, and even matches the verification performance of a 72B reward model, demonstrating its non-trivial effectiveness in enhancing self-rewarding capability.
Also, our method requires one additional token inference only to compute the self-rewarding scores for enabling the policy model to function simultaneously as both the generator and reward model, which is highly efficient and practical.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Inference-Time Scaling Results</h3>
<figure class="ltx_figure" id="S4.F7.sf1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>The majority voting (Maj@K) and weighted majority voting (RM@K) results on MATH500 and OlympiadBench.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel">Here, we explore the effectiveness of self-rewarding in the inference-time scaling via weighted majority voting. We compare majority voting results with (RM@K) and without (Maj@K) weighting by the last-token self-rewarding scores, on MATH500 and OlympiadBench.
The results are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S4.F7.sf1" title="Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">7(a)</span></a>.
We denote the three base models by “OT-3B”, “Qwen2.5-7B”, and “ORZ-7B”. The suffixes “-GRPO” and “-LaSeR” indicate the variants trained with GRPO and our method LaSeR, respectively.
The results show that <span class="ltx_text ltx_font_bold">the optimized self-rewarding capability of the model is highly effective on improving its own inference-time scaling performance</span>.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_section ltx_figure_panel" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis and Discussion</h2>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of reasoning and self-verification performance with and without reference log-probability simplification in our method. Based model is Open-Reasoner-Zero-7B.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_left" style="padding-left:3.5pt;padding-right:3.5pt;">Method</td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" style="padding-left:3.5pt;padding-right:3.5pt;">Reasoning Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" style="padding-left:3.5pt;padding-right:3.5pt;">Self-Verification F1 Score</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">MATH-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">500</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">AMC-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">23</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">AIME-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">24</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">AIME-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">25</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">Olym.-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">Bench</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">Avg.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">MATH-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">500</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">AMC-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">23</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">AIME-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">24</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">AIME-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">25</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">Olym.-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">Bench</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">Avg.</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">w/ Simpl.</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">82.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">61.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">18.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">16.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">46.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">45.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">82.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">79.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">77.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">79.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">78.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">79.4</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">w/o Simpl.</th>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">81.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">61.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">17.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">17.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">48.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">45.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">81.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">79.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">79.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">78.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">77.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">79.3</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>The Impact of Simplifying The Reference Log-Probabilities to A Constant</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p">As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.SS3" title="3.3 Other Techniques ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">3.3</span></a>,
we approximate the log-probability of the pre-specified token under the reference model, <math alttext="\log\pi_{ref}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S5.SS1.p1.m1" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log\pi_{ref}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math>, by its mean computed over a small sample set when calculating the last-token self-rewarding scores. Here, we empirically validate this practice by conducting comparison experiments on Open-Reasoner-Zero-7B, with and without reference log-probability simplification in our method. We evaluate
the checkpoint after 200 optimization steps in each setting (corresponding to the last checkpoint before advantage integration). The results are reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S5.T3" title="Table 3 ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">3</span></a>. As shown, <span class="ltx_text ltx_font_bold">the simplification does not affect the optimization of reasoning and self-rewarding capabilities</span>, since the performance under the two settings remains comparable. However, it effectively reduces the computational cost of calculating the last-token self-rewarding value by half.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>The Generalizability of LaSeR to General Reasoning Domain</h3>
<figure class="ltx_figure" id="S5.F10.sf1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>The generalizability of LaSeR on general reasoning tasks.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel">We conduct additional experiments to explore the generalizability of our method to general reasoning domain. We use a filtered version <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib58" title="">2025b</a>)</cite> of WebInstruct-verified dataset <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib30" title="">2025</a>)</cite>, and conduct RL experiments on Qwen3-4B-Base <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib55" title="">2025a</a>)</cite>. We use the <span class="ltx_text ltx_font_typewriter">general-verifier-1.5B</span> model from <cite class="ltx_cite ltx_citemacro_citet">Ma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib30" title="">2025</a>)</cite> as the model-based verifier and adopt GRPO as the RL algorithm. For our method, we do not perform the advantage integration strategy here. The reason is that we observe the self-verification F1 score of our method during training is relatively low in the general reasoning setting (only between 65% and 70%, and the self-rewarding score distributions in the test sets shown in Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:_score_distribution_on_mmlu</span> and Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:_score_distribution_on_gpqa</span> also reveal this phenomenon). This leads to large noise in the self-rewarding-based advantage estimation, and consequently, the integration of self-rewarding-based advantages results in performance degradation.
After training, we conduct evaluations on two general reasoning benchmarks: MMLU-Pro <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib50" title="">2024b</a>)</cite> and GPQA-Diamond <cite class="ltx_cite ltx_citemacro_citep">(Rein et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib39" title="">2024</a>)</cite>. We sample 4 solutions per problem on each dataset for each model, and calculate both the average accuracy and the (weighted) majority voting accuracy. Detailed training and evaluation settings are in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A8" title="Appendix H Training and Evaluation Settings in General Reasoning Experiments ‣ Appendix G Detailed Self-Verification Results ‣ Appendix F Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss ‣ Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">H</span></a>.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel">We display the evaluation accuracy in Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:_acc_on_general_reasoning_tasks</span>, and additionally, we display the self-rewarding score distributions on two datasets in Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:_score_distribution_on_mmlu</span> and Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:_score_distribution_on_gpqa</span> for reference. First, we observe that jointly optimizing the self-rewarding capability does not impact the model’s general reasoning ability, allowing the policy model to achieve comparable average reasoning accuracy to the baseline. However, as mentioned above, the optimized self-rewarding score on general reasoning tasks does not achieve the high accuracy seen in math reasoning tasks. We can see that the self-rewarding score distributions for correct and incorrect solutions on MMLU-Pro exhibit certain overlap, and the distinction further diminishes on the more challenging benchmark GPQA-Diamond. We speculate that two factors may contribute to this: (1) The model’s general reasoning ability is inherently weaker than its math reasoning ability, which limits the upper bound of its self-rewarding capabilities in the general reasoning domain. (2) The model-based verifier used in the experiment (<span class="ltx_text ltx_font_typewriter">general-verifier-1.5B</span>) has limited verification ability, resulting in high noise in the reasoning rewards, which in turn affects the optimization of the self-rewarding capability. A promising direction for future work is to further explore and unlock the full potential of our method in the general reasoning domain. Though not perfect, the optimized self-rewarding scores can still provide useful signals during inference time, leading to better weighted majority voting results.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_subsection ltx_figure_panel" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Further Reduction or Increase of Self-Rewarding Cost</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p">In this section, we discuss two additional variants of LaSeR for future work. In the current method, we calculate the last-token self-rewarding score based on the next-token log-probability distribution of the “<span class="ltx_text ltx_font_typewriter">&lt;EOS&gt;</span>” token, requiring one additional token inference compared with standard inference. One potential way to further reduce the inference cost of LaSeR is to derive the last-token self-rewarding score directly from the predicted log-probability of pre-specified token <math alttext="z_{c}" class="ltx_Math" display="inline" id="S5.SS3.p1.m1" intent=":literal"><semantics><msub><mi>z</mi><mi>c</mi></msub><annotation encoding="application/x-tex">z_{c}</annotation></semantics></math> at the “<span class="ltx_text ltx_font_typewriter">&lt;EOS&gt;</span>” token position. Specifically, let <math alttext="y_{T}" class="ltx_Math" display="inline" id="S5.SS3.p1.m2" intent=":literal"><semantics><msub><mi>y</mi><mi>T</mi></msub><annotation encoding="application/x-tex">y_{T}</annotation></semantics></math> denote the “<span class="ltx_text ltx_font_typewriter">&lt;EOS&gt;</span>” token in <math alttext="\bm{y}" class="ltx_Math" display="inline" id="S5.SS3.p1.m3" intent=":literal"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation></semantics></math>. Then, the reduced last-token self-rewarding score can be defined as <math alttext="r_{s}=\beta_{v}\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y}_{&lt;T})-\beta_{v}c_{ref}" class="ltx_Math" display="inline" id="S5.SS3.p1.m4" intent=":literal"><semantics><mrow><msub><mi>r</mi><mi>s</mi></msub><mo>=</mo><mrow><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mi>𝜽</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒚</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>β</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>c</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">r_{s}=\beta_{v}\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y}_{&lt;T})-\beta_{v}c_{ref}</annotation></semantics></math>, as we have observed that <math alttext="\pi_{\text{ref}}(z_{c}|\bm{x},\bm{y}_{&lt;T})" class="ltx_Math" display="inline" id="S5.SS3.p1.m5" intent=":literal"><semantics><mrow><msub><mi>π</mi><mtext>ref</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒚</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\text{ref}}(z_{c}|\bm{x},\bm{y}_{&lt;T})</annotation></semantics></math> remains nearly constant across <math alttext="(\bm{x},\bm{y})" class="ltx_Math" display="inline" id="S5.SS3.p1.m6" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\bm{x},\bm{y})</annotation></semantics></math> (e.g., approximately <math alttext="e^{-28}" class="ltx_Math" display="inline" id="S5.SS3.p1.m7" intent=":literal"><semantics><msup><mi>e</mi><mrow><mo>−</mo><mn>28</mn></mrow></msup><annotation encoding="application/x-tex">e^{-28}</annotation></semantics></math> for Qwen2.5-7B-Base). In this case, <span class="ltx_text ltx_font_bold">we can achieve ideally zero additional inference cost for self-rewarding compared with standard generation</span> by directly calculating the self-rewarding score from the log-probability distribution at the “<span class="ltx_text ltx_font_typewriter">&lt;EOS&gt;</span>” token position, without requiring any extra token inference. In theory, this works because setting a relatively large <math alttext="\beta_{v}" class="ltx_Math" display="inline" id="S5.SS3.p1.m8" intent=":literal"><semantics><msub><mi>β</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\beta_{v}</annotation></semantics></math> still yields a small value of <math alttext="\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y}_{&lt;T})" class="ltx_Math" display="inline" id="S5.SS3.p1.m9" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒚</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y}_{&lt;T})</annotation></semantics></math> (e.g., <math alttext="\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y}_{&lt;T})=e^{-18}" class="ltx_Math" display="inline" id="S5.SS3.p1.m10" intent=":literal"><semantics><mrow><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒚</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>18</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y}_{&lt;T})=e^{-18}</annotation></semantics></math> when <math alttext="\beta_{v}=0.1" class="ltx_Math" display="inline" id="S5.SS3.p1.m11" intent=":literal"><semantics><mrow><msub><mi>β</mi><mi>v</mi></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\beta_{v}=0.1</annotation></semantics></math> and <math alttext="c_{\text{ref}}=-28" class="ltx_Math" display="inline" id="S5.SS3.p1.m12" intent=":literal"><semantics><mrow><msub><mi>c</mi><mtext>ref</mtext></msub><mo>=</mo><mrow><mo>−</mo><mn>28</mn></mrow></mrow><annotation encoding="application/x-tex">c_{\text{ref}}=-28</annotation></semantics></math>), thereby allowing <math alttext="\pi_{\bm{\theta}}(\texttt{&lt;EOS&gt;}|\bm{x},\bm{y}_{&lt;T})" class="ltx_Math" display="inline" id="S5.SS3.p1.m13" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mtext class="ltx_mathvariant_monospace">&lt;EOS&gt;</mtext><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><msub><mi>𝒚</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\bm{\theta}}(\texttt{&lt;EOS&gt;}|\bm{x},\bm{y}_{&lt;T})</annotation></semantics></math> to still dominate the probability mass. However, although the probability is very low, we observe that the generator may still select <math alttext="z_{c}" class="ltx_Math" display="inline" id="S5.SS3.p1.m14" intent=":literal"><semantics><msub><mi>z</mi><mi>c</mi></msub><annotation encoding="application/x-tex">z_{c}</annotation></semantics></math> at the end of the sequence in few cases during training, which can adversely affect training stability as the generator continues to generate after <math alttext="z_{c}" class="ltx_Math" display="inline" id="S5.SS3.p1.m15" intent=":literal"><semantics><msub><mi>z</mi><mi>c</mi></msub><annotation encoding="application/x-tex">z_{c}</annotation></semantics></math>. One straight-forward solution may be to set the sampling hyper-parameter <math alttext="top\_p" class="ltx_Math" display="inline" id="S5.SS3.p1.m16" intent=":literal"><semantics><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">_</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">top\_p</annotation></semantics></math> to a value less than 1.0. Future work can further investigate advanced strategies to make the above adjustment more principled and robust.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p">While reducing the self-rewarding cost improves efficiency, an alternative is <span class="ltx_text ltx_font_bold">to increase the inference cost in exchange for stronger self-rewarding capability</span>. That is, instead of computing the self-rewarding score based on the log-probability distribution of a single token only, we can increase the number of additional inference tokens by calculating it over <math alttext="M" class="ltx_Math" display="inline" id="S5.SS3.p2.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> tokens as <math alttext="r_{s}=\beta_{v}\sum_{m=1}^{M}\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y},\underbrace{z_{c},\cdots,z_{c}}_{m-1\ \text{times}}))-M\beta_{v}c_{ref}" class="ltx_math_unparsed" display="inline" id="S5.SS3.p2.m2" intent=":literal"><semantics><mrow><msub><mi>r</mi><mi>s</mi></msub><mo>=</mo><msub><mi>β</mi><mi>v</mi></msub><msubsup><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mi>log</mi><msub><mi>π</mi><mi>𝜽</mi></msub><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mi>c</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo>,</mo><munder><munder accentunder="true"><mrow><msub><mi>z</mi><mi>c</mi></msub><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msub><mi>z</mi><mi>c</mi></msub></mrow><mo stretchy="true">⏟</mo></munder><mrow><mi>m</mi><mo>−</mo><mrow><mn>1</mn><mo lspace="0.410em" rspace="0em">​</mo><mtext>times</mtext></mrow></mrow></munder><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo><mo>−</mo><mi>M</mi><mi>β</mi><msub><mi></mi><mi>v</mi></msub><mi>c</mi><msub><mi></mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">r_{s}=\beta_{v}\sum_{m=1}^{M}\log\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y},\underbrace{z_{c},\cdots,z_{c}}_{m-1\ \text{times}}))-M\beta_{v}c_{ref}</annotation></semantics></math>. It is a promising direction for future research to explore whether increasing the number of additional inference tokens can yield positive inference-time scaling effect for latent self-rewarding capability.</p>
</div>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p">In this work, we propose <span class="ltx_text ltx_font_bold">LaSeR</span>, a lightweight and effective algorithm that jointly optimizes the reasoning and self-rewarding capabilities of LLMs. By deriving the closed-form solution to the RL objective of verification, we uncover a concise yet intriguing formula: the true reasoning reward provided by the verifier is equal to the last-token self-rewarding score produced by the model. This self-rewarding score depends on the model’s next-token log-probability for a pre-specified token at the final response token, a pre-calculated constant, and the KL coefficient. Based on this insight, our method simply adds a MSE loss between the verifier-based reasoning rewards and the corresponding last-token self-rewarding scores into the standard RLVR process. The optimized self-rewarding scores can not only be incorporated back into the RL process to further enhance training, but also achieve high verification accuracy at test time, thereby improving solution ranking and selection.</p>
</div>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al. (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmadian et al. (2024)</span>
<span class="ltx_bibblock">
Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker.

</span>
<span class="ltx_bibblock">Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.14740</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI-MO (2024a)</span>
<span class="ltx_bibblock">
AI-MO.

</span>
<span class="ltx_bibblock">Aime 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/AI-MO/aimo-validation-aime" title="">https://huggingface.co/datasets/AI-MO/aimo-validation-aime</a>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI-MO (2024b)</span>
<span class="ltx_bibblock">
AI-MO.

</span>
<span class="ltx_bibblock">Amc 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/AI-MO/aimo-validation-amc" title="">https://huggingface.co/datasets/AI-MO/aimo-validation-amc</a>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14168</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al. (2025)</span>
<span class="ltx_bibblock">
Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al.

</span>
<span class="ltx_bibblock">Process reinforcement through implicit rewards.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2502.01456</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2025)</span>
<span class="ltx_bibblock">
Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al.

</span>
<span class="ltx_bibblock">Agentic reinforced policy optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2507.19849</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2025)</span>
<span class="ltx_bibblock">
Run-Ze Fan, Zengzhi Wang, and Pengfei Liu.

</span>
<span class="ltx_bibblock">Megascience: Pushing the frontiers of post-training datasets for science reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2507.16812</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2024)</span>
<span class="ltx_bibblock">
Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, et al.

</span>
<span class="ltx_bibblock">Llm critics help catch bugs in mathematics: Towards a better mathematical verifier with natural language feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2406.14024</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Leo Gao, John Schulman, and Jacob Hilton.

</span>
<span class="ltx_bibblock">Scaling laws for reward model overoptimization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp.  10835–10866. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2025)</span>
<span class="ltx_bibblock">
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.

</span>
<span class="ltx_bibblock">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2501.12948</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassid et al. (2025)</span>
<span class="ltx_bibblock">
Michael Hassid, Gabriel Synnaeve, Yossi Adi, and Roy Schwartz.

</span>
<span class="ltx_bibblock">Don’t overthink it. preferring shorter thinking chains for improved llm reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.17813</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2024)</span>
<span class="ltx_bibblock">
Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al.

</span>
<span class="ltx_bibblock">Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  3828–3850, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2025)</span>
<span class="ltx_bibblock">
Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al.

</span>
<span class="ltx_bibblock">Deepmath-103k: A large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2504.11456</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring mathematical problem solving with the MATH dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=7Bywt2mQsCe" title="">https://openreview.net/forum?id=7Bywt2mQsCe</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2025)</span>
<span class="ltx_bibblock">
Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum.

</span>
<span class="ltx_bibblock">Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.24290</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huan et al. (2025)</span>
<span class="ltx_bibblock">
Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, and Xiang Yue.

</span>
<span class="ltx_bibblock">Does math reasoning improve general llm capabilities? understanding transferability of llm reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2507.00432</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaech et al. (2024)</span>
<span class="ltx_bibblock">
Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al.

</span>
<span class="ltx_bibblock">Openai o1 system card.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2412.16720</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. (2025)</span>
<span class="ltx_bibblock">
Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.

</span>
<span class="ltx_bibblock">Livecodebench: Holistic and contamination free evaluation of large language models for code.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Thirteenth International Conference on Learning Representations</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=chfJJYC3iL" title="">https://openreview.net/forum?id=chfJJYC3iL</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2025)</span>
<span class="ltx_bibblock">
Yuhua Jiang, Yuwen Xiong, Yufeng Yuan, Chao Xin, Wenyuan Xu, Yu Yue, Qianchuan Zhao, and Lin Yan.

</span>
<span class="ltx_bibblock">Pag: Multi-turn reinforced llm self-correction with policy as generative verifier.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2506.10406</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.

</span>
<span class="ltx_bibblock">Alpacaeval: An automatic evaluator of instruction-following models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/alpaca_eval" title="">https://github.com/tatsu-lab/alpaca_eval</a>, 5 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2025)</span>
<span class="ltx_bibblock">
Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, and Wenbing Huang.

</span>
<span class="ltx_bibblock">Star-r1: Spatial transformation reasoning by reinforcing multimodal llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.15804</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lightman et al. (2023)</span>
<span class="ltx_bibblock">
Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.

</span>
<span class="ltx_bibblock">Let’s verify step by step.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024a)</span>
<span class="ltx_bibblock">
Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al.

</span>
<span class="ltx_bibblock">Deepseek-v3 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2412.19437</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024b)</span>
<span class="ltx_bibblock">
Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A. Smith.

</span>
<span class="ltx_bibblock">Tuning language models by proxy.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">First Conference on Language Modeling</em>, 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=dribhnhm1i" title="">https://openreview.net/forum?id=dribhnhm1i</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu &amp; Zhang (2025)</span>
<span class="ltx_bibblock">
Jiawei Liu and Lingming Zhang.

</span>
<span class="ltx_bibblock">Code-r1: Reproducing r1 for code with reliable rewards.

</span>
<span class="ltx_bibblock">2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2025a)</span>
<span class="ltx_bibblock">
Xiaoyuan Liu, Tian Liang, Zhiwei He, Jiahao Xu, Wenxuan Wang, Pinjia He, Zhaopeng Tu, Haitao Mi, and Dong Yu.

</span>
<span class="ltx_bibblock">Trust, but verify: A self-verification approach to reinforcement learning with verifiable rewards.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.13445</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2025b)</span>
<span class="ltx_bibblock">
Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin.

</span>
<span class="ltx_bibblock">Understanding r1-zero-like training: A critical perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.20783</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2025)</span>
<span class="ltx_bibblock">
Songshuo Lu, Hua Wang, Zhi Chen, and Yaohua Tang.

</span>
<span class="ltx_bibblock">Urpo: A unified reward &amp; policy optimization framework for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2507.17515</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2025)</span>
<span class="ltx_bibblock">
Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen.

</span>
<span class="ltx_bibblock">General-reasoner: Advancing llm reasoning across all domains.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.14652</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MetaAI (2024a)</span>
<span class="ltx_bibblock">
MetaAI.

</span>
<span class="ltx_bibblock">Introducing llama 3.1: Our most capable models to date.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/meta-llama-3-1/" title="">https://ai.meta.com/blog/meta-llama-3-1/</a>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MetaAI (2024b)</span>
<span class="ltx_bibblock">
MetaAI.

</span>
<span class="ltx_bibblock">Llama 3.2: Revolutionizing edge ai and vision with open, customizable models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" title="">https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/</a>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitchell et al. (2024)</span>
<span class="ltx_bibblock">
Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D Manning.

</span>
<span class="ltx_bibblock">An emulator for fine-tuning large language models using small language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Eo7kv0sllr" title="">https://openreview.net/forum?id=Eo7kv0sllr</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mukherjee et al. (2025)</span>
<span class="ltx_bibblock">
Sagnik Mukherjee, Lifan Yuan, Dilek Hakkani-Tur, and Hao Peng.

</span>
<span class="ltx_bibblock">Reinforcement learning finetunes small subnetworks in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.11711</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenCompass (2025)</span>
<span class="ltx_bibblock">
OpenCompass.

</span>
<span class="ltx_bibblock">Aime 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/opencompass/AIME2025" title="">https://huggingface.co/datasets/opencompass/AIME2025</a>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters &amp; Schaal (2007)</span>
<span class="ltx_bibblock">
Jan Peters and Stefan Schaal.

</span>
<span class="ltx_bibblock">Reinforcement learning by reward-weighted regression for operational space control.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 24th international conference on Machine learning</em>, pp.  745–750, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qwen Team (2024)</span>
<span class="ltx_bibblock">
Qwen Team.

</span>
<span class="ltx_bibblock">Qwen2. 5: A party of foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Qwen (Sept. 2024). url: https://qwenlm. github. io/blog/qwen2</em>, 5, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al. (2023)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 36:53728–53741, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rein et al. (2024)</span>
<span class="ltx_bibblock">
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman.

</span>
<span class="ltx_bibblock">Gpqa: A graduate-level google-proof q&amp;a benchmark.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">First Conference on Language Modeling</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sareen et al. (2025)</span>
<span class="ltx_bibblock">
Kusha Sareen, Morgane M Moss, Alessandro Sordoni, Rishabh Agarwal, and Arian Hosseini.

</span>
<span class="ltx_bibblock">Putting the value back in rl: Better test-time scaling by unifying llm reasoners with verifiers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.04842</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman et al. (2017)</span>
<span class="ltx_bibblock">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.

</span>
<span class="ltx_bibblock">Proximal policy optimization algorithms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1707.06347</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2024)</span>
<span class="ltx_bibblock">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.03300</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et al. (2024)</span>
<span class="ltx_bibblock">
Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu.

</span>
<span class="ltx_bibblock">Hybridflow: A flexible and efficient rlhf framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv: 2409.19256</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skywork-o1 (2024)</span>
<span class="ltx_bibblock">
Skywork-o1.

</span>
<span class="ltx_bibblock">Skywork-o1 open series.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/Skywork" title="">https://huggingface.co/Skywork</a>, November 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/Skywork" title="">https://huggingface.co/Skywork</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snell et al. (2024)</span>
<span class="ltx_bibblock">
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.

</span>
<span class="ltx_bibblock">Scaling llm test-time compute optimally can be more effective than scaling model parameters.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2408.03314</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutton et al. (1998)</span>
<span class="ltx_bibblock">
Richard S Sutton, Andrew G Barto, et al.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Reinforcement learning: An introduction</em>, volume 1.

</span>
<span class="ltx_bibblock">MIT press Cambridge, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2025a)</span>
<span class="ltx_bibblock">
Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al.

</span>
<span class="ltx_bibblock">Kimi k2: Open agentic intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2507.20534</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2025b)</span>
<span class="ltx_bibblock">
Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al.

</span>
<span class="ltx_bibblock">Kimi k1. 5: Scaling reinforcement learning with llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2501.12599</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024a)</span>
<span class="ltx_bibblock">
Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.

</span>
<span class="ltx_bibblock">Math-shepherd: Verify and reinforce llms step-by-step without human annotations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  9426–9439, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024b)</span>
<span class="ltx_bibblock">
Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al.

</span>
<span class="ltx_bibblock">Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2406.01574</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2025a)</span>
<span class="ltx_bibblock">
Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu.

</span>
<span class="ltx_bibblock">Octothinker: Mid-training incentivizes reinforcement learning scaling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2506.20512</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2025b)</span>
<span class="ltx_bibblock">
Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al.

</span>
<span class="ltx_bibblock">Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2504.20073</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. (2025)</span>
<span class="ltx_bibblock">
Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang, Junjie Li, Ziming Miao, et al.

</span>
<span class="ltx_bibblock">Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2506.14245</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024)</span>
<span class="ltx_bibblock">
An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang.

</span>
<span class="ltx_bibblock">Qwen2.5-math technical report: Toward mathematical expert model via self-improvement.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2409.12122</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2025a)</span>
<span class="ltx_bibblock">
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al.

</span>
<span class="ltx_bibblock">Qwen3 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.09388</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2025b)</span>
<span class="ltx_bibblock">
Wenkai Yang, Jingwen Chen, Yankai Lin, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Deepcritic: Deliberate critique with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.00662</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2025a)</span>
<span class="ltx_bibblock">
Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al.

</span>
<span class="ltx_bibblock">Dapo: An open-source llm reinforcement learning system at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.14476</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2025b)</span>
<span class="ltx_bibblock">
Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, et al.

</span>
<span class="ltx_bibblock">Rlpr: Extrapolating rlvr to general domains without verifiers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2506.18254</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2024)</span>
<span class="ltx_bibblock">
Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng.

</span>
<span class="ltx_bibblock">Free process rewards without process labels.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2412.01981</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al. (2025a)</span>
<span class="ltx_bibblock">
Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang.

</span>
<span class="ltx_bibblock">Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2504.13837</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al. (2025b)</span>
<span class="ltx_bibblock">
Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al.

</span>
<span class="ltx_bibblock">Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2504.05118</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zha et al. (2025)</span>
<span class="ltx_bibblock">
Kaiwen Zha, Zhengqi Gao, Maohao Shen, Zhang-Wei Hong, Duane S Boning, and Dina Katabi.

</span>
<span class="ltx_bibblock">Rl tango: Reinforcing generator and verifier together for language reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.15034</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal.

</span>
<span class="ltx_bibblock">Generative verifiers: Reward modeling as next-token prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2408.15240</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2025)</span>
<span class="ltx_bibblock">
Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin.

</span>
<span class="ltx_bibblock">The lessons of developing process reward models in mathematical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2501.07301</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2025)</span>
<span class="ltx_bibblock">
Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, et al.

</span>
<span class="ltx_bibblock">Genprm: Scaling test-time compute of process reward models via generative reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2504.00891</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2025)</span>
<span class="ltx_bibblock">
Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al.

</span>
<span class="ltx_bibblock">Group sequence policy optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2507.18071</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2025)</span>
<span class="ltx_bibblock">
Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, and Chao Du.

</span>
<span class="ltx_bibblock">Reinforcing general reasoning without verifiers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.21493</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuo et al. (2025)</span>
<span class="ltx_bibblock">
Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al.

</span>
<span class="ltx_bibblock">Ttrl: Test-time reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2504.16084</em>, 2025.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>The Length Bias in Implicit Reward</h2>
<figure class="ltx_figure" id="A1.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A1.F11.fig1" style="width:212.5pt;">
<p class="ltx_p ltx_align_center"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="618" id="A1.F11.g1" src="x2.png" width="830"/></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Cumulative implicit reward values across 32 reasoning trajectories sampled from Open-Reasoner-Zero-7B on an AIME2024 problem. Red lines correspond to wrong solutions and green lines correspond to correct solutions.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A1.F11.fig2" style="width:212.5pt;">
<p class="ltx_p ltx_align_center"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="618" id="A1.F11.g2" src="x3.png" width="830"/></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>The mean and standard deviation of <math alttext="-\log\pi_{ref}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="A1.F11.m4" intent=":literal"><semantics><mrow><mo rspace="0.167em">−</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">-\log\pi_{ref}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> under different combinations of reference model <math alttext="\pi_{ref}" class="ltx_Math" display="inline" id="A1.F11.m5" intent=":literal"><semantics><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><annotation encoding="application/x-tex">\pi_{ref}</annotation></semantics></math> and pre-specified token <math alttext="z_{c}" class="ltx_Math" display="inline" id="A1.F11.m6" intent=":literal"><semantics><msub><mi>z</mi><mi>c</mi></msub><annotation encoding="application/x-tex">z_{c}</annotation></semantics></math> over 300 input-output pairs.</figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p">Here, we present the trend of the cumulative implicit reward values (<math alttext="\log\frac{\pi_{\bm{\theta}}(\bm{y}_{&lt;i}|\bm{x})}{\pi_{ref}(\bm{y}_{&lt;i}|\bm{x})}" class="ltx_Math" display="inline" id="A1.p1.m1" intent=":literal"><semantics><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mfrac><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒚</mi><mrow><mi></mi><mo>&lt;</mo><mi>i</mi></mrow></msub><mo fence="false">|</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝒚</mi><mrow><mi></mi><mo>&lt;</mo><mi>i</mi></mrow></msub><mo fence="false">|</mo><mi>𝒙</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\log\frac{\pi_{\bm{\theta}}(\bm{y}_{&lt;i}|\bm{x})}{\pi_{ref}(\bm{y}_{&lt;i}|\bm{x})}</annotation></semantics></math> where <math alttext="\pi_{ref}" class="ltx_Math" display="inline" id="A1.p1.m2" intent=":literal"><semantics><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><annotation encoding="application/x-tex">\pi_{ref}</annotation></semantics></math> is Qwen2.5-7B-Base) across 32 reasoning trajectories sampled from Open-Reasoner-Zero-7B on an AIME2024 problem, showing how they vary with the increasing trajectory lengths. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A1.F11" title="Figure 11 ‣ Appendix A The Length Bias in Implicit Reward ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">11</span></a>, the curves of all samples exhibit a positive correlation between the implicit reward and the number of tokens, and longer trajectories tend to yield higher final implicit reward scores, indicating a strong length bias in implicit reward. Since incorrect solutions are generally longer than correct ones in reasoning tasks <cite class="ltx_cite ltx_citemacro_citep">(Hassid et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib12" title="">2025</a>)</cite>, implicit reward is therefore not a reliable indicator of the relative quality of reasoning paths at test time.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Statistics of <math alttext="\log\pi_{ref}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="A2.m1" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log\pi_{ref}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math>
</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p">We present the mean and standard deviation of <math alttext="-\log\pi_{ref}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="A2.p1.m1" intent=":literal"><semantics><mrow><mo rspace="0.167em">−</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">-\log\pi_{ref}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> computed over 300 input-output pairs. The reference model <math alttext="\pi_{ref}" class="ltx_Math" display="inline" id="A2.p1.m2" intent=":literal"><semantics><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub><annotation encoding="application/x-tex">\pi_{ref}</annotation></semantics></math> is chosen as either Qwen2.5-7B-Base or OctoThinker-3B-Short-Base, and the evaluation is performed under two different choices of <math alttext="z_{c}" class="ltx_Math" display="inline" id="A2.p1.m3" intent=":literal"><semantics><msub><mi>z</mi><mi>c</mi></msub><annotation encoding="application/x-tex">z_{c}</annotation></semantics></math> for each reference model (one common token and one unused special token): “<span class="ltx_text ltx_font_typewriter">Yes</span>” and “<span class="ltx_text ltx_font_typewriter">&lt;vision_start&gt;</span>” for Qwen2.5-7B-Base, “<span class="ltx_text ltx_font_typewriter">Yes</span>” and “<span class="ltx_text ltx_font_typewriter">&lt;|reserved_special_token_0|&gt;</span>” for OctoThinker-3B-Short-Base. The results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A1.F11" title="Figure 11 ‣ Appendix A The Length Bias in Implicit Reward ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">11</span></a> indicates that <math alttext="-\log\pi_{ref}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="A2.p1.m4" intent=":literal"><semantics><mrow><mo rspace="0.167em">−</mo><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">-\log\pi_{ref}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> remains nearly constant and extremely small, with only a low standard deviation across different <math alttext="\bm{x}" class="ltx_Math" display="inline" id="A2.p1.m5" intent=":literal"><semantics><mi>𝒙</mi><annotation encoding="application/x-tex">\bm{x}</annotation></semantics></math> and <math alttext="\bm{y}" class="ltx_Math" display="inline" id="A2.p1.m6" intent=":literal"><semantics><mi>𝒚</mi><annotation encoding="application/x-tex">\bm{y}</annotation></semantics></math>. Thus, we can consider <math alttext="\log\pi_{ref}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="A2.p1.m7" intent=":literal"><semantics><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log\pi_{ref}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> as a constant when calculating the last-token self-rewarding scores, which effectively reduces the computational cost by half.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Detailed Training Settings</h2>
<figure class="ltx_table" id="A3.T5">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.T5.fig1" style="width:208.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 4: </span>Basic training hyper-parameters of both GRPO and LaSeR.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Hyper-parameter</th>
<td class="ltx_td ltx_align_left ltx_border_tt">Value</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Train Batch Size</th>
<td class="ltx_td ltx_align_left ltx_border_t">128</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Micro Batch Size</th>
<td class="ltx_td ltx_align_left">128</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Rollout <math alttext="n" class="ltx_Math" display="inline" id="A3.T5.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left">8</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Maximum Prompt Length</th>
<td class="ltx_td ltx_align_left">2048</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Maximum Response Length</th>
<td class="ltx_td ltx_align_left">8192</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Temperature</th>
<td class="ltx_td ltx_align_left">1.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Top <math alttext="p" class="ltx_Math" display="inline" id="A3.T5.m2" intent=":literal"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left">1.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">LR</th>
<td class="ltx_td ltx_align_left"><math alttext="1\times 10^{-6}" class="ltx_Math" display="inline" id="A3.T5.m3" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-6}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">KL Coefficient</th>
<td class="ltx_td ltx_align_left ltx_border_bb">0.0</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="A3.T5.fig2" style="width:208.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 5: </span>Unique training hyper-parameters of LaSeR.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">Hyper-parameter</td>
<td class="ltx_td ltx_align_left ltx_border_tt">Value</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Coefficient <math alttext="\beta_{v}" class="ltx_Math" display="inline" id="A3.T5.m4" intent=":literal"><semantics><msub><mi>β</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\beta_{v}</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t">0.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Loss Weight <math alttext="\alpha" class="ltx_Math" display="inline" id="A3.T5.m5" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left">0.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Self-Rewarding Adv. Weight <math alttext="\tau" class="ltx_Math" display="inline" id="A3.T5.m6" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left">0.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Reasoning Warm-Up Steps</td>
<td class="ltx_td ltx_align_left">200</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">Self-Rewarding Warm-Up Steps</td>
<td class="ltx_td ltx_align_left ltx_border_bb">200</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p">We use <span class="ltx_text ltx_font_typewriter">verl</span> <cite class="ltx_cite ltx_citemacro_citep">(Sheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib43" title="">2024</a>)</cite> as our RL training framework. The basic training hyper-parameters in both GRPO training and LaSeR training for each model are put in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A3.T5" title="Table 5 ‣ Appendix C Detailed Training Settings ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">5</span></a>, and the newly introduced training hyper-parameters for LaSeR are put in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A3.T5" title="Table 5 ‣ Appendix C Detailed Training Settings ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">5</span></a>. The number of optimization steps is 1000 for Qwen2.5-7B-Base and OctoThinker-3B-Short-Base, and 500 for Open-Reasoner-Zero-7B.
In RL, a reasoning reward of 1.0 is given if the final answer and the answer format are both correct; otherwise, it is 0.0. In our method, the reasoning warm-up is performed for Qwen2.5-7B-Base and OctoThinker-3B-Short-Base only, and the self-rewarding warm-up is performed for all models.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Ablation Studies on Self-Rewarding Hyper-Parameters</h2>
<figure class="ltx_figure ltx_figure_panel" id="A4.F13.sf1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="53" id="A4.g1" src="x4.png" width="789"/></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>The curves of training rewards and training self-verification F1 scores under different combinations of hyper-parameters with EMA smoothing (EMA coef.=0.9).
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel">Here, we display the curves (with Exponential Moving Average (EMA) smoothing) of training rewards and training self-verification F1 scores of our method under different choices of coefficient <math alttext="\beta_{v}" class="ltx_Math" display="inline" id="A4.m1" intent=":literal"><semantics><msub><mi>β</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\beta_{v}</annotation></semantics></math> and self-rewarding MSE loss weight <math alttext="\alpha" class="ltx_Math" display="inline" id="A4.m2" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>. The experiments are conducted on Open-Reasoner-Zero-7B, which help to skip the reasoning warm-up phase compared with using Qwen2.5-7B-Base and OctoThinker-3B-Short-Base, while the results are similar in other two base models after reasoning warm-up. The dynamics of training rewards and training self-verification F1 scores are displayed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A4.F13.sf1" title="Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">13(a)</span></a>. As we can see, assigning a larger weight <math alttext="\alpha" class="ltx_Math" display="inline" id="A4.m3" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> to the last-token self-rewarding loss has a more detrimental impact on the model’s reasoning capabilities. On the other hand, the coefficient <math alttext="\beta_{v}" class="ltx_Math" display="inline" id="A4.m4" intent=":literal"><semantics><msub><mi>β</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\beta_{v}</annotation></semantics></math> has little impact on optimizing the self-rewarding scores, as long as it remains within a reasonable range (<math alttext="0.1\sim 0.5" class="ltx_Math" display="inline" id="A4.m5" intent=":literal"><semantics><mrow><mn>0.1</mn><mo>∼</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">0.1\sim 0.5</annotation></semantics></math>). However, much smaller values of <math alttext="\beta_{v}" class="ltx_Math" display="inline" id="A4.m6" intent=":literal"><semantics><msub><mi>β</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\beta_{v}</annotation></semantics></math> can impair the model’s reasoning capability, as indicated by the analysis in the end of Section <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.SS4" title="3.4 Brief Discussion ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">3.4</span></a>. For example, when <math alttext="\beta_{v}=0.05" class="ltx_Math" display="inline" id="A4.m7" intent=":literal"><semantics><mrow><msub><mi>β</mi><mi>v</mi></msub><mo>=</mo><mn>0.05</mn></mrow><annotation encoding="application/x-tex">\beta_{v}=0.05</annotation></semantics></math>, we should have <math alttext="\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})=e^{-3}\approx 0.05" class="ltx_Math" display="inline" id="A4.m8" intent=":literal"><semantics><mrow><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>3</mn></mrow></msup><mo>≈</mo><mn>0.05</mn></mrow><annotation encoding="application/x-tex">\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})=e^{-3}\approx 0.05</annotation></semantics></math> under
<math alttext="\pi_{\text{ref}}(z_{c}|\bm{x},\bm{y})=e^{-23}" class="ltx_Math" display="inline" id="A4.m9" intent=":literal"><semantics><mrow><mrow><msub><mi>π</mi><mtext>ref</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>23</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\pi_{\text{ref}}(z_{c}|\bm{x},\bm{y})=e^{-23}</annotation></semantics></math> and <math alttext="r_{v}(\bm{x},\bm{y})=1" class="ltx_Math" display="inline" id="A4.m10" intent=":literal"><semantics><mrow><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_{v}(\bm{x},\bm{y})=1</annotation></semantics></math>, then the large value of <math alttext="\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="A4.m11" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> causes large
interference with the optimization of reasoning capability. In our main experiments, we choose <math alttext="(\beta_{v},\alpha)=(0.1,0.1)" class="ltx_Math" display="inline" id="A4.m12" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>β</mi><mi>v</mi></msub><mo>,</mo><mi>α</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><mo stretchy="false">(</mo><mn>0.1</mn><mo>,</mo><mn>0.1</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(\beta_{v},\alpha)=(0.1,0.1)</annotation></semantics></math>.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_appendix ltx_figure_panel" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability</h2>
<figure class="ltx_figure ltx_figure_panel" id="A5.F13.sf2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="54" id="A5.g1" src="x5.png" width="788"/></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>The curves of training rewards and training self-verification F1 scores of our method with and without class-level loss re-weighting practice (EMA coef.=0.9).
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel">We present the training dynamics of our method on Open-Reasoner-Zero-7B, with and without class-level loss re-weighting, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A5.F13.sf2" title="Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">13(b)</span></a> for comparison. As shown, applying loss re-weighting leads to a more balanced self-verification performance by mitigating the bias toward the majority class with larger sample size, while still maintaining high reasoning accuracy.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_appendix ltx_figure_panel" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss</h2>
<div class="ltx_para ltx_noindent" id="A6.p1">
<p class="ltx_p">Following the discussion in Section <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#S3.SS4" title="3.4 Brief Discussion ‣ 3 Methodology ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">3.4</span></a>, we compare the training performance of our introduced last-token self-rewarding loss with the supervised fine-tuning (SFT) loss on Open-Reasoner-Zero-7B. The training dynamics are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A6.F13.sf3" title="Figure 13(c) ‣ Appendix F Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss ‣ Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">13(c)</span></a>. As observed, applying the SFT loss to optimize the self-rewarding capability causes substantial interference with the optimization of reasoning capability, leading to a marked degradation in training rewards. Moreover, the SFT loss degrades extremely slowly, indicating that directly driving <math alttext="\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="A6.p1.m1" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> from <math alttext="0" class="ltx_Math" display="inline" id="A6.p1.m2" intent=":literal"><mn>0</mn></math> to <math alttext="1" class="ltx_Math" display="inline" id="A6.p1.m3" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> for <math alttext="r_{v}(\bm{x},\bm{y})=1" class="ltx_Math" display="inline" id="A6.p1.m4" intent=":literal"><semantics><mrow><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_{v}(\bm{x},\bm{y})=1</annotation></semantics></math> is inherently difficult. However, our method only requires fitting <math alttext="\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="A6.p1.m5" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>𝜽</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\bm{\theta}}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> to <math alttext="\exp(1/\beta_{v})\cdot\pi_{\text{ref}}(z_{c}|\bm{x},\bm{y})" class="ltx_Math" display="inline" id="A6.p1.m6" intent=":literal"><semantics><mrow><mrow><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><msub><mi>β</mi><mi>v</mi></msub></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⋅</mo><msub><mi>π</mi><mtext>ref</mtext></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>z</mi><mi>c</mi></msub><mo fence="false">|</mo><mrow><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\exp(1/\beta_{v})\cdot\pi_{\text{ref}}(z_{c}|\bm{x},\bm{y})</annotation></semantics></math> for <math alttext="r_{v}(\bm{x},\bm{y})=1" class="ltx_Math" display="inline" id="A6.p1.m7" intent=":literal"><semantics><mrow><mrow><msub><mi>r</mi><mi>v</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝒙</mi><mo>,</mo><mi>𝒚</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_{v}(\bm{x},\bm{y})=1</annotation></semantics></math>, which is considerably easier and introduces much less interference.</p>
</div>
<figure class="ltx_figure ltx_figure_panel" id="A6.fig2">
<table class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="53" id="A6.g1" src="x6.png" width="788"/></td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_break"/>
<div class="ltx_block">
<figure class="ltx_figure" id="A6.F13.sf3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>The comparison of the training dynamics between the last-token self-rewarding loss and the SFT loss.
</figcaption>
</figure>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Detailed Self-Verification Results</h2>
<figure class="ltx_table" id="A7.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Detailed self-verification results.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_r ltx_align_left">Method</td>
</tr>
</table>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">MATH500</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">AMC23</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">AIME24</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">AIME25</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">Olym.</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t">Acc.</td>
<td class="ltx_td ltx_align_center ltx_border_t">F1</td>
<td class="ltx_td ltx_align_center ltx_border_t">Acc.</td>
<td class="ltx_td ltx_align_center ltx_border_t">F1</td>
<td class="ltx_td ltx_align_center ltx_border_t">Acc.</td>
<td class="ltx_td ltx_align_center ltx_border_t">F1</td>
<td class="ltx_td ltx_align_center ltx_border_t">Acc.</td>
<td class="ltx_td ltx_align_center ltx_border_t">F1</td>
<td class="ltx_td ltx_align_center ltx_border_t">Acc.</td>
<td class="ltx_td ltx_align_center ltx_border_t">F1</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="11"><em class="ltx_emph ltx_font_italic"> <span class="ltx_text ltx_font_bold">OctoThinker-3B-Short-Base</span></em></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Base</th>
<td class="ltx_td ltx_align_center">60.2</td>
<td class="ltx_td ltx_align_center">22.3</td>
<td class="ltx_td ltx_align_center">52.3</td>
<td class="ltx_td ltx_align_center">11.2</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">62.0</td>
<td class="ltx_td ltx_align_center">13.7</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">GRPO</th>
<td class="ltx_td ltx_align_center">58.2</td>
<td class="ltx_td ltx_align_center">56.9</td>
<td class="ltx_td ltx_align_center">66.7</td>
<td class="ltx_td ltx_align_center">47.3</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">66.4</td>
<td class="ltx_td ltx_align_center">48.8</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">LaSeR</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">77.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">73.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">77.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">70.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">80.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">73.6</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">     - <span class="ltx_text ltx_font_italic">SWA</span></span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">81.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">80.4</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">84.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">70.9</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">-</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">83.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">66.0</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="11"><em class="ltx_emph ltx_font_italic"> <span class="ltx_text ltx_font_bold">Qwen2.5-7B-Base</span></em></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Base</th>
<td class="ltx_td ltx_align_center">45.0</td>
<td class="ltx_td ltx_align_center">36.4</td>
<td class="ltx_td ltx_align_center">30.7</td>
<td class="ltx_td ltx_align_center">30.8</td>
<td class="ltx_td ltx_align_center">24.5</td>
<td class="ltx_td ltx_align_center">27.6</td>
<td class="ltx_td ltx_align_center">28.2</td>
<td class="ltx_td ltx_align_center">32.9</td>
<td class="ltx_td ltx_align_center">33.8</td>
<td class="ltx_td ltx_align_center">36.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">GRPO</th>
<td class="ltx_td ltx_align_center">76.5</td>
<td class="ltx_td ltx_align_center">54.6</td>
<td class="ltx_td ltx_align_center">61.1</td>
<td class="ltx_td ltx_align_center">59.7</td>
<td class="ltx_td ltx_align_center">60.4</td>
<td class="ltx_td ltx_align_center">36.6</td>
<td class="ltx_td ltx_align_center">72.5</td>
<td class="ltx_td ltx_align_center">41.5</td>
<td class="ltx_td ltx_align_center">54.6</td>
<td class="ltx_td ltx_align_center">53.5</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">LaSeR</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">88.0</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">83.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">81.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">82.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">92.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">79.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">90.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">74.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">79.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">78.3</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">     - <span class="ltx_text ltx_font_italic">SWA</span></span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">87.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">79.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">79.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">80.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">94.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">81.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">92.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">74.9</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">83.9</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">83.3</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="11"><em class="ltx_emph ltx_font_italic"> <span class="ltx_text ltx_font_bold">Open-Reasoner-Zero-7B</span></em></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Base</th>
<td class="ltx_td ltx_align_center">79.6</td>
<td class="ltx_td ltx_align_center">26.7</td>
<td class="ltx_td ltx_align_center">66.6</td>
<td class="ltx_td ltx_align_center">51.3</td>
<td class="ltx_td ltx_align_center">39.6</td>
<td class="ltx_td ltx_align_center">45.9</td>
<td class="ltx_td ltx_align_center">47.6</td>
<td class="ltx_td ltx_align_center">55.2</td>
<td class="ltx_td ltx_align_center">55.2</td>
<td class="ltx_td ltx_align_center">37.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">GRPO</th>
<td class="ltx_td ltx_align_center">52.9</td>
<td class="ltx_td ltx_align_center">57.1</td>
<td class="ltx_td ltx_align_center">50.9</td>
<td class="ltx_td ltx_align_center">44.8</td>
<td class="ltx_td ltx_align_center">66.9</td>
<td class="ltx_td ltx_align_center">14.6</td>
<td class="ltx_td ltx_align_center">78.9</td>
<td class="ltx_td ltx_align_center">28.1</td>
<td class="ltx_td ltx_align_center">54.7</td>
<td class="ltx_td ltx_align_center">49.5</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">LaSeR</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">90.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">87.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">77.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">79.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">87.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">64.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">92.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">77.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">80.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">78.7</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E6FFFF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">     - <span class="ltx_text ltx_font_italic">SWA</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">89.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">87.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">76.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">77.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">87.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">63.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">93.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">77.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">80.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text" style="--ltx-bg-color:#E6FFFF;">77.9</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="A7.p1">
<p class="ltx_p">We report the detailed self-verification results of each model on self-generated solutions across all benchmarks in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A7.T6" title="Table 6 ‣ Appendix G Detailed Self-Verification Results ‣ Appendix F Comparison between Last-Token Self-Rewarding Loss and Supervised Fine-Tuning Loss ‣ Figure 13(b) ‣ Appendix E The Effect of Class-Level Re-Weighting on The Balanced Self-Verification Capability ‣ Figure 13(a) ‣ Appendix D Ablation Studies on Self-Rewarding Hyper-Parameters ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">6</span></a>, including both overall accuracy and F1 score. Our method consistently yields significant improvements in model’s self-rewarding and self-verification capabilities, while incurring only minimal additional computational cost.</p>
</div>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Training and Evaluation Settings in General Reasoning Experiments</h2>
<div class="ltx_para ltx_noindent" id="A8.p1">
<p class="ltx_p">The basic training and testing hyper-parameters for experiments on WebInstruct-verified are the same as those in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A3.T5" title="Table 5 ‣ Appendix C Detailed Training Settings ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">5</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#A3.T5" title="Table 5 ‣ Appendix C Detailed Training Settings ‣ 6 Conclusion ‣ 5.3 Further Reduction or Increase of Self-Rewarding Cost ‣ Figure 10(a) ‣ 5.2 The Generalizability of LaSeR to General Reasoning Domain ‣ 5 Analysis and Discussion ‣ Figure 7(a) ‣ 4.3 Inference-Time Scaling Results ‣ 4 Experiments ‣ LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"><span class="ltx_text ltx_ref_tag">5</span></a>, while the number of optimization steps here is 800. The simplified constant of the reference log-probability <math alttext="c_{\text{ref}}" class="ltx_Math" display="inline" id="A8.p1.m1" intent=":literal"><semantics><msub><mi>c</mi><mtext>ref</mtext></msub><annotation encoding="application/x-tex">c_{\text{ref}}</annotation></semantics></math> is <math alttext="-23.0" class="ltx_Math" display="inline" id="A8.p1.m2" intent=":literal"><semantics><mrow><mo>−</mo><mn>23.0</mn></mrow><annotation encoding="application/x-tex">-23.0</annotation></semantics></math>. We do not employ the advantage integration strategy here, as we find that the optimized self-rewarding capability of Qwen3-4B-LaSeR on general reasoning tasks is limited, and introducing self-rewarding-based advantage integration leads to performance degradation.</p>
</div>
<section class="ltx_appendix" id="A9">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Prompt Templates</h2>
<div class="ltx_para ltx_noindent" id="A9.p1">
<p class="ltx_p">We show the training, evaluation and self-verification prompt templates used in our experiments in the end.

</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
<div class="ltx_para ltx_noindent" id="A9.p2">
<span class="ltx_inline-block"><svg class="ltx_picture" height="515.05" id="A9.p2.pic1" overflow="visible" version="1.1" viewbox="0 0 600 515.05" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,515.05) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0" style="--ltx-fill-color:#000000;"><path d="M 0 5.91 L 0 509.14 C 0 512.4 2.64 515.05 5.91 515.05 L 594.09 515.05 C 597.36 515.05 600 512.4 600 509.14 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0" style="--ltx-fill-color:#FFFFFF;"><path d="M 1.97 5.91 L 1.97 490.94 L 598.03 490.94 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0" style="--ltx-fill-color:#E6E6E6;"><path d="M 1.97 492.9 L 1.97 509.14 C 1.97 511.32 3.73 513.08 5.91 513.08 L 594.09 513.08 C 596.27 513.08 598.03 511.32 598.03 509.14 L 598.03 492.9 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 499.53)"><foreignobject color="#000000" height="12.3" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Training and Evaluation Prompt Template for OctoThinker-3B-Short-Base</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="465.35" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:33.63em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 465.35)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p2.pic1.m1" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>bos_token<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p2.pic1.m2" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.</span>
<span class="ltx_p">User: You must put your answer inside <code class="ltx_verbatim ltx_font_typewriter">\</code>boxed{} and Your final answer will be extracted automatically by the <code class="ltx_verbatim ltx_font_typewriter">\</code>boxed{} tag.</span>
<span class="ltx_p">{question}</span>
<span class="ltx_p">Assistant:</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A9.p3">
<span class="ltx_inline-block"><svg class="ltx_picture" height="482.38" id="A9.p3.pic1" overflow="visible" version="1.1" viewbox="0 0 600 482.38" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,482.38) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0" style="--ltx-fill-color:#000000;"><path d="M 0 5.91 L 0 476.47 C 0 479.74 2.64 482.38 5.91 482.38 L 594.09 482.38 C 597.36 482.38 600 479.74 600 476.47 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0" style="--ltx-fill-color:#FFFFFF;"><path d="M 1.97 5.91 L 1.97 458.27 L 598.03 458.27 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0" style="--ltx-fill-color:#E6E6E6;"><path d="M 1.97 460.24 L 1.97 476.47 C 1.97 478.65 3.73 480.41 5.91 480.41 L 594.09 480.41 C 596.27 480.41 598.03 478.65 598.03 476.47 L 598.03 460.24 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 466.86)"><foreignobject color="#000000" height="12.3" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Training Prompt Template for Qwen2.5-7B-Base</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 14.32)"><foreignobject color="#000000" height="432.68" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:31.23em;--fo_depth :0.04em;" transform="matrix(1 0 0 -1 0 432.14)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m1" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>bos_token<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m2" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m3" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>think<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m4" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m5" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/think<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m6" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> and answer is enclosed within <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m7" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m8" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m9" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m10" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> tags, respectively, i.e., <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m11" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>think<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m12" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> reasoning process here <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m13" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/think<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m14" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m15" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m16" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> answer here <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m17" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m18" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math>.</span>
<span class="ltx_p">User: You must put your answer inside <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m19" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m20" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m21" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m22" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> tags, i.e., <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m23" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m24" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> answer here <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m25" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m26" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math>. And your final answer will be extracted automatically by the <code class="ltx_verbatim ltx_font_typewriter">\</code>boxed{} tag.</span>
<span class="ltx_p">This is the problem:</span>
<span class="ltx_p">{question}</span>
<span class="ltx_p">Assistant: <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m27" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>think<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p3.pic1.m28" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math></span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A9.p4">
<span class="ltx_inline-block"><svg class="ltx_picture" height="485.99" id="A9.p4.pic1" overflow="visible" version="1.1" viewbox="0 0 600 485.99" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,485.99) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0" style="--ltx-fill-color:#000000;"><path d="M 0 5.91 L 0 480.08 C 0 483.35 2.64 485.99 5.91 485.99 L 594.09 485.99 C 597.36 485.99 600 483.35 600 480.08 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0" style="--ltx-fill-color:#FFFFFF;"><path d="M 1.97 5.91 L 1.97 461.88 L 598.03 461.88 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0" style="--ltx-fill-color:#E6E6E6;"><path d="M 1.97 463.85 L 1.97 480.08 C 1.97 482.26 3.73 484.02 5.91 484.02 L 594.09 484.02 C 596.27 484.02 598.03 482.26 598.03 480.08 L 598.03 463.85 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 470.47)"><foreignobject color="#000000" height="12.3" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Zero-Shot Evaluation Prompt Template for Qwen2.5-7B-Base</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 17.24)"><foreignobject color="#000000" height="436.29" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:31.28em;--fo_depth :0.25em;" transform="matrix(1 0 0 -1 0 432.83)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><math alttext="&lt;|" class="ltx_math_unparsed" display="inline" id="A9.p4.pic1.m1" intent=":literal"><semantics><mrow><mo rspace="0em">&lt;</mo><mo fence="false" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">&lt;|</annotation></semantics></math>im_start<math alttext="|&gt;" class="ltx_math_unparsed" display="inline" id="A9.p4.pic1.m2" intent=":literal"><semantics><mrow><mo fence="false" stretchy="false">|</mo><mo lspace="0.167em">&gt;</mo></mrow><annotation encoding="application/x-tex">|&gt;</annotation></semantics></math>system</span>
<span class="ltx_p">You are a helpful assistant.<math alttext="&lt;|" class="ltx_math_unparsed" display="inline" id="A9.p4.pic1.m3" intent=":literal"><semantics><mrow><mo rspace="0em">&lt;</mo><mo fence="false" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">&lt;|</annotation></semantics></math>im_end<math alttext="|&gt;" class="ltx_math_unparsed" display="inline" id="A9.p4.pic1.m4" intent=":literal"><semantics><mrow><mo fence="false" stretchy="false">|</mo><mo lspace="0.167em">&gt;</mo></mrow><annotation encoding="application/x-tex">|&gt;</annotation></semantics></math></span>
<span class="ltx_p"><math alttext="&lt;|" class="ltx_math_unparsed" display="inline" id="A9.p4.pic1.m5" intent=":literal"><semantics><mrow><mo rspace="0em">&lt;</mo><mo fence="false" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">&lt;|</annotation></semantics></math>im_start<math alttext="|&gt;" class="ltx_math_unparsed" display="inline" id="A9.p4.pic1.m6" intent=":literal"><semantics><mrow><mo fence="false" stretchy="false">|</mo><mo lspace="0.167em">&gt;</mo></mrow><annotation encoding="application/x-tex">|&gt;</annotation></semantics></math>user</span>
<span class="ltx_p">{question}</span>
<span class="ltx_p">Please reason step by step, and put your final answer within <code class="ltx_verbatim ltx_font_typewriter">\</code>boxed{}.<math alttext="&lt;|" class="ltx_math_unparsed" display="inline" id="A9.p4.pic1.m7" intent=":literal"><semantics><mrow><mo rspace="0em">&lt;</mo><mo fence="false" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">&lt;|</annotation></semantics></math>im_end<math alttext="|&gt;" class="ltx_math_unparsed" display="inline" id="A9.p4.pic1.m8" intent=":literal"><semantics><mrow><mo fence="false" stretchy="false">|</mo><mo lspace="0.167em">&gt;</mo></mrow><annotation encoding="application/x-tex">|&gt;</annotation></semantics></math></span>
<span class="ltx_p"><math alttext="&lt;|" class="ltx_math_unparsed" display="inline" id="A9.p4.pic1.m9" intent=":literal"><semantics><mrow><mo rspace="0em">&lt;</mo><mo fence="false" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">&lt;|</annotation></semantics></math>im_start<math alttext="|&gt;" class="ltx_math_unparsed" display="inline" id="A9.p4.pic1.m10" intent=":literal"><semantics><mrow><mo fence="false" stretchy="false">|</mo><mo lspace="0.167em">&gt;</mo></mrow><annotation encoding="application/x-tex">|&gt;</annotation></semantics></math>assistant</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A9.p5">
<span class="ltx_inline-block"><svg class="ltx_picture" height="399.28" id="A9.p5.pic1" overflow="visible" version="1.1" viewbox="0 0 600 399.28" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,399.28) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0" style="--ltx-fill-color:#000000;"><path d="M 0 5.91 L 0 393.37 C 0 396.64 2.64 399.28 5.91 399.28 L 594.09 399.28 C 597.36 399.28 600 396.64 600 393.37 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0" style="--ltx-fill-color:#FFFFFF;"><path d="M 1.97 5.91 L 1.97 375.17 L 598.03 375.17 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0" style="--ltx-fill-color:#E6E6E6;"><path d="M 1.97 377.14 L 1.97 393.37 C 1.97 395.55 3.73 397.31 5.91 397.31 L 594.09 397.31 C 596.27 397.31 598.03 395.55 598.03 393.37 L 598.03 377.14 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 383.77)"><foreignobject color="#000000" height="12.3" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Training and Evaluation Prompt Template for Open-Reasoner-Zero-7B</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 14.32)"><foreignobject color="#000000" height="349.58" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:25.22em;--fo_depth :0.04em;" transform="matrix(1 0 0 -1 0 349.04)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p">A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m1" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>think<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m2" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m3" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/think<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m4" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> and answer is enclosed within <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m5" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m6" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m7" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m8" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> tags, respectively, i.e., <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m9" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>think<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m10" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> reasoning process here <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m11" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/think<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m12" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m13" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m14" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> answer here <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m15" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m16" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math>.</span>
<span class="ltx_p">User: You must put your answer inside <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m17" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m18" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m19" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m20" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> tags, i.e., <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m21" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m22" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math> answer here <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m23" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>/answer<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m24" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math>. And your final answer will be extracted automatically by the <code class="ltx_verbatim ltx_font_typewriter">\</code>boxed{} tag.</span>
<span class="ltx_p">{question}</span>
<span class="ltx_p">Assistant: <math alttext="&lt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m25" intent=":literal"><semantics><mo>&lt;</mo><annotation encoding="application/x-tex">&lt;</annotation></semantics></math>think<math alttext="&gt;" class="ltx_Math" display="inline" id="A9.p5.pic1.m26" intent=":literal"><semantics><mo>&gt;</mo><annotation encoding="application/x-tex">&gt;</annotation></semantics></math></span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A9.p6">
<span class="ltx_inline-block"><svg class="ltx_picture" height="336.55" id="A9.p6.pic1" overflow="visible" version="1.1" viewbox="0 0 600 336.55" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,336.55) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0" style="--ltx-fill-color:#000000;"><path d="M 0 5.91 L 0 330.64 C 0 333.91 2.64 336.55 5.91 336.55 L 594.09 336.55 C 597.36 336.55 600 333.91 600 330.64 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0" style="--ltx-fill-color:#FFFFFF;"><path d="M 1.97 5.91 L 1.97 312.44 L 598.03 312.44 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0" style="--ltx-fill-color:#E6E6E6;"><path d="M 1.97 314.41 L 1.97 330.64 C 1.97 332.82 3.73 334.58 5.91 334.58 L 594.09 334.58 C 596.27 334.58 598.03 332.82 598.03 330.64 L 598.03 314.41 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 321.04)"><foreignobject color="#000000" height="12.3" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Training and Evaluation Prompt Template for Qwen3-4B-Base</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 17.24)"><foreignobject color="#000000" height="286.85" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:20.48em;--fo_depth :0.25em;" transform="matrix(1 0 0 -1 0 283.39)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><math alttext="&lt;|" class="ltx_math_unparsed" display="inline" id="A9.p6.pic1.m1" intent=":literal"><semantics><mrow><mo rspace="0em">&lt;</mo><mo fence="false" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">&lt;|</annotation></semantics></math>im_start<math alttext="|&gt;" class="ltx_math_unparsed" display="inline" id="A9.p6.pic1.m2" intent=":literal"><semantics><mrow><mo fence="false" stretchy="false">|</mo><mo lspace="0.167em">&gt;</mo></mrow><annotation encoding="application/x-tex">|&gt;</annotation></semantics></math>user</span>
<span class="ltx_p">{question}</span>
<span class="ltx_p">Please reason step by step, and put your final answer within <code class="ltx_verbatim ltx_font_typewriter">\</code>boxed{}.<math alttext="&lt;|" class="ltx_math_unparsed" display="inline" id="A9.p6.pic1.m3" intent=":literal"><semantics><mrow><mo rspace="0em">&lt;</mo><mo fence="false" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">&lt;|</annotation></semantics></math>im_end<math alttext="|&gt;" class="ltx_math_unparsed" display="inline" id="A9.p6.pic1.m4" intent=":literal"><semantics><mrow><mo fence="false" stretchy="false">|</mo><mo lspace="0.167em">&gt;</mo></mrow><annotation encoding="application/x-tex">|&gt;</annotation></semantics></math></span>
<span class="ltx_p"><math alttext="&lt;|" class="ltx_math_unparsed" display="inline" id="A9.p6.pic1.m5" intent=":literal"><semantics><mrow><mo rspace="0em">&lt;</mo><mo fence="false" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">&lt;|</annotation></semantics></math>im_start<math alttext="|&gt;" class="ltx_math_unparsed" display="inline" id="A9.p6.pic1.m6" intent=":literal"><semantics><mrow><mo fence="false" stretchy="false">|</mo><mo lspace="0.167em">&gt;</mo></mrow><annotation encoding="application/x-tex">|&gt;</annotation></semantics></math>assistant</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A9.p7">
<span class="ltx_inline-block"><svg class="ltx_picture" height="732.36" id="A9.p7.pic1" overflow="visible" version="1.1" viewbox="0 0 600 732.36" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,732.36) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0" style="--ltx-fill-color:#000000;"><path d="M 0 5.91 L 0 726.46 C 0 729.72 2.64 732.36 5.91 732.36 L 594.09 732.36 C 597.36 732.36 600 729.72 600 726.46 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0" style="--ltx-fill-color:#FFFFFF;"><path d="M 1.97 5.91 L 1.97 690.11 L 598.03 690.11 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.0" style="--ltx-fill-color:#E6E6E6;"><path d="M 1.97 692.08 L 1.97 726.46 C 1.97 728.63 3.73 730.4 5.91 730.4 L 594.09 730.4 C 596.27 730.4 598.03 728.63 598.03 726.46 L 598.03 692.08 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 699.48)"><foreignobject color="#000000" height="30.44" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:1.95em;--fo_depth :0.25em;" transform="matrix(1 0 0 -1 0 26.98)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt Template for Self-Verification (Modified from <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14943v1#bib.bib27" title="">2025a</a>)</cite>)</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="664.52" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:48.02em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 664.52)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p">Below you are presented with a question and a tentative response. Your task is to evaluate the response and assign a rating to the response based on the following clear criteria:</span>
<span class="ltx_p">Rating Criteria:</span>
<span class="ltx_p">1. Missing final answer, or incorrect response with the wrong final answer: assign <code class="ltx_verbatim ltx_font_typewriter">\</code>boxed{0}.</span>
<span class="ltx_p">2. Correct response with the correct final answer: assign
<code class="ltx_verbatim ltx_font_typewriter">\</code>boxed{1}.</span>
<span class="ltx_p">### Question Begin ###</span>
<span class="ltx_p">{question}</span>
<span class="ltx_p">### Question End ###</span>
<span class="ltx_p">### Response Begin ###</span>
<span class="ltx_p">{response}</span>
<span class="ltx_p">### Response End ###</span>
<span class="ltx_p">First provide your evaluation process, then clearly state your final rating value enclosed in <code class="ltx_verbatim ltx_font_typewriter">\</code>boxed{} at the end.</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
</section>
</section>
</section>
</div>
</figure>
</section>
</div>
</div>
</figure>
</section>
</div>
</div>
</figure>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 16 17:56:58 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
