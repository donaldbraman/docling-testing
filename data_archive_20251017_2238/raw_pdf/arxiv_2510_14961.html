<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models</title>
<!--Generated on Thu Oct 16 17:53:59 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2510.14961v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S1" title="In Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S2" title="In Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S3" title="In Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Applying Diffusion Forcing to Recurrent-Depth Models</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S3.SS1" title="In 3 Applying Diffusion Forcing to Recurrent-Depth Models ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Background on Recurrent-Depth Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S3.SS2" title="In 3 Applying Diffusion Forcing to Recurrent-Depth Models ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>The Ingredients for Diffusion Forcing Sampling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S3.SS3" title="In 3 Applying Diffusion Forcing to Recurrent-Depth Models ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>A Simplified Version of the Sampling Algorithm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S3.SS4" title="In 3 Applying Diffusion Forcing to Recurrent-Depth Models ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Stabilizing components based on Diffusion Principles</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S3.SS5" title="In 3 Applying Diffusion Forcing to Recurrent-Depth Models ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Adaptive Exits</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S4" title="In Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Theoretical Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S4.SS1" title="In 4 Theoretical Analysis ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Problem Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S4.SS2" title="In 4 Theoretical Analysis ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>LLMs should prioritize depth scaling during prefilling.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S4.SS3" title="In 4 Theoretical Analysis ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>LLMs should prioritize width scaling during decoding.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5" title="In Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.SS1" title="In 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Benchmark Results.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.SS2" title="In 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Variants and Hyperparameters</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.SS2.SSS0.Px1" title="In 5.2 Variants and Hyperparameters ‣ 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title">Moving Forward Multiple Steps.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S6" title="In Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions: Are Recurrent-depth Transformers secretly continuous language diffusion models?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A1" title="In Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A1.SS1" title="In Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>AdditionaL Algorithm Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A1.SS2" title="In Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Additional Variants</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A1.SS2.SSS0.Px1" title="In A.2 Additional Variants ‣ Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title">Larger Batch Sizes.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A1.SS3" title="In Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Additional Information.</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A1.SS3.SSS0.Px1" title="In A.3 Additional Information. ‣ Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title">Finetuned Math Model:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A1.SS3.SSS0.Px2" title="In A.3 Additional Information. ‣ Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title">Dataset Details.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A1.SS4" title="In Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Qualitative Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A2" title="In Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Theoretical Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A2.SS1" title="In Appendix B Theoretical Analysis ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Problem Formulations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A2.SS2" title="In Appendix B Theoretical Analysis ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>LLMs should prioritize depth scaling during prefilling.</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jonas Geiping
<br class="ltx_break"/>ELLIS Institute Tübingen &amp;
<br class="ltx_break"/>Max-Planck Institute for Intelligent Systems,
<br class="ltx_break"/>Tübingen AI Center
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">jonas@tue.ellis.eu</span>
<br class="ltx_break"/>&amp;Xinyu Yang
<br class="ltx_break"/>Electrical and Computer Engineering
<br class="ltx_break"/>Carnegie Mellon University
<br class="ltx_break"/>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guinan Su
<br class="ltx_break"/>ELLIS Institute Tübingen &amp;
<br class="ltx_break"/>Max-Planck Institute for Intelligent Systems,
<br class="ltx_break"/>Tübingen AI Center
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers. Recent efforts in pretraining have demonstrated that these architectures can scale to modern language modeling tasks while exhibiting advantages in reasoning tasks.
In this work, we examine the relationship between recurrent-depth models and diffusion language models. Building on their similarities, we develop a new diffusion forcing sampler for these models to accelerate generation. The sampler advances by decoding new tokens at every forward pass of the model, while the latent states of these tokens can be further refined in parallel through recurrence. Theoretically, generation with our sampler is strictly more expressive than the baseline autoregressive generation using the same time budget on modern hardware.
Moreover, this sampler, based on principles from diffusion literature, can be directly applied to existing 3.5B recurrent-depth transformers without any tuning, leading to up to a <span class="ltx_text ltx_font_bold">5</span>x speedup. Consequently, our findings not only provide an efficient mechanism for parallelizing the extra computation in recurrent-depth models at inference, but also suggest that such models can be naturally viewed as strong continuous, though causal, diffusion language models.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p">Conventional large language models (LLMs) are constructed as fixed-depth neural networks with a predetermined number of layers (often merely a two-digit count), a property that not only allows these models to be trained efficiently, but in practice appears sufficient for many tasks <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib56" title="">2019</a>)</cite>. However, more challenging tasks in mathematics and programming often require conceptual leaps over multiple steps in a logical chain that are hard for these models to learn robustly. More formally, fixed-depth transformers fall within the complexity class <math alttext="\mathsf{TC}^{\mathsf{0}}" class="ltx_Math" display="inline" id="S1.p1.m1" intent=":literal"><semantics><msup><mi>𝖳𝖢</mi><mn>𝟢</mn></msup><annotation encoding="application/x-tex">\mathsf{TC}^{\mathsf{0}}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Merrill and Sabharwal, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib49" title="">2023</a>)</cite>. To resolve this, recent efforts have focused on training models to “verbalize” their internal reasoning into chains-of-thought composed of small sub-steps, each of which the model is capable of learning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p">An alternative to fixed-depth are models with <span class="ltx_text ltx_font_italic">recurrent depth</span> <cite class="ltx_cite ltx_citemacro_citep">(Dehghani et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib20" title="">2019</a>; Schwarzschild et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib60" title="">2021</a>)</cite>, which can repeat layers. Consequently, these models are also referred to as <span class="ltx_text ltx_font_italic">looped</span> transformers <cite class="ltx_cite ltx_citemacro_citep">(Giannou et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib27" title="">2023</a>)</cite>, or, as <span class="ltx_text ltx_font_italic">universal</span> transformers, <cite class="ltx_cite ltx_citemacro_citep">(Dehghani et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib20" title="">2019</a>)</cite> when highlighting the motivation for these systems to represent universal Turing machines <cite class="ltx_cite ltx_citemacro_citep">(Graves et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib32" title="">2014</a>; Graves, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib30" title="">2017</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Merrill and Sabharwal (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib50" title="">2025</a>)</cite> showcase that, in contrast to fixed-depth models, models with arbitrary recurrence are indeed capable of representing a larger complexity class.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">However, generation with autoregressive recurrent-depth models is typically slow, given that every repetition of the model layers must be executed sequentially before the next token can be produced.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="292" id="S1.F1.g1" src="x1.png" width="624"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Different generation schemes for autoregressive, recurrent-depth models. <span class="ltx_text ltx_font_bold">Left:</span> Standard sequential generation, which proceeds one token and step of the recurrence at a time (time steps denoted by integers). <span class="ltx_text ltx_font_bold">Right:</span> A diffusion forcing sampler used for the same model can parallelize generation “diagonally”, by computing one step of the recurrence per token position, iteratively refining its estimate of the generated sequence. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p">In this work, we discuss how generation from recurrent-depth models can be efficiently parallelized by connecting this architecture to diffusion model architectures. Both architectures “recur” in a related sense, and even though both are trained with different objectives, we show that samplers adapted from diffusion literature, namely, <span class="ltx_text ltx_font_italic">diffusion forcing</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib12" title="">2024a</a>)</cite>, can be directly applied to parallelize the generation of already existing recurrent-depth models from <cite class="ltx_cite ltx_citemacro_cite">Geiping et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib25" title="">2025</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p">We discuss how to adapt diffusion forcing sampling to recurrent-depth models, identifying the essential architectural components and strategies required to ensure both stability of the iterates and bounded memory usage. As illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S1.F1" title="In 1 Introduction ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">1</span></a>, rather than waiting for the recurrence at sequence position <math alttext="n" class="ltx_Math" display="inline" id="S1.p5.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> to fully converge before generating the next token, our sampler immediately produces token drafts from intermediate iterates. It then advances to position <math alttext="n+1" class="ltx_Math" display="inline" id="S1.p5.m2" intent=":literal"><semantics><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n+1</annotation></semantics></math>, where the subsequent forward pass simultaneously refines the drafts for steps <math alttext="n" class="ltx_Math" display="inline" id="S1.p5.m3" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> and <math alttext="n+1" class="ltx_Math" display="inline" id="S1.p5.m4" intent=":literal"><semantics><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n+1</annotation></semantics></math>, while also decoding an initial draft for <math alttext="n+2" class="ltx_Math" display="inline" id="S1.p5.m5" intent=":literal"><semantics><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">n+2</annotation></semantics></math>. In this way, the sampler achieves parallelism along the sequence dimension, akin to speculative decoding. Importantly, because the underlying model is trained as a causal language model, information still propagates strictly from left to right, and the output sequence is iteratively refined across recurrences. While this approach does not reduce FLOPs, it effectively exploits modern GPU architectures by unlocking additional opportunities for parallelization. Overall, in this work, we</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p">Clarify the connection between recurrent-depth models and diffusion models via diffusion forcing and block or wave-based inference strategies for sequence-based diffusion models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p">Describe how to apply principles from diffusion forcing to efficiently parallelize the inference of models with recurrent depth.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p">Verify that recurrent-depth models equipped with diffusion-forcing samplers achieve the strongest balance between practical efficiency and theoretical expressiveness in both prefilling and decoding.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i4.p1">
<p class="ltx_p">Show that diffusion forcing sampling outperforms even well-tuned speculative decoding baselines for the same model with speed gains that can be smoothly traded off against accuracy.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p">We briefly introduce both recurrent models and diffusion models, focusing on language applications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Recurrent Models.</span>
Models with recurrent computations have long been central to machine learning <cite class="ltx_cite ltx_citemacro_citep">(Amari, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib2" title="">1972</a>; Hopfield, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib37" title="">1982</a>; Braitenberg, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib10" title="">1986</a>; Gers and Schmidhuber, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib26" title="">2000</a>; Sutskever et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib64" title="">2008</a>)</cite>, not only due to significant inspiration from recurrent firing patterns found in neuroscience <cite class="ltx_cite ltx_citemacro_citep">(Hopfield, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib37" title="">1982</a>; Lamme and Roelfsema, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib41" title="">2000</a>; Douglas and Martin, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib22" title="">2004</a>)</cite>, and early successes in language modeling centered on recurrent neural networks <cite class="ltx_cite ltx_citemacro_citep">(Mikolov et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib52" title="">2010</a>; Sutskever et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib65" title="">2011</a>)</cite>. With the advent of transformer models, these architectures were considered less scalable, yet recurrence, now as <span class="ltx_text ltx_font_italic">recurrence in depth</span>, was swiftly re-introduced as <span class="ltx_text ltx_font_italic">universal transformers</span>, <cite class="ltx_cite ltx_citemacro_citet">Dehghani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib20" title="">2019</a>)</cite>, motivating that these models could be capable of modeling universal Turing machines <cite class="ltx_cite ltx_citemacro_citep">(Graves et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib32" title="">2014</a>)</cite>. Other work showed that recurrent models were capable of learning algorithms <cite class="ltx_cite ltx_citemacro_citep">(Schwarzschild et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib60" title="">2021</a>; Bansal et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib8" title="">2022</a>; Bear et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib9" title="">2024</a>)</cite>.
That recurrence was capable of representing universal computation was explicitly constructed for transformer models in <cite class="ltx_cite ltx_citemacro_citet">Giannou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib27" title="">2023</a>)</cite>, and following work on <span class="ltx_text ltx_font_italic">looped</span> transformers has shown that these models are capable learners <cite class="ltx_cite ltx_citemacro_citep">(Giannou et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib27" title="">2023</a>; Gatmiry et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib24" title="">2024</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib72" title="">2024</a>; McLeish et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib48" title="">2024</a>; Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib23" title="">2025</a>)</cite>. These findings have led to a wave of work training larger, general-purpose recurrent-depth models of language <cite class="ltx_cite ltx_citemacro_citep">(Tan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib66" title="">2023</a>; Abnar et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib1" title="">2023</a>; Mathur et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib47" title="">2024</a>; Csordás et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib17" title="">2024</a>; Geiping et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib25" title="">2025</a>)</cite>, as well as work retro-fitting recurrence into trained models <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib43" title="">2020</a>; Bae et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib6" title="">2024</a>; Hay and Wolf, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib35" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib45" title="">2024b</a>)</cite>. Several of these works also highlight the possibility of implementing <span class="ltx_text ltx_font_italic">latent reasoning</span> via recurrence, that is to complement or replace verbalized chains-of-thought, with recurrence. Examples for this line of thinking are <span class="ltx_text ltx_font_italic">Coconut</span> <cite class="ltx_cite ltx_citemacro_citep">(Hao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib34" title="">2024</a>)</cite>, as well as <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib44" title="">2024a</a>); Cheng and Durme (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib16" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="330" id="S2.F2.g1" src="x2.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example of a text sequence being generated with the proposed diffusion forcing sampler from a depth-recurrent model. While the original recurrent-depth model requires 32 recurrence steps to produce a single token (the default for this model), the diffusion sampler has already produced and committed 8 new tokens (green). As described, the sampler advances by at least one token per step of the recurrence. Decoded candidate tokens are initial spell out incoherent text, but map into the right concepts, and quickly improve with more steps. Note that the “freeze” decision is dynamic, based on distance to the previous state in latent space (not pictured).
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p">In this work, we propose a generic sampling algorithm for depth-recurrent models, which we test with the models developed in <cite class="ltx_cite ltx_citemacro_citet">Geiping et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib25" title="">2025</a>)</cite>, which are trained for general language understanding and reasoning on 800B tokens, with 3.5B parameters, and openly accessible.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Diffusion Language Models.</span>
Diffusion models are general-purpose generative models, with early applications focusing on continuous domains, such as images <cite class="ltx_cite ltx_citemacro_citep">(Song and Ermon, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib62" title="">2019</a>; Rombach et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib58" title="">2022</a>; Peebles and Xie, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib55" title="">2023</a>)</cite>, which lead to substantial interest in extending diffusion also to discrete domains, such as text <cite class="ltx_cite ltx_citemacro_citep">(Austin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib5" title="">2021</a>; Hoogeboom et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib36" title="">2021</a>)</cite>. Approaches to language diffusion are split on whether to incorporate diffusion processes on a continuous variable (that is then projected into discrete space) <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib14" title="">2022</a>; Dieleman et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib21" title="">2022</a>; Han et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib33" title="">2023</a>; Karimi Mahabadi et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib40" title="">2024</a>; Jo and Hwang, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib38" title="">2025</a>; Graves et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib31" title="">2025</a>)</cite>, or diffusion processes that directly act on discrete variables.<cite class="ltx_cite ltx_citemacro_citep">(Lou et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib46" title="">2024</a>; Richemond et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib57" title="">2023</a>)</cite>. The latter though, especially using <span class="ltx_text ltx_font_italic">masking</span> as the discrete forward diffusion step, is currently the most scalable approach, employed in large-scale efforts to train language diffusion models, competitive with autoregressive models <cite class="ltx_cite ltx_citemacro_citep">(Gong et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib28" title="">2025a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib29" title="">b</a>; DeepMind, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib18" title="">2025</a>; Nie et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib54" title="">2025</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib69" title="">2025b</a>; Xie et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib71" title="">2025</a>; Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib73" title="">2025</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Inference Strategies for Diffusion Language Models.</span>
To make diffusion tractable for arbitrarily long sequences requires techniques such as block diffusion <cite class="ltx_cite ltx_citemacro_citep">(Arriola et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib4" title="">2025</a>)</cite>, where chunks of text are being modified by the diffusion model, and then frozen and their KV entries cached, with the sampler moving to the next chunk. A more free-form approach to handle sequence-based diffusion is to use <span class="ltx_text ltx_font_italic">diffusion forcing</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib12" title="">2024a</a>)</cite>, a hybrid model, where noise is added to future tokens in a sequence relative to the position of the current token, allowing the sampler to move both on both the sequence dimension and the diffusion time dimension.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p6">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Inference Acceleration for Fixed-Depth Transformers.</span>
Inference in transformers, in particular in small-batch settings is memory-bound, meaning that the transfer of data (or, in the default case, model parameters) to and from the L1 cache of the accelerator, is the dominating cost during inference, allowing algorithms such as speculative decoding <cite class="ltx_cite ltx_citemacro_citep">(Leviathan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib42" title="">2023</a>)</cite> and follow-ups <cite class="ltx_cite ltx_citemacro_citep">(Cai et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib11" title="">2024</a>; Miao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib51" title="">2024</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib15" title="">2024c</a>)</cite> to improve inference speed through speculative parallelization. Using smaller draft models, these algorithms draft text several tokens in the future, which can then be verified using the original model, as verification of the entire text sequence is compute-bound and hence, fast.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Applying Diffusion Forcing to Recurrent-Depth Models</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p">In this section, we present our diffusion forcing sampler for recurrent-depth models, which accelerates text generation by advancing at least one token in each recurrence step, as illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S2.F2" title="In 2 Related Work ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Background on Recurrent-Depth Models</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p">Before detailing the diffusion forcing sampler, we briefly describe the particular recurrent-depth architecture proposed by <cite class="ltx_cite ltx_citemacro_citet">Geiping et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib25" title="">2025</a>)</cite>, emphasizing features of the model that are pertinent to the sampler’s functionality. We will use the checkpoint name <span class="ltx_text ltx_font_italic">Huginn-0125</span> when referring to the trained model. The architecture of this model contains three main blocks, each composed of multiple transformer layers: (i) a prelude block <math alttext="P" class="ltx_Math" display="inline" id="S3.SS1.p1.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>, projecting the embedded input tokens into a latent space; (ii) a recurrent block <math alttext="R" class="ltx_Math" display="inline" id="S3.SS1.p1.m2" intent=":literal"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>, iterating <math alttext="r" class="ltx_Math" display="inline" id="S3.SS1.p1.m3" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> times in this latent space by refining a state vector <span class="ltx_text ltx_markedasmath ltx_font_bold">s</span>, and (iii) a coda block <math alttext="C" class="ltx_Math" display="inline" id="S3.SS1.p1.m5" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> that processes the latent state and produces the model’s probabilities for the next token, formally</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.EGx1">
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbf{e}" class="ltx_Math" display="inline" id="S3.Ex1.m1" intent=":literal"><semantics><mi>𝐞</mi><annotation encoding="application/x-tex">\displaystyle\mathbf{e}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=P(\mathbf{x})" class="ltx_Math" display="inline" id="S3.Ex1.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐱</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=P(\mathbf{x})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbf{s}_{0}" class="ltx_Math" display="inline" id="S3.Ex2.m1" intent=":literal"><semantics><msub><mi>𝐬</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\displaystyle\mathbf{s}_{0}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\sim\mathcal{N}(\mathbf{0},\sigma^{2}I)" class="ltx_Math" display="inline" id="S3.Ex2.m2" intent=":literal"><semantics><mrow><mi></mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>I</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\sim\mathcal{N}(\mathbf{0},\sigma^{2}I)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbf{s}_{i}" class="ltx_Math" display="inline" id="S3.Ex3.m1" intent=":literal"><semantics><msub><mi>𝐬</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\displaystyle\mathbf{s}_{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=R(\mathbf{e},\mathbf{s}_{i-1})\qquad\textnormal{for}\quad i\in\{1,\dots,r\}" class="ltx_Math" display="inline" id="S3.Ex3.m2" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐞</mi><mo>,</mo><msub><mi>𝐬</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mspace style="width:2em;" width="2em"></mspace><mtext>for</mtext></mrow></mrow><mspace style="width:1em;" width="1em"></mspace><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>r</mi><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=R(\mathbf{e},\mathbf{s}_{i-1})\qquad\textnormal{for}\quad i\in\{1,\dots,r\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbf{p}" class="ltx_Math" display="inline" id="S3.Ex4.m1" intent=":literal"><semantics><mi>𝐩</mi><annotation encoding="application/x-tex">\displaystyle\mathbf{p}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=C(\mathbf{s}_{r})." class="ltx_Math" display="inline" id="S3.Ex4.m2" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐬</mi><mi>r</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=C(\mathbf{s}_{r}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Notably, while this architecture is derived from looping the middle layers of fixed-depth transformer models <cite class="ltx_cite ltx_citemacro_citep">(Skean et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib61" title="">2024</a>; Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib63" title="">2024</a>; Kaplan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib39" title="">2024</a>)</cite>, with features such as input injection and random state initialization from the literature of recurrent-depth models <cite class="ltx_cite ltx_citemacro_citep">(Bansal et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib8" title="">2022</a>; Anil et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib3" title="">2022</a>)</cite>, it can also be interpreted as a <span class="ltx_text ltx_font_italic">latent-space diffusion model</span> following the formulation of <cite class="ltx_cite ltx_citemacro_citet">Rombach et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib58" title="">2022</a>)</cite>: Starting from an initial random state <math alttext="s_{0}" class="ltx_Math" display="inline" id="S3.SS1.p1.m6" intent=":literal"><semantics><msub><mi>s</mi><mn>0</mn></msub><annotation encoding="application/x-tex">s_{0}</annotation></semantics></math>, the model iteratively refines this state conditioned on the embedded input sequence <math alttext="e" class="ltx_Math" display="inline" id="S3.SS1.p1.m7" intent=":literal"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math>, until we assume the state to be completely denoised at the end of the process, at which point it will be decoded into the next token using <math alttext="C" class="ltx_Math" display="inline" id="S3.SS1.p1.m8" intent=":literal"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p">In <cite class="ltx_cite ltx_citemacro_citet">Geiping et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib25" title="">2025</a>)</cite>, this model is trained using randomized unrolling with truncated backpropagation, i.e. a random number of iterates <math alttext="r" class="ltx_Math" display="inline" id="S3.SS1.p2.m1" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> is sampled (from a Poisson-lognormal distribution), and then the entire current batch of training sequences is iterated up to <math alttext="r" class="ltx_Math" display="inline" id="S3.SS1.p2.m2" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>, which is not directly related to diffusion language modeling, which most effectively trains by randomized masking and adaptation from autoregressive models <cite class="ltx_cite ltx_citemacro_citep">(Nie et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib54" title="">2025</a>; Xie et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib71" title="">2025</a>; Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib73" title="">2025</a>; Gong et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib28" title="">2025a</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>The Ingredients for Diffusion Forcing Sampling</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p">While we will describe experiments using this particular recurrent-depth model, the sampler can be applied to all recurrent-depth models that fulfill the following requirements.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Input Injection.</span>
The first necessary component, aside from the recurrence over layers itself, is the input injection, i.e., the conditioning of the recurrence on <math alttext="e" class="ltx_Math" display="inline" id="S3.SS2.p2.m1" intent=":literal"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math>. This will allow the sampler to “course-correct” if conditioning changes without having to jettison a partially computed state <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.p2.m2" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>. The other component that may improve the connection to diffusion modeling is the initialization of random states, but while we speculate that this is beneficial, it is not architecturally necessary. As such, recurrent-depth models trained in <cite class="ltx_cite ltx_citemacro_citet">Csordás et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib17" title="">2024</a>); Schöne et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib59" title="">2025</a>); Mohtashami et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib53" title="">2024</a>)</cite> or <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib68" title="">2025a</a>)</cite> could also benefit from this sampler. However, looped architectures such as <span class="ltx_text ltx_font_italic">Coconut</span> <cite class="ltx_cite ltx_citemacro_citep">(Hao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib34" title="">2024</a>)</cite>, which train to feed the outputs of a transformer back in as inputs, are not immediately supported and require retraining to incorporate input injection, separating their recurrent state from their input data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Robust Recurrence.</span> The second necessary property is that the intermediate state at every step of the recurrence must be decodable to approximately correct solutions. While this property is generally satisfied, it may fail in models trained exclusively with a fixed number of recurrences <math alttext="r" class="ltx_Math" display="inline" id="S3.SS2.p3.m1" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>, where decoding from earlier steps can yield nonsensical outputs rather than approximate versions of the intended result.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="365" id="S3.F3.g1" src="x3.png" width="365"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The <span class="ltx_text ltx_font_italic">Huginn-0125</span> recurrent-depth model can match the baseline performance on the GSM8k dataset when enabling KV cache sharing (with a minimal cache size of 1), using <math alttext="r" class="ltx_Math" display="inline" id="S3.F3.m2" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>-times less memory for KV states.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">KV Cache Sharing.</span> The third property, while not strictly required but highly beneficial for diffusion forcing samplers, is the ability of different recurrent depths to share their KV cache across iterations during generation. Without fungible KV states, all KV states from previous recurrences and tokens must be retained in memory, causing the cache to grow with both sequence length and recurrence depth. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S3.F3" title="In 3.2 The Ingredients for Diffusion Forcing Sampling ‣ 3 Applying Diffusion Forcing to Recurrent-Depth Models ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">3</span></a>, the trained <span class="ltx_text ltx_font_italic">Huginn-0125</span> model inherently supports KV cache sharing, allowing us to store only the KV state of the most recent recurrence for each token position<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>With this form of KV sharing, the cache requires no more memory than that of a parameter-matched fixed-depth transformer.</span></span></span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>A Simplified Version of the Sampling Algorithm</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p">Next, we present the algorithm for our sampler.
Given a prompt <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p1.m1" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#alg1" title="In 3.4 Stabilizing components based on Diffusion Principles ‣ 3 Applying Diffusion Forcing to Recurrent-Depth Models ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Algorithm</span>˜<span class="ltx_text ltx_ref_tag">1</span></a> describes a simplified version that directly adapts diffusion forcing principles to parallelize generation across the sequence dimension. This approach yields improvements in tokens/second while maintaining equivalent total FLOP requirements. An example of the sampler’s behavior is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p">We emphasize several important aspects. First, the number of inner recurrences <math alttext="r^{\prime}" class="ltx_Math" display="inline" id="S3.SS3.p2.m1" intent=":literal"><semantics><msup><mi>r</mi><mo>′</mo></msup><annotation encoding="application/x-tex">r^{\prime}</annotation></semantics></math> may be chosen to exceed one. These additional iterations are relatively inexpensive, since the broader logic of the sampler is not yet invoked. More importantly, they serve to stabilize the recurrence. Because the conditioning on the input embedding <math alttext="\mathbf{e}" class="ltx_Math" display="inline" id="S3.SS3.p2.m2" intent=":literal"><semantics><mi>𝐞</mi><annotation encoding="application/x-tex">\mathbf{e}</annotation></semantics></math> may vary across successive steps of the sampler, the model risks becoming trapped in oscillatory behavior unless sufficient steps are allowed to adapt the current state to the evolving conditioning. This mechanism closely parallels practices in the diffusion literature, such as the use of supplementary diffusion steps in <cite class="ltx_cite ltx_citemacro_citet">Bansal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib7" title="">2023</a>)</cite> to incorporate complex guidance signals into image diffusion models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p">Second, we naturally employ this sampler only during the generation phase, as the prefill phase is already parallelizable in the sequence dimension, as the recurrence can be computed on all token positions of the prompt simultaneously.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p">Further, in terms of efficiency, we note that we do not actually want to keep the state for all tokens changing indefinitely, as doing so would slow down generation again, as well as increase memory usage dramatically. As such, similar to block-diffusion samplers <cite class="ltx_cite ltx_citemacro_citep">(Arriola et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib4" title="">2025</a>)</cite>, we look for rules that decide when each position is “finished”. In the simplified version of the sampler, we freeze the last token once we reach a predetermined number of recurrence steps at this position – which naturally happens <math alttext="r" class="ltx_Math" display="inline" id="S3.SS3.p4.m1" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> positions behind the current maximal extent of the sequence. Frozen tokens are removed from the state vector and their KV states are added to the cache, so that, as in block diffusion models <cite class="ltx_cite ltx_citemacro_citep">(Arriola et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib4" title="">2025</a>)</cite>, at each point in time, only a small subset of tokens in being modified and the full generation runs like a wave over the generating sequence. Finally, note that with this simplified exit rule, <math alttext="r^{\prime}=r" class="ltx_Math" display="inline" id="S3.SS3.p4.m2" intent=":literal"><semantics><mrow><msup><mi>r</mi><mo>′</mo></msup><mo>=</mo><mi>r</mi></mrow><annotation encoding="application/x-tex">r^{\prime}=r</annotation></semantics></math> exactly recovers the original autoregressive sampler.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Stabilizing components based on Diffusion Principles</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p">Further, we also experiment with adding momentum to the input conditioning <math alttext="\mathbf{e}" class="ltx_Math" display="inline" id="S3.SS4.p1.m1" intent=":literal"><semantics><mi>𝐞</mi><annotation encoding="application/x-tex">\mathbf{e}</annotation></semantics></math>, setting</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{e}=\eta~\mathbf{e}_{\text{prev}}+(1-\eta)\mathcal{P}(y_{\text{current}})," class="ltx_Math" display="block" id="S3.E1.m1" intent=":literal"><semantics><mrow><mrow><mi>𝐞</mi><mo>=</mo><mrow><mrow><mi>η</mi><mo lspace="0.330em" rspace="0em">​</mo><msub><mi>𝐞</mi><mtext>prev</mtext></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>η</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mtext>current</mtext></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{e}=\eta~\mathbf{e}_{\text{prev}}+(1-\eta)\mathcal{P}(y_{\text{current}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which we find can stabilize the recurrence in challenging sequences, providing a small, but robust gain on average.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p">Secondly, surprisingly, we find that even though these models are never trained with noise injected into intermediate states, that artificially adding noise to the state in each step of the sampler, in analogy to sampling from continuous diffusion models, i.e.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{z^{\prime}}=(1-\beta_{t})\mathbf{z}+\beta_{t}\,\mathbf{z}_{\text{noise}}\qquad\qquad\textnormal{where}\quad\mathbf{z}_{\text{noise}}=\operatorname{InitState}(1,\alpha)," class="ltx_Math" display="block" id="S3.E2.m1" intent=":literal"><semantics><mrow><mrow><mrow><msup><mi>𝐳</mi><mo>′</mo></msup><mo>=</mo><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>β</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝐳</mi></mrow><mo>+</mo><mrow><msub><mi>β</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝐳</mi><mtext>noise</mtext></msub></mrow></mrow><mspace style="width:4em;" width="4em"></mspace><mtext>where</mtext></mrow></mrow><mspace style="width:1em;" width="1em"></mspace><mrow><msub><mi>𝐳</mi><mtext>noise</mtext></msub><mo>=</mo><mrow><mi>InitState</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{z^{\prime}}=(1-\beta_{t})\mathbf{z}+\beta_{t}\,\mathbf{z}_{\text{noise}}\qquad\qquad\textnormal{where}\quad\mathbf{z}_{\text{noise}}=\operatorname{InitState}(1,\alpha),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">can stabilize the iterative process, leading to gains in both accuracy and throughput if <math alttext="r^{\prime}" class="ltx_Math" display="inline" id="S3.SS4.p2.m1" intent=":literal"><semantics><msup><mi>r</mi><mo>′</mo></msup><annotation encoding="application/x-tex">r^{\prime}</annotation></semantics></math> is small. In practice, we schedule <math alttext="\beta_{t}" class="ltx_Math" display="inline" id="S3.SS4.p2.m2" intent=":literal"><semantics><msub><mi>β</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\beta_{t}</annotation></semantics></math> linearly as a function of steps <math alttext="t" class="ltx_Math" display="inline" id="S3.SS4.p2.m3" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> at each position, so that the latter steps are naturally less noisy <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib13" title="">2024b</a>)</cite>, which we find to outperform either scheduling <math alttext="\beta_{t}" class="ltx_Math" display="inline" id="S3.SS4.p2.m4" intent=":literal"><semantics><msub><mi>β</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\beta_{t}</annotation></semantics></math> scaled by the square root of the number of recurrences at each position or keeping it constant. However, the optimal value of <math alttext="\beta_{t}" class="ltx_Math" display="inline" id="S3.SS4.p2.m5" intent=":literal"><semantics><msub><mi>β</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\beta_{t}</annotation></semantics></math> depends on <math alttext="r^{\prime}" class="ltx_Math" display="inline" id="S3.SS4.p2.m6" intent=":literal"><semantics><msup><mi>r</mi><mo>′</mo></msup><annotation encoding="application/x-tex">r^{\prime}</annotation></semantics></math>.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Diffusion-forcing-style generation, simplified version (Full Version in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#alg2" title="In A.1 AdditionaL Algorithm Details ‣ Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Algorithm</span>˜<span class="ltx_text ltx_ref_tag">2</span></a>)</figcaption>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span>current text context <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="alg1.l1.m1" intent=":literal"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>, max new tokens <math alttext="N" class="ltx_Math" display="inline" id="alg1.l1.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>, inner recurrence <math alttext="r^{\prime}" class="ltx_Math" display="inline" id="alg1.l1.m3" intent=":literal"><semantics><msup><mi>r</mi><mo>′</mo></msup><annotation encoding="application/x-tex">r^{\prime}</annotation></semantics></math>, total recurrences per token <math alttext="r" class="ltx_Math" display="inline" id="alg1.l1.m4" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>, diffusion steps <math alttext="T" class="ltx_Math" display="inline" id="alg1.l1.m5" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>, init scale <math alttext="\alpha" class="ltx_Math" display="inline" id="alg1.l1.m6" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span><math alttext="\mathbf{y}_{\mathrm{frozen}}\leftarrow\mathbf{x}" class="ltx_Math" display="inline" id="alg1.l2.m1" intent=":literal"><semantics><mrow><msub><mi>𝐲</mi><mi>frozen</mi></msub><mo stretchy="false">←</mo><mi>𝐱</mi></mrow><annotation encoding="application/x-tex">\mathbf{y}_{\mathrm{frozen}}\leftarrow\mathbf{x}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span><math alttext="\mathbf{y}_{\mathrm{current}}\leftarrow\mathbf{x}" class="ltx_Math" display="inline" id="alg1.l3.m1" intent=":literal"><semantics><mrow><msub><mi>𝐲</mi><mi>current</mi></msub><mo stretchy="false">←</mo><mi>𝐱</mi></mrow><annotation encoding="application/x-tex">\mathbf{y}_{\mathrm{current}}\leftarrow\mathbf{x}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span><math alttext="\mathbf{z}\leftarrow\operatorname{InitState}(1,\alpha)" class="ltx_Math" display="inline" id="alg1.l4.m1" intent=":literal"><semantics><mrow><mi>𝐳</mi><mo stretchy="false">←</mo><mrow><mi>InitState</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}\leftarrow\operatorname{InitState}(1,\alpha)</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span><span class="ltx_text ltx_font_bold">for</span> step <math alttext="t=1,\dots,T" class="ltx_Math" display="inline" id="alg1.l5.m1" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>T</mi></mrow></mrow><annotation encoding="application/x-tex">t=1,\dots,T</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>  <math alttext="\mathbf{e}\leftarrow\mathcal{P}(\mathbf{y}_{\mathrm{current}})" class="ltx_Math" display="inline" id="alg1.l6.m1" intent=":literal"><semantics><mrow><mi>𝐞</mi><mo stretchy="false">←</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐲</mi><mi>current</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{e}\leftarrow\mathcal{P}(\mathbf{y}_{\mathrm{current}})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>  <math alttext="\mathbf{z}_{\text{noise}}\leftarrow\operatorname{InitState}(1,\alpha)" class="ltx_Math" display="inline" id="alg1.l7.m1" intent=":literal"><semantics><mrow><msub><mi>𝐳</mi><mtext>noise</mtext></msub><mo stretchy="false">←</mo><mrow><mi>InitState</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}_{\text{noise}}\leftarrow\operatorname{InitState}(1,\alpha)</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span>  <math alttext="\mathbf{z}\leftarrow(1-\beta_{t})\mathbf{z}+\beta_{t}\,\mathbf{z}_{\text{noise}}" class="ltx_Math" display="inline" id="alg1.l8.m1" intent=":literal"><semantics><mrow><mi>𝐳</mi><mo stretchy="false">←</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>β</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝐳</mi></mrow><mo>+</mo><mrow><msub><mi>β</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝐳</mi><mtext>noise</mtext></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}\leftarrow(1-\beta_{t})\mathbf{z}+\beta_{t}\,\mathbf{z}_{\text{noise}}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span>  <span class="ltx_text ltx_font_bold">for</span> <math alttext="j=1,\dots,r^{\prime}" class="ltx_Math" display="inline" id="alg1.l9.m1" intent=":literal"><semantics><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>r</mi><mo>′</mo></msup></mrow></mrow><annotation encoding="application/x-tex">j=1,\dots,r^{\prime}</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span>   <math alttext="\mathbf{z}\leftarrow\mathcal{R}(\mathbf{z},\mathbf{e})" class="ltx_Math" display="inline" id="alg1.l10.m1" intent=":literal"><semantics><mrow><mi>𝐳</mi><mo stretchy="false">←</mo><mrow><mi class="ltx_font_mathcaligraphic">ℛ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐳</mi><mo>,</mo><mi>𝐞</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}\leftarrow\mathcal{R}(\mathbf{z},\mathbf{e})</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l10.m2" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> Inner recurrence
</span>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline">11:</span>  <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline">12:</span>  <math alttext="\mathbf{p}\leftarrow\mathcal{C}(\mathbf{z})" class="ltx_Math" display="inline" id="alg1.l12.m1" intent=":literal"><semantics><mrow><mi>𝐩</mi><mo stretchy="false">←</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐳</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{p}\leftarrow\mathcal{C}(\mathbf{z})</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l12.m2" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> project latent states to logits
</span>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline">13:</span>  <math alttext="\hat{\mathbf{y}}\leftarrow\operatorname{Sample}(\mathbf{p})" class="ltx_Math" display="inline" id="alg1.l13.m1" intent=":literal"><semantics><mrow><mover accent="true"><mi>𝐲</mi><mo>^</mo></mover><mo stretchy="false">←</mo><mrow><mi>Sample</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝐩</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\mathbf{y}}\leftarrow\operatorname{Sample}(\mathbf{p})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline">14:</span>  <math alttext="\mathbf{y}_{\mathrm{current}}\leftarrow[\mathbf{y}_{\mathrm{frozen}},\hat{\mathbf{y}}]" class="ltx_Math" display="inline" id="alg1.l14.m1" intent=":literal"><semantics><mrow><msub><mi>𝐲</mi><mi>current</mi></msub><mo stretchy="false">←</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝐲</mi><mi>frozen</mi></msub><mo>,</mo><mover accent="true"><mi>𝐲</mi><mo>^</mo></mover><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{y}_{\mathrm{current}}\leftarrow[\mathbf{y}_{\mathrm{frozen}},\hat{\mathbf{y}}]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline">15:</span>  <math alttext="\mathbf{y}_{\mathrm{frozen}}\leftarrow" class="ltx_Math" display="inline" id="alg1.l15.m1" intent=":literal"><semantics><mrow><msub><mi>𝐲</mi><mi>frozen</mi></msub><mo stretchy="false">←</mo><mi></mi></mrow><annotation encoding="application/x-tex">\mathbf{y}_{\mathrm{frozen}}\leftarrow</annotation></semantics></math> Assign <math alttext="\mathbf{y}_{\mathrm{current}}" class="ltx_Math" display="inline" id="alg1.l15.m2" intent=":literal"><semantics><msub><mi>𝐲</mi><mi>current</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_{\mathrm{current}}</annotation></semantics></math> up to the last <math alttext="\lceil{\frac{r}{r^{\prime}}}\rceil" class="ltx_Math" display="inline" id="alg1.l15.m3" intent=":literal"><semantics><mrow><mo stretchy="false">⌈</mo><mfrac><mi>r</mi><msup><mi>r</mi><mo>′</mo></msup></mfrac><mo stretchy="false">⌉</mo></mrow><annotation encoding="application/x-tex">\lceil{\frac{r}{r^{\prime}}}\rceil</annotation></semantics></math> entries <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l15.m4" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> Freeze completed tokens
</span>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline">16:</span>  <span class="ltx_text ltx_font_bold">if</span> <math alttext="|\mathbf{y}_{\mathrm{frozen}}|-|\mathbf{x}|\geq N" class="ltx_Math" display="inline" id="alg1.l16.m1" intent=":literal"><semantics><mrow><mrow><mrow><mo stretchy="false">|</mo><msub><mi>𝐲</mi><mi>frozen</mi></msub><mo stretchy="false">|</mo></mrow><mo>−</mo><mrow><mo stretchy="false">|</mo><mi>𝐱</mi><mo stretchy="false">|</mo></mrow></mrow><mo>≥</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">|\mathbf{y}_{\mathrm{frozen}}|-|\mathbf{x}|\geq N</annotation></semantics></math> <span class="ltx_text ltx_font_bold">then</span> <span class="ltx_text ltx_font_bold">break</span>
</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline">17:</span>  <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">if</span>
</div>
<div class="ltx_listingline" id="alg1.l18">
<span class="ltx_tag ltx_tag_listingline">18:</span>  <math alttext="\mathbf{z}\leftarrow[\mathbf{z},\,\operatorname{InitState}(1,\alpha)]" class="ltx_Math" display="inline" id="alg1.l18.m1" intent=":literal"><semantics><mrow><mi>𝐳</mi><mo stretchy="false">←</mo><mrow><mo stretchy="false">[</mo><mi>𝐳</mi><mo rspace="0.337em">,</mo><mrow><mi>InitState</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}\leftarrow[\mathbf{z},\,\operatorname{InitState}(1,\alpha)]</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l18.m2" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> Append a new latent state for the next position
</span>
</div>
<div class="ltx_listingline" id="alg1.l19">
<span class="ltx_tag ltx_tag_listingline">19:</span><span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l20">
<span class="ltx_tag ltx_tag_listingline">20:</span><span class="ltx_text ltx_font_bold">return</span> <math alttext="\mathbf{y}_{\mathrm{frozen}}" class="ltx_Math" display="inline" id="alg1.l20.m1" intent=":literal"><semantics><msub><mi>𝐲</mi><mi>frozen</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_{\mathrm{frozen}}</annotation></semantics></math>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Adaptive Exits</h3>
<div class="ltx_para ltx_noindent" id="S3.SS5.p1">
<p class="ltx_p">However, the fixed exit scheme of the simplified sampler can run into issues. The recurrent-depth model is causal and how quickly states converge depends on the complexity of the query. This can lead to situations where either, compute is wasted because the states at certain positions have already converged quicker than <math alttext="r" class="ltx_Math" display="inline" id="S3.SS5.p1.m1" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>, or, more problematically, states where, due to a late change in the conditioning of prior tokens, the states have not converged in time. Freezing these unfinished states would worsen generation, in the worst case leading to a spiral where each token that is frozen incorrectly slows down convergence further, leading to a response that becomes more incorrect with each token.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p2">
<p class="ltx_p">However, we can remedy both cases through adaptive compute. We pick the simplest adaptive exit criterion, the normalized distance in latent space, and compute this quantity for each position and freeze up to all positions where this distance <math alttext="\delta_{i}" class="ltx_Math" display="inline" id="S3.SS5.p2.m1" intent=":literal"><semantics><msub><mi>δ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\delta_{i}</annotation></semantics></math> is smaller than a threshold <math alttext="\varepsilon" class="ltx_Math" display="inline" id="S3.SS5.p2.m2" intent=":literal"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>.</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.EGx2">
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\delta_{i}=\frac{\|\mathbf{z}_{i}-\mathbf{z}_{\text{prev},i}\|_{2}}{\|\mathbf{z}_{i}\|_{2}},\qquad k^{*}=\max\{k:\delta_{j}&lt;\varepsilon\text{ for all }j\leq k\}" class="ltx_Math" display="inline" id="S3.E3.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>δ</mi><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><msub><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝐳</mi><mi>i</mi></msub><mo>−</mo><msub><mi>𝐳</mi><mrow><mtext>prev</mtext><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><msub><mrow><mo stretchy="false">‖</mo><msub><mi>𝐳</mi><mi>i</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mfrac></mstyle></mrow><mo rspace="2.167em">,</mo><mrow><msup><mi>k</mi><mo>∗</mo></msup><mo>=</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo stretchy="false">{</mo><mrow><mi>k</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msub><mi>δ</mi><mi>j</mi></msub><mo>&lt;</mo><mrow><mi>ε</mi><mo lspace="0em" rspace="0em">​</mo><mtext> for all </mtext><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mo>≤</mo><mi>k</mi></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\delta_{i}=\frac{\|\mathbf{z}_{i}-\mathbf{z}_{\text{prev},i}\|_{2}}{\|\mathbf{z}_{i}\|_{2}},\qquad k^{*}=\max\{k:\delta_{j}&lt;\varepsilon\text{ for all }j\leq k\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">We combine this with a limiter on the maximum length of the wavefront of the algorithm to guarantee that both 1) the number of states currently being modified, so the maximum memory footprint, is bounded and 2) only positions with converged states are frozen. The full algorithm is described in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#alg2" title="In A.1 AdditionaL Algorithm Details ‣ Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Algorithm</span>˜<span class="ltx_text ltx_ref_tag">2</span></a>. With these rules in place, we note that setting the wavefront to 1 token, we exactly recover the token-per-token adaptive compute sampler from <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib25" title="">2025</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p3">
<p class="ltx_p">We show the practical outcome of this sampler for a challenging input sequence from GSM8k in a series of heatmaps in the appendix, see <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A1.F12" title="In A.4 Qualitative Evaluation ‣ Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">12</span></a>. The heatmap shows the development of the sequence as a function of generation steps and tokens. We see that the wave first advances quickly, but then halts for a short amount of steps, before resuming the advance.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="372" id="S3.F4.g1" src="x4.png" width="265"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="372" id="S3.F4.g2" src="x5.png" width="265"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="372" id="S3.F4.g3" src="x6.png" width="265"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold">Examples of adaptive sampler behavior</span>. Each color represents a token id in the vocabulary of the model, showing the development of the generated sequence (running left to right) as a function of sampler steps (running top to bottom) for <span class="ltx_text ltx_font_italic">different hyperparameter choices</span>. The leftmost example is <math alttext="r^{\prime}=4" class="ltx_Math" display="inline" id="S3.F4.m3" intent=":literal"><semantics><mrow><msup><mi>r</mi><mo>′</mo></msup><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">r^{\prime}=4</annotation></semantics></math>, and tokens are frozen quickly, whereas middle and right show sequences with <math alttext="r&lt;4" class="ltx_Math" display="inline" id="S3.F4.m4" intent=":literal"><semantics><mrow><mi>r</mi><mo>&lt;</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">r&lt;4</annotation></semantics></math> require more adaptive computation, and in both cases the sampler stalls after hitting the maximal length of the wavefront (here 32 to visualize), before resolving the sequence and advancing again.</figcaption>
</figure>
<div class="ltx_theorem ltx_theorem_remark" id="S3.Thmtheorem1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 3.1</span></span><span class="ltx_text ltx_font_bold"> </span>(Convergence of the Adaptive Diffusion Sampler)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="S3.Thmtheorem1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">With this algorithm, we can, in principle guarantee convergence to the same solution as when sampling autoregressively, if we assume that the recurrent block <math alttext="R" class="ltx_Math" display="inline" id="S3.Thmtheorem1.p1.m1" intent=":literal"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> is a contraction. Then, convergence of iterates, i.e. <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S3.E3" title="In 3.5 Adaptive Exits ‣ 3 Applying Diffusion Forcing to Recurrent-Depth Models ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Equation</span>˜<span class="ltx_text ltx_ref_tag">3</span></a>, implies convergence to the fixed point of the operator. Second, because the model is causal, convergence of the first token position does not depend others and will converge at some step <math alttext="t" class="ltx_Math" display="inline" id="S3.Thmtheorem1.p1.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>. At this step, the conditioning of the subsequent token is frozen, so it will also converge, proving convergence of the full sequence to the autoregressive solution by induction. However, in practice, large-scale recurrent-depth models are not easily proven to be contractive, even if models are approximately path-independent <cite class="ltx_cite ltx_citemacro_citep">(Anil et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib3" title="">2022</a>)</cite>.</span></p>
</div>
</div>
<div class="ltx_para" id="S3.SS5.p4">
<p class="ltx_p">Finally, we remark on practical back-of-the-envelope estimates of runtime cost.</p>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="S3.Thmtheorem2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 3.2</span></span><span class="ltx_text ltx_font_bold"> </span>(Computational Cost)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="S3.Thmtheorem2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">In comparison to the baseline autoregressive sampling algorithm where the recurrence is computed one token at a time, there are two additional sources of computational cost, the cost to encode and decode latent states using <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="S3.Thmtheorem2.p1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒫</mi><annotation encoding="application/x-tex">\mathcal{P}</annotation></semantics></math> and <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.Thmtheorem2.p1.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math>, and the potential cost incurred if convergence is slower than in baseline due to cascading effects of tokens changing late, as seen in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S3.F4" title="In 3.5 Adaptive Exits ‣ 3 Applying Diffusion Forcing to Recurrent-Depth Models ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">4</span></a> if the adaptive version is used. The first cost depends on the size of the recurrent block <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S3.Thmtheorem2.p1.m3" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℛ</mi><annotation encoding="application/x-tex">\mathcal{R}</annotation></semantics></math>, relative to prelude and coda. For the model we study in this work this is disadvantageous as the FLOP costs for prelude and coda equal one pass through the recurrent block. We define the FLOP costs of one pass through <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S3.Thmtheorem2.p1.m4" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℛ</mi><annotation encoding="application/x-tex">\mathcal{R}</annotation></semantics></math> as <math alttext="f" class="ltx_Math" display="inline" id="S3.Thmtheorem2.p1.m5" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, ignoring attention, so that the FLOP costs of one iteration of the sampler is roughly <math alttext="(r^{\prime}+1)f" class="ltx_Math" display="inline" id="S3.Thmtheorem2.p1.m6" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><msup><mi>r</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow><annotation encoding="application/x-tex">(r^{\prime}+1)f</annotation></semantics></math>. Then, the total FLOP costs of running the baseline algorithm for <math alttext="w" class="ltx_Math" display="inline" id="S3.Thmtheorem2.p1.m7" intent=":literal"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math> tokens are <math alttext="(r+1)fw" class="ltx_Math" display="inline" id="S3.Thmtheorem2.p1.m8" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>r</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">(r+1)fw</annotation></semantics></math>, compared to <math alttext="(r+\frac{r}{r^{\prime}})fw" class="ltx_Math" display="inline" id="S3.Thmtheorem2.p1.m9" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>r</mi><mo>+</mo><mfrac><mi>r</mi><msup><mi>r</mi><mo>′</mo></msup></mfrac></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">(r+\frac{r}{r^{\prime}})fw</annotation></semantics></math> for the non-adaptive diffusion sampler. However, as we will see, this FLOP inefficiency is counteracted in practice by the parallelization gains obtained from the sampler.</span></p>
</div>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Theoretical Analysis</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p">This section develops a theoretical framework to justify the optimality of our design in balancing efficiency and expressiveness with two research questions (RQs): <span class="ltx_text ltx_font_bold">(i)</span> Why should models prioritize recurrence, i.e. <span class="ltx_text ltx_font_italic">depth scaling</span>, during prefilling? and <span class="ltx_text ltx_font_bold">(ii)</span> Why should models prioritize parallelizing decoding from a larger wavefront of tokens using the sampler described in the previous section, i.e. <span class="ltx_text ltx_font_italic">width scaling</span> during decoding?</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Problem Formulation</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p">Before answering these RQs, we formalize the notions of depth and width within our framework, which limits our analysis to Transformer-based autoregressive LLMs. In particular, we focus exclusively on the comparison between depth and width, without considering length (i.e., CoT) scaling.</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="S4.Thmtheorem1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition 4.1</span></span><span class="ltx_text ltx_font_bold"> </span>(Depth and Width in Recurrent-Depth Models, informal)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="S4.Thmtheorem1.p1">
<p class="ltx_p">For recurrent-depth models, we define <em class="ltx_emph ltx_font_italic">depth</em> <math alttext="d_{t}" class="ltx_Math" display="inline" id="S4.Thmtheorem1.p1.m1" intent=":literal"><semantics><msub><mi>d</mi><mi>t</mi></msub><annotation encoding="application/x-tex">d_{t}</annotation></semantics></math> and <em class="ltx_emph ltx_font_italic">width</em> <math alttext="w_{t}" class="ltx_Math" display="inline" id="S4.Thmtheorem1.p1.m2" intent=":literal"><semantics><msub><mi>w</mi><mi>t</mi></msub><annotation encoding="application/x-tex">w_{t}</annotation></semantics></math> at each time step <math alttext="t\in\mathbb{N}" class="ltx_Math" display="inline" id="S4.Thmtheorem1.p1.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">t\in\mathbb{N}</annotation></semantics></math>, with initial conditions <math alttext="d_{0}=0" class="ltx_Math" display="inline" id="S4.Thmtheorem1.p1.m4" intent=":literal"><semantics><mrow><msub><mi>d</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">d_{0}=0</annotation></semantics></math> and <math alttext="w_{0}=L_{0}" class="ltx_Math" display="inline" id="S4.Thmtheorem1.p1.m5" intent=":literal"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><msub><mi>L</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">w_{0}=L_{0}</annotation></semantics></math> (where <math alttext="L_{0}" class="ltx_Math" display="inline" id="S4.Thmtheorem1.p1.m6" intent=":literal"><semantics><msub><mi>L</mi><mn>0</mn></msub><annotation encoding="application/x-tex">L_{0}</annotation></semantics></math> denotes the input sequence length). The corresponding update rules are given as follows:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Depth Update:</span> At each step <math alttext="t" class="ltx_Math" display="inline" id="S4.I1.i1.p1.m1" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, <math alttext="d_{t+1}=d_{t}+1" class="ltx_Math" display="inline" id="S4.I1.i1.p1.m2" intent=":literal"><semantics><mrow><msub><mi>d</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>d</mi><mi>t</mi></msub><mo>+</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">d_{t+1}=d_{t}+1</annotation></semantics></math> with <math alttext="d_{0}=0" class="ltx_Math" display="inline" id="S4.I1.i1.p1.m3" intent=":literal"><semantics><mrow><msub><mi>d</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">d_{0}=0</annotation></semantics></math>, therefore <math alttext="d_{t}=t" class="ltx_Math" display="inline" id="S4.I1.i1.p1.m4" intent=":literal"><semantics><mrow><msub><mi>d</mi><mi>t</mi></msub><mo>=</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">d_{t}=t</annotation></semantics></math> for all <math alttext="t\in\mathbb{N}" class="ltx_Math" display="inline" id="S4.I1.i1.p1.m5" intent=":literal"><semantics><mrow><mi>t</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">t\in\mathbb{N}</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Width Update:</span> At each step <math alttext="t" class="ltx_Math" display="inline" id="S4.I1.i2.p1.m1" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, width changes only through token exits and token entries:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\delta^{(t)}=\begin{cases}-1,&amp;\text{if a hidden state decodes from the model (exit event)},\\
+1,&amp;\text{if a latest token encodes into the model (entry event)}.\end{cases}" class="ltx_Math" display="block" id="S4.Ex5.m1" intent=":literal"><semantics><mrow><msup><mi>δ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mo>−</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtext>if a hidden state decodes from the model (exit event)</mtext><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mo>+</mo><mn>1</mn></mrow><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtext>if a latest token encodes into the model (entry event)</mtext><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\delta^{(t)}=\begin{cases}-1,&amp;\text{if a hidden state decodes from the model (exit event)},\\
+1,&amp;\text{if a latest token encodes into the model (entry event)}.\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</li>
</ol>
</div>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>LLMs should prioritize depth scaling during prefilling.</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p">To establish this, we first define a width scaling architecture without increasing model parameters following <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib70" title="">2025</a>)</cite>. Concretely, we repeat each token along the sequence dimension. Note that during prefilling, increasing the number of such repeated tokens is equivalent to width scaling under our definition, since this expands the input sequence length. Here, we introduce two variants:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p">Width Scaling without KV Sharing (Width-NoShare): For the <math alttext="j" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m1" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th copy of token <math alttext="i" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>, attention is allowed to all copies of tokens <math alttext="0,\dots,i-1" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m3" intent=":literal"><semantics><mrow><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">0,\dots,i-1</annotation></semantics></math>, as well as the first <math alttext="j-1" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m4" intent=":literal"><semantics><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">j-1</annotation></semantics></math> copies of token <math alttext="i" class="ltx_Math" display="inline" id="S4.I2.i1.p1.m5" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I2.i2.p1">
<p class="ltx_p">Width Scaling with KV Sharing (Width-KVShare): For the <math alttext="j" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m1" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th copy of token <math alttext="i" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>, attention is limited to (i) the last copy of tokens <math alttext="0,\dots,i-1" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m3" intent=":literal"><semantics><mrow><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">0,\dots,i-1</annotation></semantics></math>, and (ii) the first <math alttext="j-1" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m4" intent=":literal"><semantics><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">j-1</annotation></semantics></math> copies of token <math alttext="i" class="ltx_Math" display="inline" id="S4.I2.i2.p1.m5" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p">Based on the above definition, we state the importance of depth scaling during prefilling stage.</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="S4.Thmtheorem2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 4.2</span></span><span class="ltx_text ltx_font_bold"> </span>(Depth vs. Width Scaling in Prefilling, informal)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="S4.Thmtheorem2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Given the width-scaling architecture above and our recurrent-depth model with the same scaling factor <math alttext="s" class="ltx_Math" display="inline" id="S4.Thmtheorem2.p1.m1" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>. Then the following hold:</span></p>
<ol class="ltx_enumerate" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold ltx_font_italic">Expressiveness.</span><span class="ltx_text ltx_font_italic"> Under equal scaling factors, depth scaling is more expressive than width scaling.</span></p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold ltx_font_italic">Complexity.</span><span class="ltx_text ltx_font_italic"> For asymptotic prefill cost (including both attention and linear layers), we have</span></p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="E_{\textnormal{Depth}}\;\leq\;E_{\textnormal{Width\!-\!KVShare}}\;&lt;\;E_{\textnormal{Width\!-\!NoShare}}." class="ltx_Math" display="block" id="S4.Ex6.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>E</mi><mtext>Depth</mtext></msub><mo lspace="0.558em" rspace="0.558em">≤</mo><msub><mi>E</mi><mtext>Width​-​KVShare</mtext></msub><mo rspace="0.558em">&lt;</mo><msub><mi>E</mi><mtext>Width​-​NoShare</mtext></msub></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">E_{\textnormal{Depth}}\;\leq\;E_{\textnormal{Width\!-\!KVShare}}\;&lt;\;E_{\textnormal{Width\!-\!NoShare}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="S4.I3.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold ltx_font_italic">Parallelism.</span><span class="ltx_text ltx_font_italic"> There exists a threshold </span><math alttext="L_{\star}" class="ltx_Math" display="inline" id="S4.I3.i3.p1.m1" intent=":literal"><semantics><msub><mi>L</mi><mo>⋆</mo></msub><annotation encoding="application/x-tex">L_{\star}</annotation></semantics></math><span class="ltx_text ltx_font_italic"> such that for </span><math alttext="L&lt;L_{\star}" class="ltx_Math" display="inline" id="S4.I3.i3.p1.m2" intent=":literal"><semantics><mrow><mi>L</mi><mo>&lt;</mo><msub><mi>L</mi><mo>⋆</mo></msub></mrow><annotation encoding="application/x-tex">L&lt;L_{\star}</annotation></semantics></math><span class="ltx_text ltx_font_italic">, width scaling provides </span><math alttext="s^{2}" class="ltx_Math" display="inline" id="S4.I3.i3.p1.m3" intent=":literal"><semantics><msup><mi>s</mi><mn>2</mn></msup><annotation encoding="application/x-tex">s^{2}</annotation></semantics></math><span class="ltx_text ltx_font_italic"> times the parallelism of depth scaling, while for </span><math alttext="L\geq L_{\star}" class="ltx_Math" display="inline" id="S4.I3.i3.p1.m4" intent=":literal"><semantics><mrow><mi>L</mi><mo>≥</mo><msub><mi>L</mi><mo>⋆</mo></msub></mrow><annotation encoding="application/x-tex">L\geq L_{\star}</annotation></semantics></math><span class="ltx_text ltx_font_italic"> both saturate with similar parallelism.</span></p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="S4.Thmtheorem3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 4.3</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="S4.Thmtheorem3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Let <math alttext="L" class="ltx_Math" display="inline" id="S4.Thmtheorem3.p1.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> be a random variable for prompt length with distribution <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S4.Thmtheorem3.p1.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation></semantics></math>. Then the probability that depth scaling is more efficient than width scaling equals <math alttext="\Pr_{L\sim\mathcal{D}}[L\geq L_{\star}]" class="ltx_Math" display="inline" id="S4.Thmtheorem3.p1.m3" intent=":literal"><semantics><mrow><msub><mi>Pr</mi><mrow><mi>L</mi><mo>∼</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi></mrow></msub><mo>⁡</mo><mrow><mo stretchy="false">[</mo><mrow><mi>L</mi><mo>≥</mo><msub><mi>L</mi><mo>⋆</mo></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\Pr_{L\sim\mathcal{D}}[L\geq L_{\star}]</annotation></semantics></math>. Since <math alttext="L_{\star}" class="ltx_Math" display="inline" id="S4.Thmtheorem3.p1.m4" intent=":literal"><semantics><msub><mi>L</mi><mo>⋆</mo></msub><annotation encoding="application/x-tex">L_{\star}</annotation></semantics></math> on modern GPUs typically lies between a few hundred and a few thousand tokens while empirical input length distributions place substantial mass above this range, the probability is indeed close to <math alttext="1" class="ltx_Math" display="inline" id="S4.Thmtheorem3.p1.m5" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> in practice.</span></p>
</div>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>LLMs should prioritize width scaling during decoding.</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p">Next, we prove that recurrent-depth models should use diffusion forcing samplers during decoding.</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="S4.Thmtheorem4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem 4.4</span></span><span class="ltx_text ltx_font_bold"> </span>(Depth vs. Width Scaling in Decoding, informal)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="S4.Thmtheorem4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">For recurrent-depth models with <math alttext="r&gt;1" class="ltx_Math" display="inline" id="S4.Thmtheorem4.p1.m1" intent=":literal"><semantics><mrow><mi>r</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r&gt;1</annotation></semantics></math> inner recurrences, if diffusion forcing sampling and KV-cache sharing are employed with wavefront size <math alttext="W\leq L_{\star}" class="ltx_Math" display="inline" id="S4.Thmtheorem4.p1.m2" intent=":literal"><semantics><mrow><mi>W</mi><mo>≤</mo><msub><mi>L</mi><mo>⋆</mo></msub></mrow><annotation encoding="application/x-tex">W\leq L_{\star}</annotation></semantics></math>, then diffusion forcing decoding achieves equal depth and strictly greater width compared to standard autoregressive decoding under the same runtime constraints. Mathematically, this relationship can be expressed as:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d_{\text{DF}}(T)=d_{\text{AR}}(T)\quad\text{and}\quad w_{\text{DF}}(T)&gt;w_{\text{AR}}(T)," class="ltx_Math" display="block" id="S4.Ex7.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><msub><mi>d</mi><mtext class="ltx_mathvariant_italic">DF</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>d</mi><mtext class="ltx_mathvariant_italic">AR</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></mrow><mspace style="width:1em;" width="1em"></mspace><mtext class="ltx_mathvariant_italic">and</mtext></mrow></mrow><mspace style="width:1em;" width="1em"></mspace><mrow><mrow><msub><mi>w</mi><mtext class="ltx_mathvariant_italic">DF</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mrow><msub><mi>w</mi><mtext class="ltx_mathvariant_italic">AR</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">d_{\text{DF}}(T)=d_{\text{AR}}(T)\quad\text{and}\quad w_{\text{DF}}(T)&gt;w_{\text{AR}}(T),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where <math alttext="T" class="ltx_Math" display="inline" id="S4.Thmtheorem4.p1.m3" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is the runtime budget, and <span class="ltx_text ltx_markedasmath">DF</span> and <span class="ltx_text ltx_markedasmath">AR</span> denote diffusion forcing and autoregressive decoding.</span></p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="S4.Thmtheorem5">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark 4.5</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="S4.Thmtheorem5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Since model parameters and KV states are shared, the I/O cost of processing multiple tokens is asymptotically equivalent to processing a single token, enabling increased token generation within identical runtime constraints. At each decoding step, an expanded wavefront enables greater width scaling, providing superior expressiveness compared to autoregressive decoding. Empirically, since maximum recurrence depth rarely exceeds <math alttext="r\approx 100" class="ltx_Math" display="inline" id="S4.Thmtheorem5.p1.m1" intent=":literal"><semantics><mrow><mi>r</mi><mo>≈</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">r\approx 100</annotation></semantics></math>, the condition <math alttext="W\leq L_{\star}" class="ltx_Math" display="inline" id="S4.Thmtheorem5.p1.m2" intent=":literal"><semantics><mrow><mi>W</mi><mo>≤</mo><msub><mi>L</mi><mo>⋆</mo></msub></mrow><annotation encoding="application/x-tex">W\leq L_{\star}</annotation></semantics></math> typically holds.</span></p>
</div>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Evaluation</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p">To assess whether our method really accelerates generation, we compare our sampler against an equally optimized implementation of standard autoregressive sampling, both evaluated with a batch size of 1. Extensions to larger batch sizes are conceivable but fall outside the scope of this study, see additional discussion in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A1.SS2" title="A.2 Additional Variants ‣ Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Section</span>˜<span class="ltx_text ltx_ref_tag">A.2</span></a></p>
</div>
<figure class="ltx_table" id="S5.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:140.2pt;vertical-align:-67.5pt;"><span class="ltx_transformed_inner" style="transform:translate(7.8pt,-2.5pt) scale(1.03710661555946,1.03710661555946) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2" style="padding-left:2.8pt;padding-right:2.8pt;">Sampler</th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding-left:2.8pt;padding-right:2.8pt;">GSM8K</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding-left:2.8pt;padding-right:2.8pt;">MATH500</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding-left:2.8pt;padding-right:2.8pt;">HumanEval</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding-left:2.8pt;padding-right:2.8pt;">MBPP</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Acc</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">t/s</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Acc</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">t/s</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Acc</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">t/s</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Acc</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">t/s</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Static AR (<math alttext="r=32" class="ltx_Math" display="inline" id="S5.T1.m1" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">r=32</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">41.77%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">36.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">17.60%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">6.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">22.56%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">13.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">31.60%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">15.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Static AR (<math alttext="r=4" class="ltx_Math" display="inline" id="S5.T1.m2" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">r=4</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">1.59%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">312.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">3.20%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">18.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">0.61%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">244.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">1.40%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">49.6</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.8pt;padding-right:2.8pt;">Static AR (<math alttext="r=8" class="ltx_Math" display="inline" id="S5.T1.m3" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">r=8</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">31.61%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">137.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">14.80%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">23.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">21.34%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">61.7</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">27.40%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">57.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.8pt;padding-right:2.8pt;">Static AR (<math alttext="r=64" class="ltx_Math" display="inline" id="S5.T1.m4" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">r=64</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">42.15%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">18.2</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">18.60%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">3.4</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">22.56%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">7.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">30.20%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">7.6</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Adaptive Compute AR</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">42.23%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">66.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">18.20%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">12.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">21.95%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">26.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">30.20%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">29.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.8pt;padding-right:2.8pt;">Speculative Decoding AR</th>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">42.76%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">69.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">17.80%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">13.4</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">20.12%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">27.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">30.60%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">31.6</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Diff. Sampler (<math alttext="r^{\prime}=2,\beta_{t}=0.5" class="ltx_Math" display="inline" id="S5.T1.m5" intent=":literal"><semantics><mrow><mrow><msup><mi>r</mi><mo>′</mo></msup><mo>=</mo><mn>2</mn></mrow><mo>,</mo><mrow><msub><mi>β</mi><mi>t</mi></msub><mo>=</mo><mn>0.5</mn></mrow></mrow><annotation encoding="application/x-tex">r^{\prime}=2,\beta_{t}=0.5</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">40.71%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">182.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">17.60%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">35.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">20.12%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">67.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">27.80%</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">92.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.8pt;padding-right:2.8pt;">Diff. Sampler (<math alttext="r^{\prime}=4,\beta_{t}=0" class="ltx_Math" display="inline" id="S5.T1.m6" intent=":literal"><semantics><mrow><mrow><msup><mi>r</mi><mo>′</mo></msup><mo>=</mo><mn>4</mn></mrow><mo>,</mo><mrow><msub><mi>β</mi><mi>t</mi></msub><mo>=</mo><mn>0</mn></mrow></mrow><annotation encoding="application/x-tex">r^{\prime}=4,\beta_{t}=0</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">42.08%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">157.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">18.00%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">30.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">20.12%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">64.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">31.00%</td>
<td class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">70.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Relative Diff to AR (<math alttext="r=32" class="ltx_Math" display="inline" id="S5.T1.m7" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">r=32</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">+0.31</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold">4.36x</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">+0.40</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold">4.73x</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">-2.44</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold">4.81x</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">-0.60</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold">4.59x</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison of autoregressive (AR) and diffusion samplers for the <span class="ltx_text ltx_font_italic">Huginn-0125</span> model using a comparable backend (batch size 1, <span class="ltx_text ltx_font_typewriter">transformers</span> with dynamic KV caching, no further inference optimizations). For both samplers, we record the total evaluation time divided by the number of samples. “Acc” denotes task accuracy, and “t/s” denotes the median of tokens/second measurements for all samples in the task.
</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="193" id="S5.F5.g1" src="x7.png" width="407"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="194" id="S5.F5.g2" src="x8.png" width="407"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Trade-off between accuracy and speed on GSM8k under different hyperparameter choices. <span class="ltx_text ltx_font_bold">Left:</span> Effect of increasing inner recurrence <math alttext="r^{\prime}" class="ltx_Math" display="inline" id="S5.F5.m3" intent=":literal"><semantics><msup><mi>r</mi><mo>′</mo></msup><annotation encoding="application/x-tex">r^{\prime}</annotation></semantics></math>. Inner recurrence stabilizes the sampling, increasing accuracy at the cost of throughput. <span class="ltx_text ltx_font_bold">Right:</span> Effect of varying the exit threshold <math alttext="\varepsilon" class="ltx_Math" display="inline" id="S5.F5.m4" intent=":literal"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>. Modulating the exit threshold most directly trades off throughput and accuracy.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p">We evaluate the 4 generative benchmarks (GSM8K, MATH500, HumanEval and MBPP) also evaluated in <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib25" title="">2025</a>)</cite>, which we rerun using our sampler and compare against a number of baselines.
Aside from the <span class="ltx_text ltx_font_bold">static, autoregressive baseline</span> (static AR), at different recurrence steps, we also compare against the <span class="ltx_text ltx_font_bold">adaptive compute</span> sampler of the original work, which still samples token-by-token, but exits the recurrence at every token, once the difference in latent space is small enough. We tune this sampler, finding that its hyperparameter, the threshold <math alttext="\varepsilon" class="ltx_Math" display="inline" id="S5.p2.m1" intent=":literal"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math> is similar to the diffusion sampler.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p">Finally, we also compare against a heavily tuned <span class="ltx_text ltx_font_bold">self-speculative decoding baseline</span>. It was observed in <cite class="ltx_cite ltx_citemacro_citet">Geiping et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib25" title="">2025</a>)</cite> that recurrent-depth models can be natively used as their own draft models, using fewer steps to draft. We find that drafting 4 tokens into the future, each with 4 draft steps is optimal for the <span class="ltx_text ltx_font_italic">Huginn-0125</span> checkpoint on GSM8k.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p">We implement all samplers in comparable Hugging Face <span class="ltx_text ltx_font_typewriter">transformers</span> implementations with dynamic KV caching and we measure mean accuracy and median tokens per second, computed over queries from each benchmark. All timings are obtained from CUDA event measurements on sandboxed A100-40GB GPUs. If not otherwise mentioned, we default to conservative settings for the sampler, always setting an exit threshold of <math alttext="\varepsilon=0.03" class="ltx_Math" display="inline" id="S5.p4.m1" intent=":literal"><semantics><mrow><mi>ε</mi><mo>=</mo><mn>0.03</mn></mrow><annotation encoding="application/x-tex">\varepsilon=0.03</annotation></semantics></math>, <math alttext="\beta_{t}=0" class="ltx_Math" display="inline" id="S5.p4.m2" intent=":literal"><semantics><mrow><msub><mi>β</mi><mi>t</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_{t}=0</annotation></semantics></math>, <math alttext="\eta=0.1" class="ltx_Math" display="inline" id="S5.p4.m3" intent=":literal"><semantics><mrow><mi>η</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\eta=0.1</annotation></semantics></math> and <math alttext="r^{\prime}=4" class="ltx_Math" display="inline" id="S5.p4.m4" intent=":literal"><semantics><mrow><msup><mi>r</mi><mo>′</mo></msup><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">r^{\prime}=4</annotation></semantics></math>, for a maximum wavefront size of <math alttext="128" class="ltx_Math" display="inline" id="S5.p4.m5" intent=":literal"><semantics><mn>128</mn><annotation encoding="application/x-tex">128</annotation></semantics></math>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Benchmark Results.</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p">We summarize our findings in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.T1" title="In 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Table</span>˜<span class="ltx_text ltx_ref_tag">1</span></a>.
We find that on all benchmarks, executing the parallelized sampler leads to significant speedups of around 5x, with only minor trade-offs in generation quality of around 1%, depending on the task, owing to the trade-off set by our default hyperparameters. In <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.T2" title="In 5.1 Benchmark Results. ‣ 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Table</span>˜<span class="ltx_text ltx_ref_tag">2</span></a> we repeat all benchmarks for two additional model checkpoints, the SWA model also released in <cite class="ltx_cite ltx_citemacro_citet">Geiping et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib25" title="">2025</a>)</cite>, and a math variant, that we finetuned on the MetaMath dataset <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib74" title="">2023</a>)</cite>. Even though these model variants differ noticeably in their benchmark scores, they show similar gains and trade-offfs when using the diffusion sampler.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2">Sampler</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">GSM8K</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Minerva Math</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">HumanEval</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">MBPP</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Acc</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Time</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Acc</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Time</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Acc</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Time</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Acc</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Time</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="8">Huginn-0125</th>
<td class="ltx_td ltx_border_t"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Static AR (<math alttext="r=32" class="ltx_Math" display="inline" id="S5.T2.m1" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">r=32</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_t">41.77%</td>
<td class="ltx_td ltx_align_center ltx_border_t">36.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">12.98%</td>
<td class="ltx_td ltx_align_center ltx_border_t">21.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">22.56%</td>
<td class="ltx_td ltx_align_center ltx_border_t">13.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">31.60%</td>
<td class="ltx_td ltx_align_center ltx_border_t">15.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Diff. Sampler (<math alttext="r^{\prime}=4,\beta_{t}=0" class="ltx_Math" display="inline" id="S5.T2.m2" intent=":literal"><semantics><mrow><mrow><msup><mi>r</mi><mo>′</mo></msup><mo>=</mo><mn>4</mn></mrow><mo>,</mo><mrow><msub><mi>β</mi><mi>t</mi></msub><mo>=</mo><mn>0</mn></mrow></mrow><annotation encoding="application/x-tex">r^{\prime}=4,\beta_{t}=0</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center">42.08%</td>
<td class="ltx_td ltx_align_center">157.3</td>
<td class="ltx_td ltx_align_center">13.06%</td>
<td class="ltx_td ltx_align_center">96.0</td>
<td class="ltx_td ltx_align_center">20.12%</td>
<td class="ltx_td ltx_align_center">64.9</td>
<td class="ltx_td ltx_align_center">31.00%</td>
<td class="ltx_td ltx_align_center">70.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="8">SWA Model Variant</th>
<td class="ltx_td ltx_border_t"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Static AR (<math alttext="r=32" class="ltx_Math" display="inline" id="S5.T2.m3" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">r=32</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_t">47.99%</td>
<td class="ltx_td ltx_align_center ltx_border_t">36.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">14.86%</td>
<td class="ltx_td ltx_align_center ltx_border_t">22.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">23.78%</td>
<td class="ltx_td ltx_align_center ltx_border_t">14.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">31.20%</td>
<td class="ltx_td ltx_align_center ltx_border_t">11.8</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Diff. Sampler (<math alttext="r^{\prime}=4,\beta_{t}=0" class="ltx_Math" display="inline" id="S5.T2.m4" intent=":literal"><semantics><mrow><mrow><msup><mi>r</mi><mo>′</mo></msup><mo>=</mo><mn>4</mn></mrow><mo>,</mo><mrow><msub><mi>β</mi><mi>t</mi></msub><mo>=</mo><mn>0</mn></mrow></mrow><annotation encoding="application/x-tex">r^{\prime}=4,\beta_{t}=0</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center">47.08%</td>
<td class="ltx_td ltx_align_center">143.1</td>
<td class="ltx_td ltx_align_center">14.52%</td>
<td class="ltx_td ltx_align_center">101.4</td>
<td class="ltx_td ltx_align_center">23.78%</td>
<td class="ltx_td ltx_align_center">71.2</td>
<td class="ltx_td ltx_align_center">29.20%</td>
<td class="ltx_td ltx_align_center">59.7</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="8">Math-Finetuned Model</th>
<td class="ltx_td ltx_border_t"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Static AR (<math alttext="r=32" class="ltx_Math" display="inline" id="S5.T2.m5" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">r=32</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_t">58.91%</td>
<td class="ltx_td ltx_align_center ltx_border_t">29.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">22.20%</td>
<td class="ltx_td ltx_align_center ltx_border_t">7.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">17.07%</td>
<td class="ltx_td ltx_align_center ltx_border_t">11.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">28.80%</td>
<td class="ltx_td ltx_align_center ltx_border_t">11.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Diff. Sampler (<math alttext="r^{\prime}=4,\beta_{t}=0" class="ltx_Math" display="inline" id="S5.T2.m6" intent=":literal"><semantics><mrow><mrow><msup><mi>r</mi><mo>′</mo></msup><mo>=</mo><mn>4</mn></mrow><mo>,</mo><mrow><msub><mi>β</mi><mi>t</mi></msub><mo>=</mo><mn>0</mn></mrow></mrow><annotation encoding="application/x-tex">r^{\prime}=4,\beta_{t}=0</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_bb">58.45%</td>
<td class="ltx_td ltx_align_center ltx_border_bb">144.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb">21.40%</td>
<td class="ltx_td ltx_align_center ltx_border_bb">39.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb">15.24%</td>
<td class="ltx_td ltx_align_center ltx_border_bb">47.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb">27.60%</td>
<td class="ltx_td ltx_align_center ltx_border_bb">57.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Hyperparameters remain stable across different model variants. For example, both the weight-averaged checkpoint from the original work and the model finetuned on MetaMath for this study exhibit consistent speed gains in the range of 4–5× and accuracy deviations within 0.5–1%, even when baseline values change.
</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="195" id="S5.F6.g1" src="x9.png" width="407"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="194" id="S5.F6.g2" src="x10.png" width="407"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold">Left:</span> Scaling the amount of momentum <math alttext="\eta" class="ltx_Math" display="inline" id="S5.F6.m7" intent=":literal"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math> in the conditioning., showing that small, but non-zero <math alttext="\eta" class="ltx_Math" display="inline" id="S5.F6.m8" intent=":literal"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math> values are optimal. <span class="ltx_text ltx_font_bold">Right:</span> Scaling the amount of noise added during inference for <math alttext="r^{\prime}=4" class="ltx_Math" display="inline" id="S5.F6.m9" intent=":literal"><semantics><mrow><msup><mi>r</mi><mo>′</mo></msup><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">r^{\prime}=4</annotation></semantics></math>, scheduled linearly in the number of recurrence steps, also measured on GSM8k. At <math alttext="r^{\prime}=4" class="ltx_Math" display="inline" id="S5.F6.m10" intent=":literal"><semantics><mrow><msup><mi>r</mi><mo>′</mo></msup><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">r^{\prime}=4</annotation></semantics></math>, adding noise is not optimal. We plot the full spectrum of <math alttext="r^{\prime}" class="ltx_Math" display="inline" id="S5.F6.m11" intent=":literal"><semantics><msup><mi>r</mi><mo>′</mo></msup><annotation encoding="application/x-tex">r^{\prime}</annotation></semantics></math> to <math alttext="\beta_{t}" class="ltx_Math" display="inline" id="S5.F6.m12" intent=":literal"><semantics><msub><mi>β</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\beta_{t}</annotation></semantics></math> in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.F7" title="In Moving Forward Multiple Steps. ‣ 5.2 Variants and Hyperparameters ‣ 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">7</span></a>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Variants and Hyperparameters</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Hyperparameter Choices.</span> We show the trade-off curves arising when varying the inner recurrence <math alttext="r^{\prime}" class="ltx_Math" display="inline" id="S5.SS2.p1.m1" intent=":literal"><semantics><msup><mi>r</mi><mo>′</mo></msup><annotation encoding="application/x-tex">r^{\prime}</annotation></semantics></math> and the exit threshold <math alttext="\varepsilon" class="ltx_Math" display="inline" id="S5.SS2.p1.m2" intent=":literal"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math> in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.F5" title="In 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">5</span></a> for two settings of noise <math alttext="\beta_{t}" class="ltx_Math" display="inline" id="S5.SS2.p1.m3" intent=":literal"><semantics><msub><mi>β</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\beta_{t}</annotation></semantics></math>, finding that we can effectively trade-off additional generation speed against minor losses in accuracy. We further vary the embedding EMA <math alttext="\eta" class="ltx_Math" display="inline" id="S5.SS2.p1.m4" intent=":literal"><semantics><mi>η</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math> and the noise schedule in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.F6" title="In 5.1 Benchmark Results. ‣ 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">6</span></a>, showing that the sampler is robust to a broad range of settings for both options, although upsides are also limited.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p">In <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.F7" title="In Moving Forward Multiple Steps. ‣ 5.2 Variants and Hyperparameters ‣ 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">7</span></a>, we sweep a range of values for <math alttext="r^{\prime}" class="ltx_Math" display="inline" id="S5.SS2.p2.m1" intent=":literal"><semantics><msup><mi>r</mi><mo>′</mo></msup><annotation encoding="application/x-tex">r^{\prime}</annotation></semantics></math> and <math alttext="\beta_{t}" class="ltx_Math" display="inline" id="S5.SS2.p2.m2" intent=":literal"><semantics><msub><mi>β</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\beta_{t}</annotation></semantics></math>, showing that, on average, more noise is helpful if the model takes fewer inner recurrence steps. In <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.F8" title="In Moving Forward Multiple Steps. ‣ 5.2 Variants and Hyperparameters ‣ 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">8</span></a> (left), we confirm that larger maximum wavefront sizes (i.e. the number of tokens that is modified at once in the adaptive sampler) allow for better parallelization. For the tested A100 GPU, the optimal maximal wavefront size is between 64 and 128, although this is likely accelerator-specific.</p>
</div>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Moving Forward Multiple Steps.</h5>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p">In principle, there is no limitation of only advancing one token at a time, and so we can consider <span class="ltx_text ltx_font_italic">headways</span> greater than 1, however, for these, we have no prior position to decode from, so we can only fill these positions with random tokens, or a particular padding token. And, given that the model is still causal, it will take several steps for sequential dependencies to be resolved, even if we sample a large headway in every step. We experiment with headways greater than one, but while interestingly stable, this accelerates the speed of the sampler only marginally at a cost to accuracy, see <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.F8" title="In Moving Forward Multiple Steps. ‣ 5.2 Variants and Hyperparameters ‣ 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">8</span></a>, right.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="445" id="S5.F7.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The Pareto Curve of Accuracy and Throughput on GSM8k spanned by varying inner recurrence and noise hyperparameter pairs <math alttext="(r^{\prime},\beta_{t})" class="ltx_Math" display="inline" id="S5.F7.m4" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>r</mi><mo>′</mo></msup><mo>,</mo><msub><mi>β</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(r^{\prime},\beta_{t})</annotation></semantics></math>. Adding moderate amounts of noise, e.g. <math alttext="\beta_{t}=0.2" class="ltx_Math" display="inline" id="S5.F7.m5" intent=":literal"><semantics><mrow><msub><mi>β</mi><mi>t</mi></msub><mo>=</mo><mn>0.2</mn></mrow><annotation encoding="application/x-tex">\beta_{t}=0.2</annotation></semantics></math> is dominating runs with no noise added. Note also the scale of y-axis, as even at the rightmost part of the frontier, we are observing accuracy losses of only <math alttext="2\%" class="ltx_Math" display="inline" id="S5.F7.m6" intent=":literal"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">2\%</annotation></semantics></math>.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="196" id="S5.F8.g1" src="x12.png" width="407"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="195" id="S5.F8.g2" src="x13.png" width="407"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Impact of Additional Hyperparameter Choices on GSM8k. <span class="ltx_text ltx_font_bold">Left:</span> Size of the wavefront. Increasing wavefront size up to a value around 64-128 appears optimal. We note that the optimal wavefront size is also likely to be accelerator-specific. <span class="ltx_text ltx_font_bold">Right:</span> Amount of headway. Larger amounts of headway than 1, i.e. advancing the sampler more than 1 token per step, do not seem to materialize practical speedups for the studied model. </figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions: Are Recurrent-depth Transformers secretly continuous language diffusion models?</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p">We have shown that, surprisingly, diffusion forcing samplers can be directly applied to parallelize the inference of existing recurrent-depth language models, which we justify theoretically, and implement in practice, leading to five times faster single-sequence inference, even on reasoning and coding benchmark questions. Interestingly, we could also interpret this relationship in the opposite direction, namely that the recurrent-depth models of <cite class="ltx_cite ltx_citemacro_citet">Geiping et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib25" title="">2025</a>)</cite> <span class="ltx_text ltx_font_italic">are</span> effectively continuous latent language diffusion models, just trained with an unusual objective, namely truncated unrolling. This would imply that unrolling objectives could be competitive objectives for future language diffusion models.
However, while this comparison is possible, the recurrent models like <span class="ltx_text ltx_font_italic">Huginn-0125</span> are still causal, at least without further training, and so this advantage of diffusion modeling remains elusive.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS0.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>
<div class="ltx_para ltx_noindent" id="S6.SS0.SSSx1.p1">
<p class="ltx_p">JG acknowledges the support of the Hector foundation and the Max Planck Computing and Data Facility (MPCDF), especially the compute cluster Raven. We are especially thankful that the MPCDF team was able to address the overheating issues that coincided with the large-scale deployment of the evaluation of this sampling algorithm to the Raven compute cluster. GS acknowledges the support of the
International Max Planck Research School for Intelligent Systems (IMPRS-IS).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS0.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Reproducibility Statement</h4>
<div class="ltx_para" id="S6.SS0.SSSx2.p1">
<p class="ltx_p">We provide the complete sampling algorithm we describe, including all options at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/seal-rg/recurrent-pretraining" title="">https://github.com/seal-rg/recurrent-pretraining</a>. We provide experimental details in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5" title="5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Section</span>˜<span class="ltx_text ltx_ref_tag">5</span></a> and provide further ablations and variants in the appendix. If not otherwise mentioned, all measured values are based on at least 5 repeated experiments. All timing are measured using CUDA events on GPUs of equal power, and are comparable to timings in the same table or figure.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abnar et al. (2023)</span>
<span class="ltx_bibblock">
Samira Abnar, Omid Saremi, Laurent Dinh, Shantel Wilson, Miguel Angel Bautista, Chen Huang, Vimal Thilak, Etai Littwin, Jiatao Gu, Josh Susskind, and Samy Bengio. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2310.08866" title="">Adaptivity and Modularity for Efficient Generalization Over Task Complexity</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2310.08866[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amari (1972)</span>
<span class="ltx_bibblock">
S.-I. Amari. 1972.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/T-C.1972.223477" title="">Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">IEEE Transactions on Computers</em>, C-21(11):1197–1206.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil et al. (2022)</span>
<span class="ltx_bibblock">
Cem Anil, Ashwini Pokle, Kaiqu Liang, Johannes Treutlein, Yuhuai Wu, Shaojie Bai, J. Zico Kolter, and Roger Baker Grosse. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=kgT6D7Z4Xv9" title="">Path Independent Equilibrium Models Can Better Exploit Test-Time Computation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arriola et al. (2025)</span>
<span class="ltx_bibblock">
Marianne Arriola, Aaron Gokaslan, Justin T. Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2503.09573" title="">Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2503.09573[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin et al. (2021)</span>
<span class="ltx_bibblock">
Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html" title="">Structured Denoising Diffusion Models in Discrete State-Spaces</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 34, pages 17981–17993. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bae et al. (2024)</span>
<span class="ltx_bibblock">
Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, and Tal Schuster. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2410.20672" title="">Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al. (2023)</span>
<span class="ltx_bibblock">
Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Roni Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=pzpWBbnwiJ" title="">Universal Guidance for Diffusion Models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al. (2022)</span>
<span class="ltx_bibblock">
Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=PPjSKy40XUB" title="">End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bear et al. (2024)</span>
<span class="ltx_bibblock">
Jay Bear, Adam Prügel-Bennett, and Jonathon Hare. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2410.23451" title="">Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2410.23451[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braitenberg (1986)</span>
<span class="ltx_bibblock">
Valentino Braitenberg. 1986.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Vehicles: Experiments in Synthetic Psychology</em>.

</span>
<span class="ltx_bibblock">MIT press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2024)</span>
<span class="ltx_bibblock">
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2401.10774" title="">Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2401.10774[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024a)</span>
<span class="ltx_bibblock">
Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2407.01392" title="">Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2407.01392[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024b)</span>
<span class="ltx_bibblock">
Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2407.01392" title="">Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2407.01392[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=3itjR9QxFw" title="">Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024c)</span>
<span class="ltx_bibblock">
Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. 2024c.

</span>
<span class="ltx_bibblock">Sequoia: Scalable, robust, and hardware-aware speculative decoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.12374</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng and Durme (2024)</span>
<span class="ltx_bibblock">
Jeffrey Cheng and Benjamin Van Durme. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2412.13171" title="">Compressed Chain of Thought: Efficient Reasoning Through Dense Representations</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2412.13171[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Csordás et al. (2024)</span>
<span class="ltx_bibblock">
Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber, Christopher Potts, and Christopher D. Manning. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=ZxVrkm7Bjl&amp;noteId=xzoi2mTLOI" title="">MoEUT: Mixture-of-Experts Universal Transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Thirty-eighth Annual Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepMind (2025)</span>
<span class="ltx_bibblock">
Google DeepMind. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://blog.google/technology/google-deepmind/gemini-diffusion/" title="">Gemini Diffusion</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI et al. (2025)</span>
<span class="ltx_bibblock">
DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2501.12948" title="">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2501.12948[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dehghani et al. (2019)</span>
<span class="ltx_bibblock">
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1807.03819" title="">Universal Transformers</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:1807.03819[cs, stat]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dieleman et al. (2022)</span>
<span class="ltx_bibblock">
Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, Rémi Leblond, Will Grathwohl, and Jonas Adler. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2211.15089" title="">Continuous diffusion for categorical data</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2211.15089[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Douglas and Martin (2004)</span>
<span class="ltx_bibblock">
Rodney J. Douglas and Kevan A. C. Martin. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1146/annurev.neuro.27.070203.144152" title="">Neuronal circuits of the neocortex</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Annual Review of Neuroscience</em>, 27:419–451.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2025)</span>
<span class="ltx_bibblock">
Ying Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=2edigk8yoU" title="">Looped Transformers for Length Generalization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Thirteenth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gatmiry et al. (2024)</span>
<span class="ltx_bibblock">
Khashayar Gatmiry, Nikunj Saunshi, Sashank J. Reddi, Stefanie Jegelka, and Sanjiv Kumar. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2410.08292" title="">Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiping et al. (2025)</span>
<span class="ltx_bibblock">
Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2502.05171" title="">Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2502.05171[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gers and Schmidhuber (2000)</span>
<span class="ltx_bibblock">
F.A. Gers and J. Schmidhuber. 2000.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/IJCNN.2000.861302" title="">Recurrent nets that time and count</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium</em>, volume 3, pages 189–194 vol.3.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Giannou et al. (2023)</span>
<span class="ltx_bibblock">
Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopoulos. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/giannou23a.html" title="">Looped Transformers as Programmable Computers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, pages 11398–11442. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. (2025a)</span>
<span class="ltx_bibblock">
Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong. 2025a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2410.17891" title="">Scaling Diffusion Language Models via Adaptation from Autoregressive Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2410.17891[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. (2025b)</span>
<span class="ltx_bibblock">
Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. 2025b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2506.20639" title="">DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2506.20639[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves (2017)</span>
<span class="ltx_bibblock">
Alex Graves. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.1603.08983" title="">Adaptive Computation Time for Recurrent Neural Networks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:1603.08983[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et al. (2025)</span>
<span class="ltx_bibblock">
Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2308.07037" title="">Bayesian Flow Networks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2308.07037[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et al. (2014)</span>
<span class="ltx_bibblock">
Alex Graves, Greg Wayne, and Ivo Danihelka. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1410.5401" title="">Neural Turing Machines</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:1410.5401[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2023)</span>
<span class="ltx_bibblock">
Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.647" title="">SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 11575–11596, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al. (2024)</span>
<span class="ltx_bibblock">
Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2412.06769" title="">Training Large Language Models to Reason in a Continuous Latent Space</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2412.06769[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hay and Wolf (2023)</span>
<span class="ltx_bibblock">
Tamir David Hay and Lior Wolf. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=d4uL2MSe0z" title="">Dynamic Layer Tying for Parameter-Efficient Transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoogeboom et al. (2021)</span>
<span class="ltx_bibblock">
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2021/hash/67d96d458abdef21792e6d8e590244e7-Abstract.html" title="">Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 34, pages 12454–12465. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hopfield (1982)</span>
<span class="ltx_bibblock">
J J Hopfield. 1982.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/" title="">Neural networks and physical systems with emergent collective computational abilities.</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences of the United States of America</em>, 79(8):2554–2558.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jo and Hwang (2025)</span>
<span class="ltx_bibblock">
Jaehyeong Jo and Sung Ju Hwang. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2502.11564" title="">Continuous Diffusion Model for Language Modeling</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2502.11564[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al. (2024)</span>
<span class="ltx_bibblock">
Guy Kaplan, Matanel Oren, Yuval Reif, and Roy Schwartz. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2410.05864" title="">From Tokens to Words: On the Inner Lexicon of LLMs</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2410.05864[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimi Mahabadi et al. (2024)</span>
<span class="ltx_bibblock">
Rabeeh Karimi Mahabadi, Hamish Ivison, Jaesung Tae, James Henderson, Iz Beltagy, Matthew Peters, and Arman Cohan. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2024.eacl-long.144" title="">TESS: Text-to-Text Self-Conditioned Simplex Diffusion</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2347–2361, St. Julian’s, Malta. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lamme and Roelfsema (2000)</span>
<span class="ltx_bibblock">
V. A. Lamme and P. R. Roelfsema. 2000.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/s0166-2236(00)01657-x" title="">The distinct modes of vision offered by feedforward and recurrent processing</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Trends in Neurosciences</em>, 23(11):571–579.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leviathan et al. (2023)</span>
<span class="ltx_bibblock">
Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/leviathan23a.html" title="">Fast Inference from Transformers via Speculative Decoding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, pages 19274–19286. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Xian Li, Asa Cooper Stickland, Yuqing Tang, and Xiang Kong. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2009.13102" title="">Deep Transformers with Latent Depth</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2009.13102[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024a)</span>
<span class="ltx_bibblock">
Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, and Arthur Szlam. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2412.17747" title="">Deliberation in Latent Space via Differentiable Cache Augmentation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2412.17747[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024b)</span>
<span class="ltx_bibblock">
Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, and Vikas Chandra. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.14905" title="">MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2402.14905[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lou et al. (2024)</span>
<span class="ltx_bibblock">
Aaron Lou, Chenlin Meng, and Stefano Ermon. 2024.

</span>
<span class="ltx_bibblock">Discrete diffusion modeling by estimating the ratios of the data distribution.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 41st International Conference on Machine Learning</em>, volume 235 of <em class="ltx_emph ltx_font_italic">ICML’24</em>, pages 32819–32848, Vienna, Austria. JMLR.org.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathur et al. (2024)</span>
<span class="ltx_bibblock">
Mrinal Mathur, Barak A. Pearlmutter, and Sergey M. Plis. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=EjJGND0m1x" title="">MIND over Body: Adaptive Thinking using Dynamic Computation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Thirteenth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McLeish et al. (2024)</span>
<span class="ltx_bibblock">
Sean Michael McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild, and Tom Goldstein. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=aIyNLWXuDO" title="">Transformers Can Do Arithmetic with the Right Embeddings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Thirty-eighth Annual Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merrill and Sabharwal (2023)</span>
<span class="ltx_bibblock">
William Merrill and Ashish Sabharwal. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00562" title="">The Parallelism Tradeoff: Limitations of Log-Precision Transformers</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:531–545.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merrill and Sabharwal (2025)</span>
<span class="ltx_bibblock">
William Merrill and Ashish Sabharwal. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2503.03961" title="">A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2503.03961[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao et al. (2024)</span>
<span class="ltx_bibblock">
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, and 1 others. 2024.

</span>
<span class="ltx_bibblock">Specinfer: Accelerating large language model serving with tree-based speculative inference and verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</em>, pages 932–949.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. (2010)</span>
<span class="ltx_bibblock">
Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černocký, and Sanjeev Khudanpur. 2010.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.21437/Interspeech.2010-343" title="">Recurrent neural network based language model</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proc. Interspeech 2010</em>, pages 1045–1048.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohtashami et al. (2024)</span>
<span class="ltx_bibblock">
Amirkeivan Mohtashami, Matteo Pagliardini, and Martin Jaggi. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=7igPXQFupX" title="">CoTFormer: A Chain of Thought Driven Architecture with Budget-Adaptive Computation Cost at Inference</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Thirteenth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie et al. (2025)</span>
<span class="ltx_bibblock">
Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2502.09992" title="">Large Language Diffusion Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2502.09992[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peebles and Xie (2023)</span>
<span class="ltx_bibblock">
William Peebles and Saining Xie. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html" title="">Scalable Diffusion Models with Transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 4195–4205.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.

</span>
<span class="ltx_bibblock">Language Models are Unsupervised Multitask Learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">OpenAI</em>, page 24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richemond et al. (2023)</span>
<span class="ltx_bibblock">
Pierre Harvey Richemond, Sander Dieleman, and Arnaud Doucet. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=6rETbXxGX5#all" title="">Categorical SDEs with Simplex Diffusion</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">ICML 2023 Workshop: Sampling and Optimization in Discrete Space</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html" title="">High-Resolution Image Synthesis With Latent Diffusion Models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 10684–10695.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schöne et al. (2025)</span>
<span class="ltx_bibblock">
Mark Schöne, Babak Rahmani, Heiner Kremer, Fabian Falck, Hitesh Ballani, and Jannes Gladrow. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2502.07827" title="">Implicit Language Models are RNNs: Balancing Parallelization and Expressivity</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2502.07827[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwarzschild et al. (2021)</span>
<span class="ltx_bibblock">
Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/3501672ebc68a5524629080e3ef60aef-Abstract.html" title="">Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 34, pages 6695–6706. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skean et al. (2024)</span>
<span class="ltx_bibblock">
Oscar Skean, Md Rifat Arefin, Yann LeCun, and Ravid Shwartz-Ziv. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2412.09563" title="">Does Representation Matter? Exploring Intermediate Layers in Large Language Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2412.09563[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song and Ermon (2019)</span>
<span class="ltx_bibblock">
Yang Song and Stefano Ermon. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1907.05600" title="">Generative Modeling by Estimating Gradients of the Data Distribution</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv:1907.05600 [cs, stat]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024)</span>
<span class="ltx_bibblock">
Qi Sun, Marc Pickett, Aakash Kumar Nain, and Llion Jones. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2407.09298" title="">Transformer Layers as Painters</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2407.09298[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutskever et al. (2008)</span>
<span class="ltx_bibblock">
Ilya Sutskever, Geoffrey E Hinton, and Graham W Taylor. 2008.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2008/hash/9ad6aaed513b73148b7d49f70afcfb32-Abstract.html" title="">The Recurrent Temporal Restricted Boltzmann Machine</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 21. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutskever et al. (2011)</span>
<span class="ltx_bibblock">
Ilya Sutskever, James Martens, and Geoffrey Hinton. 2011.

</span>
<span class="ltx_bibblock">Generating text with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 28th International Conference on International Conference on Machine Learning</em>, ICML’11, pages 1017–1024, Madison, WI, USA. Omnipress.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. (2023)</span>
<span class="ltx_bibblock">
Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron Courville, and Chuang Gan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2310.07096" title="">Sparse Universal Transformer</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2310.07096[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, and 49 others. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2307.09288" title="">Llama 2: Open Foundation and Fine-Tuned Chat Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2307.09288[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2025a)</span>
<span class="ltx_bibblock">
Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, and Yasin Abbasi Yadkori. 2025a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2506.21734" title="">Hierarchical Reasoning Model</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2506.21734[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2025b)</span>
<span class="ltx_bibblock">
Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. 2025b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2508.09192" title="">Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2508.09192[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2025)</span>
<span class="ltx_bibblock">
Bohong Wu, Shen Yan, Sijun Zhang, Jianqiao Lu, Yutao Zeng, Ya Wang, and Xun Zhou. 2025.

</span>
<span class="ltx_bibblock">Efficient pretraining length scaling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2504.14992</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2025)</span>
<span class="ltx_bibblock">
Zhihui Xie, Jiacheng Ye, Lin Zheng, Jiahui Gao, Jingwei Dong, Zirui Wu, Xueliang Zhao, Shansan Gong, Xin Jiang, Zhenguo Li, and Lingpeng Kong. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2509.01142" title="">Dream-Coder 7B: An Open Diffusion Language Model for Code</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2509.01142[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024)</span>
<span class="ltx_bibblock">
Liu Yang, Kangwook Lee, Robert D. Nowak, and Dimitris Papailiopoulos. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=HHbRxoDTxE" title="">Looped Transformers are Better at Learning Learning Algorithms</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2025)</span>
<span class="ltx_bibblock">
Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2508.15487" title="">Dream 7B: Diffusion Large Language Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arxiv:2508.15487[cs]</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=N8N0hgNDRt" title="">MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>AdditionaL Algorithm Details</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p">We provide the full algorithm, including adaptive exiting in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#alg2" title="In A.1 AdditionaL Algorithm Details ‣ Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Algorithm</span>˜<span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 2</span> </span> Diffusion-style generation with latent-diference-based freezing</figcaption>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg2.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span>prompt <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="alg2.l1.m1" intent=":literal"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math>, max new tokens <math alttext="N" class="ltx_Math" display="inline" id="alg2.l1.m2" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>, inner recurrence <math alttext="r" class="ltx_Math" display="inline" id="alg2.l1.m3" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>, diffusion steps <math alttext="T" class="ltx_Math" display="inline" id="alg2.l1.m4" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>, init scale <math alttext="\alpha" class="ltx_Math" display="inline" id="alg2.l1.m5" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>, exit threshold <math alttext="\varepsilon" class="ltx_Math" display="inline" id="alg2.l1.m6" intent=":literal"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span><math alttext="\mathbf{y}_{\mathrm{frozen}}\leftarrow\mathbf{x}" class="ltx_Math" display="inline" id="alg2.l2.m1" intent=":literal"><semantics><mrow><msub><mi>𝐲</mi><mi>frozen</mi></msub><mo stretchy="false">←</mo><mi>𝐱</mi></mrow><annotation encoding="application/x-tex">\mathbf{y}_{\mathrm{frozen}}\leftarrow\mathbf{x}</annotation></semantics></math>, <math alttext="\mathbf{y}_{\mathrm{current}}\leftarrow\mathbf{x}" class="ltx_Math" display="inline" id="alg2.l2.m2" intent=":literal"><semantics><mrow><msub><mi>𝐲</mi><mi>current</mi></msub><mo stretchy="false">←</mo><mi>𝐱</mi></mrow><annotation encoding="application/x-tex">\mathbf{y}_{\mathrm{current}}\leftarrow\mathbf{x}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span><math alttext="\mathbf{z}\leftarrow\operatorname{InitState}(|\mathbf{x}|,\alpha)" class="ltx_Math" display="inline" id="alg2.l3.m1" intent=":literal"><semantics><mrow><mi>𝐳</mi><mo stretchy="false">←</mo><mrow><mi>InitState</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">|</mo><mi>𝐱</mi><mo stretchy="false">|</mo></mrow><mo>,</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}\leftarrow\operatorname{InitState}(|\mathbf{x}|,\alpha)</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span><math alttext="\mathbf{z}_{\mathrm{prev}}\leftarrow\mathbf{z}" class="ltx_Math" display="inline" id="alg2.l4.m1" intent=":literal"><semantics><mrow><msub><mi>𝐳</mi><mi>prev</mi></msub><mo stretchy="false">←</mo><mi>𝐳</mi></mrow><annotation encoding="application/x-tex">\mathbf{z}_{\mathrm{prev}}\leftarrow\mathbf{z}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span><span class="ltx_text ltx_font_bold">for</span> step <math alttext="t=1,\dots,T" class="ltx_Math" display="inline" id="alg2.l5.m1" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>T</mi></mrow></mrow><annotation encoding="application/x-tex">t=1,\dots,T</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg2.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>  <math alttext="\mathbf{e}\leftarrow\mathcal{P}(\mathbf{y}_{\mathrm{current}})" class="ltx_Math" display="inline" id="alg2.l6.m1" intent=":literal"><semantics><mrow><mi>𝐞</mi><mo stretchy="false">←</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐲</mi><mi>current</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{e}\leftarrow\mathcal{P}(\mathbf{y}_{\mathrm{current}})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>  <math alttext="\mathbf{z}_{\text{noise}}\sim\mathcal{N}(0,\sigma^{2}I)" class="ltx_Math" display="inline" id="alg2.l7.m1" intent=":literal"><semantics><mrow><msub><mi>𝐳</mi><mtext>noise</mtext></msub><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">​</mo><mi>I</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}_{\text{noise}}\sim\mathcal{N}(0,\sigma^{2}I)</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span>  <math alttext="\mathbf{z}\leftarrow(1-\beta_{r})\mathbf{z}+\beta_{r}\mathbf{z}_{\text{noise}}" class="ltx_Math" display="inline" id="alg2.l8.m1" intent=":literal"><semantics><mrow><mi>𝐳</mi><mo stretchy="false">←</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><msub><mi>β</mi><mi>r</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝐳</mi></mrow><mo>+</mo><mrow><msub><mi>β</mi><mi>r</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝐳</mi><mtext>noise</mtext></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}\leftarrow(1-\beta_{r})\mathbf{z}+\beta_{r}\mathbf{z}_{\text{noise}}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span>  <span class="ltx_text ltx_font_bold">for</span> <math alttext="j=1,\dots,r" class="ltx_Math" display="inline" id="alg2.l9.m1" intent=":literal"><semantics><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>r</mi></mrow></mrow><annotation encoding="application/x-tex">j=1,\dots,r</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg2.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span>   <math alttext="\mathbf{z}\leftarrow\mathcal{R}(\mathbf{z},\mathbf{e})" class="ltx_Math" display="inline" id="alg2.l10.m1" intent=":literal"><semantics><mrow><mi>𝐳</mi><mo stretchy="false">←</mo><mrow><mi class="ltx_font_mathcaligraphic">ℛ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐳</mi><mo>,</mo><mi>𝐞</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}\leftarrow\mathcal{R}(\mathbf{z},\mathbf{e})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l11">
<span class="ltx_tag ltx_tag_listingline">11:</span>  <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg2.l12">
<span class="ltx_tag ltx_tag_listingline">12:</span>  <math alttext="\mathbf{p}\leftarrow\mathcal{C}(\mathbf{z})" class="ltx_Math" display="inline" id="alg2.l12.m1" intent=":literal"><semantics><mrow><mi>𝐩</mi><mo stretchy="false">←</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐳</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{p}\leftarrow\mathcal{C}(\mathbf{z})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l13">
<span class="ltx_tag ltx_tag_listingline">13:</span>  <math alttext="\hat{\mathbf{y}}\leftarrow\operatorname{Sample}(\mathbf{p})" class="ltx_Math" display="inline" id="alg2.l13.m1" intent=":literal"><semantics><mrow><mover accent="true"><mi>𝐲</mi><mo>^</mo></mover><mo stretchy="false">←</mo><mrow><mi>Sample</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mi>𝐩</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{\mathbf{y}}\leftarrow\operatorname{Sample}(\mathbf{p})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l14">
<span class="ltx_tag ltx_tag_listingline">14:</span>  <math alttext="\mathbf{y}_{\mathrm{current}}\leftarrow[\mathbf{y}_{\mathrm{frozen}},\hat{\mathbf{y}}]" class="ltx_Math" display="inline" id="alg2.l14.m1" intent=":literal"><semantics><mrow><msub><mi>𝐲</mi><mi>current</mi></msub><mo stretchy="false">←</mo><mrow><mo stretchy="false">[</mo><msub><mi>𝐲</mi><mi>frozen</mi></msub><mo>,</mo><mover accent="true"><mi>𝐲</mi><mo>^</mo></mover><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{y}_{\mathrm{current}}\leftarrow[\mathbf{y}_{\mathrm{frozen}},\hat{\mathbf{y}}]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l15">
<span class="ltx_tag ltx_tag_listingline">15:</span>  <math alttext="\delta_{i}\leftarrow||\mathbf{z}_{i}-\mathbf{z}_{\mathrm{prev},i}||_{2}/||{\mathbf{z}_{i}||_{2}}" class="ltx_Math" display="inline" id="alg2.l15.m1" intent=":literal"><semantics><mrow><msub><mi>δ</mi><mi>i</mi></msub><mo stretchy="false">←</mo><mrow><msub><mrow><mo stretchy="false">‖</mo><mrow><msub><mi>𝐳</mi><mi>i</mi></msub><mo>−</mo><msub><mi>𝐳</mi><mrow><mi>prev</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub><mo>/</mo><msub><mrow><mo stretchy="false">‖</mo><msub><mi>𝐳</mi><mi>i</mi></msub><mo stretchy="false">‖</mo></mrow><mn>2</mn></msub></mrow></mrow><annotation encoding="application/x-tex">\delta_{i}\leftarrow||\mathbf{z}_{i}-\mathbf{z}_{\mathrm{prev},i}||_{2}/||{\mathbf{z}_{i}||_{2}}</annotation></semantics></math>
<span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l15.m2" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> Compute relative changes in latents at each position.
</span>
</div>
<div class="ltx_listingline" id="alg2.l16">
<span class="ltx_tag ltx_tag_listingline">16:</span>  <span class="ltx_text ltx_font_bold">if</span> exists position <math alttext="i" class="ltx_Math" display="inline" id="alg2.l16.m1" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> with <math alttext="\delta_{i}&lt;\varepsilon" class="ltx_Math" display="inline" id="alg2.l16.m2" intent=":literal"><semantics><mrow><msub><mi>δ</mi><mi>i</mi></msub><mo>&lt;</mo><mi>ε</mi></mrow><annotation encoding="application/x-tex">\delta_{i}&lt;\varepsilon</annotation></semantics></math> <span class="ltx_text ltx_font_bold">then</span>
</div>
<div class="ltx_listingline" id="alg2.l17">
<span class="ltx_tag ltx_tag_listingline">17:</span>   let <math alttext="k^{*}\leftarrow" class="ltx_Math" display="inline" id="alg2.l17.m1" intent=":literal"><semantics><mrow><msup><mi>k</mi><mo>∗</mo></msup><mo stretchy="false">←</mo><mi></mi></mrow><annotation encoding="application/x-tex">k^{*}\leftarrow</annotation></semantics></math> index of the last such freezable position where <math alttext="\delta_{i}&lt;\varepsilon" class="ltx_Math" display="inline" id="alg2.l17.m2" intent=":literal"><semantics><mrow><msub><mi>δ</mi><mi>i</mi></msub><mo>&lt;</mo><mi>ε</mi></mrow><annotation encoding="application/x-tex">\delta_{i}&lt;\varepsilon</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l17.m3" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> freeze up to <math alttext="k^{*}" class="ltx_Math" display="inline" id="alg2.l17.m4" intent=":literal"><semantics><msup><mi>k</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">k^{*}</annotation></semantics></math>
</span>
</div>
<div class="ltx_listingline" id="alg2.l18">
<span class="ltx_tag ltx_tag_listingline">18:</span>   <math alttext="\mathbf{y}_{\mathrm{frozen}}\leftarrow\mathbf{y}_{\mathrm{current}}[1{:}k^{*}]" class="ltx_math_unparsed" display="inline" id="alg2.l18.m1" intent=":literal"><semantics><mrow><msub><mi>𝐲</mi><mi>frozen</mi></msub><mo stretchy="false">←</mo><msub><mi>𝐲</mi><mi>current</mi></msub><mrow><mo stretchy="false">[</mo><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><msup><mi>k</mi><mo>∗</mo></msup><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{y}_{\mathrm{frozen}}\leftarrow\mathbf{y}_{\mathrm{current}}[1{:}k^{*}]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l19">
<span class="ltx_tag ltx_tag_listingline">19:</span>   keep only unfrozen tail of latents: <math alttext="\mathbf{z}\leftarrow\mathbf{z}[k^{*}-\ell{:}]" class="ltx_math_unparsed" display="inline" id="alg2.l19.m1" intent=":literal"><semantics><mrow><mi>𝐳</mi><mo stretchy="false">←</mo><mi>𝐳</mi><mrow><mo stretchy="false">[</mo><msup><mi>k</mi><mo>∗</mo></msup><mo>−</mo><mi mathvariant="normal">ℓ</mi><mo lspace="0.278em" rspace="0em">:</mo><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}\leftarrow\mathbf{z}[k^{*}-\ell{:}]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l20">
<span class="ltx_tag ltx_tag_listingline">20:</span>  <span class="ltx_text ltx_font_bold">else</span>
</div>
<div class="ltx_listingline" id="alg2.l21">
<span class="ltx_tag ltx_tag_listingline">21:</span>   no tokens frozen this step

</div>
<div class="ltx_listingline" id="alg2.l22">
<span class="ltx_tag ltx_tag_listingline">22:</span>  <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">if</span>
</div>
<div class="ltx_listingline" id="alg2.l23">
<span class="ltx_tag ltx_tag_listingline">23:</span>  <span class="ltx_text ltx_font_bold">if</span> <math alttext="|\mathbf{y}_{\mathrm{frozen}}|-|\mathbf{x}|\geq N" class="ltx_Math" display="inline" id="alg2.l23.m1" intent=":literal"><semantics><mrow><mrow><mrow><mo stretchy="false">|</mo><msub><mi>𝐲</mi><mi>frozen</mi></msub><mo stretchy="false">|</mo></mrow><mo>−</mo><mrow><mo stretchy="false">|</mo><mi>𝐱</mi><mo stretchy="false">|</mo></mrow></mrow><mo>≥</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">|\mathbf{y}_{\mathrm{frozen}}|-|\mathbf{x}|\geq N</annotation></semantics></math> <span class="ltx_text ltx_font_bold">then</span> <span class="ltx_text ltx_font_bold">break</span>
</div>
<div class="ltx_listingline" id="alg2.l24">
<span class="ltx_tag ltx_tag_listingline">24:</span>  <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">if</span>
</div>
<div class="ltx_listingline" id="alg2.l25">
<span class="ltx_tag ltx_tag_listingline">25:</span>  <math alttext="\mathbf{z}\leftarrow[\mathbf{z},\,\operatorname{InitState}(1,\alpha)]" class="ltx_Math" display="inline" id="alg2.l25.m1" intent=":literal"><semantics><mrow><mi>𝐳</mi><mo stretchy="false">←</mo><mrow><mo stretchy="false">[</mo><mi>𝐳</mi><mo rspace="0.337em">,</mo><mrow><mi>InitState</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>,</mo><mi>α</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{z}\leftarrow[\mathbf{z},\,\operatorname{InitState}(1,\alpha)]</annotation></semantics></math> <span class="ltx_text" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg2.l25.m2" intent=":literal"><semantics><mo>⊳</mo><annotation encoding="application/x-tex">\triangleright</annotation></semantics></math> Append a new latent state for the next position
</span>
</div>
<div class="ltx_listingline" id="alg2.l26">
<span class="ltx_tag ltx_tag_listingline">26:</span>  <math alttext="\mathbf{z}_{\mathrm{prev}}\leftarrow\mathbf{z}" class="ltx_Math" display="inline" id="alg2.l26.m1" intent=":literal"><semantics><mrow><msub><mi>𝐳</mi><mi>prev</mi></msub><mo stretchy="false">←</mo><mi>𝐳</mi></mrow><annotation encoding="application/x-tex">\mathbf{z}_{\mathrm{prev}}\leftarrow\mathbf{z}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l27">
<span class="ltx_tag ltx_tag_listingline">27:</span><span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg2.l28">
<span class="ltx_tag ltx_tag_listingline">28:</span><span class="ltx_text ltx_font_bold">return</span> <math alttext="\mathbf{y}_{\mathrm{frozen}}" class="ltx_Math" display="inline" id="alg2.l28.m1" intent=":literal"><semantics><msub><mi>𝐲</mi><mi>frozen</mi></msub><annotation encoding="application/x-tex">\mathbf{y}_{\mathrm{frozen}}</annotation></semantics></math>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Additional Variants</h3>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Larger Batch Sizes.</h5>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS0.Px1.p1">
<p class="ltx_p">The sampler discussed in this work could, in principle, also be deployed in batched or continuously-batched inference settings. In that scenario, similar to a paged KV cache, the sampler would reserve a number of slots for hidden states up to an occupancy multiplier of the maximum wavefront size, and would be capable of scheduling recurrent updates in tandem with sequence updates. For larger models, this would, if implemented efficiently, actually simplify deployment, as recurrent states are fungible, and e.g. states could be evicted from one device, and then bundled into the next forward call of the model on a different device, as the slots of the model’s hidden states do not have to correspond to contiguous sequences in either the sequence or the recurrence dimension. However, due to to the imminent complexity of such an inference engine, we refrained from engaging with this direction in this work, and focus only on properly bringing the general idea of diffusion sampling to recurrent-depth models, and leave a batched inference engine as a limitation, potentially motivating future work.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Additional Information.</h3>
<section class="ltx_paragraph" id="A1.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Finetuned Math Model:</h5>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS0.Px1.p1">
<p class="ltx_p">To verify that our findings are not limited to the particular model checkpoint we evaluate, and its capabilities, we finetune the original checkpoint for one epoch with a trapezoidal learning rate schedule with a peak learning rate of <math alttext="5\times 10^{-7}" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m1" intent=":literal"><semantics><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>7</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\times 10^{-7}</annotation></semantics></math> using the MetaMath dataset <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib74" title="">2023</a>)</cite>. As suggested in the original work, we train the model with randomized unrolling, we set a mean of <math alttext="r=32" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m2" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">r=32</annotation></semantics></math> and sample <math alttext="r" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px1.p1.m3" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> from an Exponential distribution. As a sidenote, we remark that while we do train the full model, most of the gains can also be achieved by just finetuning the adapter component of the model that maps inputs and states into the recurrent block.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Dataset Details.</h5>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS0.Px2.p1">
<p class="ltx_p">When evaluating GSM8k, we always refer to the CoT version of the dataset, which we provide to the model with the 8 few-shot examples associated with this variant as in <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib67" title="">2023</a>)</cite>. We always score GSM8k using the flexible-extract metric, i.e. by matching the last number in the model response against the reference answer. For MATH500, we follow the format of <cite class="ltx_cite ltx_citemacro_citet">DeepSeek-AI et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib19" title="">2025</a>)</cite>, while for Minerva Math, we follow the updated format established in the lm-eval harness. For both, we grade answers using <span class="ltx_text ltx_font_italic">math-verify</span>. For MBPP and HumanEval, we grade these benchmarks as normal. During inference we sample with a temperature of 0.2 and top-p factor of <math alttext="0.95" class="ltx_Math" display="inline" id="A1.SS3.SSS0.Px2.p1.m1" intent=":literal"><semantics><mn>0.95</mn><annotation encoding="application/x-tex">0.95</annotation></semantics></math> as in <cite class="ltx_cite ltx_citemacro_citet">Geiping et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#bib.bib25" title="">2025</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="A1.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="195" id="A1.F9.g1" src="x14.png" width="407"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="196" id="A1.F9.g2" src="x15.png" width="407"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Impact of Additional Hyperparameter Choices, also on GSM8k. <span class="ltx_text ltx_font_bold">Left</span> Initialization Scale of new states, which has only a minor effect of the result. <span class="ltx_text ltx_font_bold">Right:</span> Continuous Compute, i.e. choosing to initialize new states with previously computed states (We initialize new states with the latest state from the position one step to the left). This is less effective for our sampler, given that the position one step to the left is only the result of <math alttext="r^{\prime}" class="ltx_Math" display="inline" id="A1.F9.m2" intent=":literal"><semantics><msup><mi>r</mi><mo>′</mo></msup><annotation encoding="application/x-tex">r^{\prime}</annotation></semantics></math> recurrences.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="648" id="A1.F10.g1" src="x16.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>A heatmap of accuracy and throughput measurements spanned by varying noise and inner recurrence.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="548" id="A1.F11.g1" src="x17.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Additional visualizations of the trade-off of noise and inner recurrence in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S5.F7" title="In Moving Forward Multiple Steps. ‣ 5.2 Variants and Hyperparameters ‣ 5 Experimental Evaluation ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">7</span></a>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Qualitative Evaluation</h3>
<div class="ltx_para ltx_noindent" id="A1.SS4.p1">
<p class="ltx_p">To visualize the progress (or temporary lack thereof) of the sampler on a challenging sequence from the GSM8k validation set, we provide a few additional visualizations in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#A1.F12" title="In A.4 Qualitative Evaluation ‣ Appendix A Appendix ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="251" id="A1.F12.g1" src="plots/modeling01_token_stability_heatmaps.png" width="494"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>A full example of a sampler hyperparameter failure. As in <a class="ltx_ref" href="https://arxiv.org/html/2510.14961v1#S3.F4" title="In 3.5 Adaptive Exits ‣ 3 Applying Diffusion Forcing to Recurrent-Depth Models ‣ Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models"><span class="ltx_text ltx_ref_tag">Figure</span>˜<span class="ltx_text ltx_ref_tag">4</span></a>, this figure shows the token ids on the left, as they change during successive steps of the sampler (running from top to bottom) over the sequence dimension (running left to right). We see that the model tries various configurations for the current tokens, before they are gradually frozen as their latent states converge. Due to a few hard decisions (from the perspective of the model), as seen on the stability charts on the right, early in the sequence, progress stalls until these tokens are decided, but then picks up speed again. However, large points of the wavefront all decode into the whitespace token (dark blue color), so that no useful states information is computed until the earlier tokens are resolved.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="194" id="A1.F13.g1" src="x18.png" width="406"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="192" id="A1.F13.g2" src="x19.png" width="406"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="194" id="A1.F13.g3" src="x20.png" width="406"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="193" id="A1.F13.g4" src="x21.png" width="406"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="193" id="A1.F13.g5" src="x22.png" width="406"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Hyperparameter Robustness for the finetuned math model on GSM8k. These figure repeat the ablation study from the main body concerning hyperparameter robustness also for the finetuned math model, showing that behaviors are largely similar, even though the model’s capability has noticeably changed.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Theoretical Analysis</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Problem Formulations</h3>
<div class="ltx_theorem ltx_theorem_definition" id="A2.Thmtheorem1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition B.1</span></span><span class="ltx_text ltx_font_bold"> </span>(Depth and Width in Recurrent-Depth Models)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para ltx_noindent" id="A2.Thmtheorem1.p1">
<p class="ltx_p">Consider a recurrent-depth model <math alttext="\mathcal{M}_{d}" class="ltx_Math" display="inline" id="A2.Thmtheorem1.p1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>d</mi></msub><annotation encoding="application/x-tex">\mathcal{M}_{d}</annotation></semantics></math> that processes an input sequence <math alttext="\mathbf{x}\in\mathbb{R}^{L_{0}\times h}" class="ltx_Math" display="inline" id="A2.Thmtheorem1.p1.m2" intent=":literal"><semantics><mrow><mi>𝐱</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>L</mi><mn>0</mn></msub><mo lspace="0.222em" rspace="0.222em">×</mo><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{x}\in\mathbb{R}^{L_{0}\times h}</annotation></semantics></math>, where <math alttext="L_{0}\in\mathbb{N}" class="ltx_Math" display="inline" id="A2.Thmtheorem1.p1.m3" intent=":literal"><semantics><mrow><msub><mi>L</mi><mn>0</mn></msub><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">L_{0}\in\mathbb{N}</annotation></semantics></math> is the sequence length and <math alttext="h\in\mathbb{N}" class="ltx_Math" display="inline" id="A2.Thmtheorem1.p1.m4" intent=":literal"><semantics><mrow><mi>h</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">h\in\mathbb{N}</annotation></semantics></math> is the hidden dimension. At each generation step <math alttext="t\in\mathbb{N}" class="ltx_Math" display="inline" id="A2.Thmtheorem1.p1.m5" intent=":literal"><semantics><mrow><mi>t</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">t\in\mathbb{N}</annotation></semantics></math>, we define a <em class="ltx_emph ltx_font_italic">hidden state</em> as the <math alttext="h" class="ltx_Math" display="inline" id="A2.Thmtheorem1.p1.m6" intent=":literal"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>-dimensional output vector produced by a Transformer block for an input token. Let <math alttext="\mathbf{H}_{t}\in\mathbb{R}^{w_{t}\times h}" class="ltx_Math" display="inline" id="A2.Thmtheorem1.p1.m7" intent=":literal"><semantics><mrow><msub><mi>𝐇</mi><mi>t</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>w</mi><mi>t</mi></msub><mo lspace="0.222em" rspace="0.222em">×</mo><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{H}_{t}\in\mathbb{R}^{w_{t}\times h}</annotation></semantics></math> denote the 2D-matrix containing all hidden states at step <math alttext="t" class="ltx_Math" display="inline" id="A2.Thmtheorem1.p1.m8" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>. We define the following two associated quantities:</p>
</div>
<div class="ltx_para" id="A2.Thmtheorem1.p2">
<ul class="ltx_itemize" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p">the <em class="ltx_emph ltx_font_italic">depth</em> <math alttext="d_{t}\in\mathbb{N}" class="ltx_Math" display="inline" id="A2.I1.i1.p1.m1" intent=":literal"><semantics><mrow><msub><mi>d</mi><mi>t</mi></msub><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">d_{t}\in\mathbb{N}</annotation></semantics></math>, defined as the number of <em class="ltx_emph ltx_font_italic">serial</em>
Transformer block forward passes used to obtain <math alttext="\mathbf{H}_{t}" class="ltx_Math" display="inline" id="A2.I1.i1.p1.m2" intent=":literal"><semantics><msub><mi>𝐇</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{H}_{t}</annotation></semantics></math> from the initial <math alttext="L_{0}" class="ltx_Math" display="inline" id="A2.I1.i1.p1.m3" intent=":literal"><semantics><msub><mi>L</mi><mn>0</mn></msub><annotation encoding="application/x-tex">L_{0}</annotation></semantics></math> input tokens
(i.e., the generation step), while ignoring any discretization;</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A2.I1.i2.p1">
<p class="ltx_p">the <em class="ltx_emph ltx_font_italic">width</em> <math alttext="w_{t}\in\mathbb{N}" class="ltx_Math" display="inline" id="A2.I1.i2.p1.m1" intent=":literal"><semantics><mrow><msub><mi>w</mi><mi>t</mi></msub><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">w_{t}\in\mathbb{N}</annotation></semantics></math>, defined as the cardinality of the active hidden-state set
<math alttext="\mathbf{H}_{t}" class="ltx_Math" display="inline" id="A2.I1.i2.p1.m2" intent=":literal"><semantics><msub><mi>𝐇</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{H}_{t}</annotation></semantics></math> ( i.e., the number of <math alttext="h" class="ltx_Math" display="inline" id="A2.I1.i2.p1.m3" intent=":literal"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math>-dimensional hidden states that are processed in <em class="ltx_emph ltx_font_italic">parallel</em>
at generation step <math alttext="t" class="ltx_Math" display="inline" id="A2.I1.i2.p1.m4" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>).</p>
</div>
</li>
</ul>
<p class="ltx_p">These quantities evolve according to the following rules:</p>
<ol class="ltx_enumerate" id="A2.I2">
<li class="ltx_item" id="A2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A2.I2.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Initialization.</span> At time <math alttext="t=0" class="ltx_Math" display="inline" id="A2.I2.i1.p1.m1" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation></semantics></math>, we set</p>
<table class="ltx_equation ltx_eqn_table" id="A2.Ex8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d_{0}=0,\qquad w_{0}=L_{0}." class="ltx_Math" display="block" id="A2.Ex8.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>d</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><mo rspace="2.167em">,</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><msub><mi>L</mi><mn>0</mn></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">d_{0}=0,\qquad w_{0}=L_{0}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="A2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A2.I2.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Depth update.</span> At each step <math alttext="t\geq 0" class="ltx_Math" display="inline" id="A2.I2.i2.p1.m1" intent=":literal"><semantics><mrow><mi>t</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t\geq 0</annotation></semantics></math>, one additional Transformer block is applied, hence</p>
<table class="ltx_equation ltx_eqn_table" id="A2.Ex9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d_{t+1}=d_{t}+1," class="ltx_Math" display="block" id="A2.Ex9.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>d</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><msub><mi>d</mi><mi>t</mi></msub><mo>+</mo><mn>1</mn></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">d_{t+1}=d_{t}+1,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">so that <math alttext="d_{t}=t" class="ltx_Math" display="inline" id="A2.I2.i2.p1.m2" intent=":literal"><semantics><mrow><msub><mi>d</mi><mi>t</mi></msub><mo>=</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">d_{t}=t</annotation></semantics></math> for all <math alttext="t\in\mathbb{N}" class="ltx_Math" display="inline" id="A2.I2.i2.p1.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">t\in\mathbb{N}</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="A2.I2.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Width update.</span>
At each step <math alttext="t\geq 0" class="ltx_Math" display="inline" id="A2.I2.i3.p1.m1" intent=":literal"><semantics><mrow><mi>t</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t\geq 0</annotation></semantics></math>, the width changes only due to two types of events:</p>
<ul class="ltx_itemize" id="A2.I2.i3.I1">
<li class="ltx_item" id="A2.I2.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i3.I1.i1.p1">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">Token entry:</em> let <math alttext="e^{(t)}\in\mathbb{N}_{0}" class="ltx_Math" display="inline" id="A2.I2.i3.I1.i1.p1.m1" intent=":literal"><semantics><mrow><msup><mi>e</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><msub><mi>ℕ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">e^{(t)}\in\mathbb{N}_{0}</annotation></semantics></math> denote the number of new tokens encoded
into the model at step <math alttext="t" class="ltx_Math" display="inline" id="A2.I2.i3.I1.i1.p1.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, each contributing a new hidden state;</p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A2.I2.i3.I1.i2.p1">
<p class="ltx_p"><em class="ltx_emph ltx_font_italic">Hidden-state exit:</em> let <math alttext="x^{(t)}\in\mathbb{N}_{0}" class="ltx_Math" display="inline" id="A2.I2.i3.I1.i2.p1.m1" intent=":literal"><semantics><mrow><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><msub><mi>ℕ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x^{(t)}\in\mathbb{N}_{0}</annotation></semantics></math> denote the number of hidden states
removed from the model at step <math alttext="t" class="ltx_Math" display="inline" id="A2.I2.i3.I1.i2.p1.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> due to decoding.</p>
</div>
</li>
</ul>
<p class="ltx_p">Then the width evolves as</p>
<table class="ltx_equation ltx_eqn_table" id="A2.Ex10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="w_{t+1}=w_{t}+e^{(t)}-x^{(t)}." class="ltx_Math" display="block" id="A2.Ex10.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><msub><mi>w</mi><mi>t</mi></msub><mo>+</mo><msup><mi>e</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>−</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">w_{t+1}=w_{t}+e^{(t)}-x^{(t)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Equivalently, the net change can be written as <math alttext="\delta^{(t)}=e^{(t)}-x^{(t)}" class="ltx_Math" display="inline" id="A2.I2.i3.p1.m2" intent=":literal"><semantics><mrow><msup><mi>δ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mrow><msup><mi>e</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">\delta^{(t)}=e^{(t)}-x^{(t)}</annotation></semantics></math>,
so that <math alttext="\delta^{(t)}&gt;0" class="ltx_Math" display="inline" id="A2.I2.i3.p1.m3" intent=":literal"><semantics><mrow><msup><mi>δ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta^{(t)}&gt;0</annotation></semantics></math> corresponds to entries (more tokens encoded),
and <math alttext="\delta^{(t)}&lt;0" class="ltx_Math" display="inline" id="A2.I2.i3.p1.m4" intent=":literal"><semantics><mrow><msup><mi>δ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>&lt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta^{(t)}&lt;0</annotation></semantics></math> corresponds to exits (more hidden states decoded).</p>
</div>
</li>
</ol>
</div>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="A2.Thmtheorem2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark B.2</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="A2.Thmtheorem2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">At any generation step <math alttext="t" class="ltx_Math" display="inline" id="A2.Thmtheorem2.p1.m1" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, all hidden states in <math alttext="H_{t}" class="ltx_Math" display="inline" id="A2.Thmtheorem2.p1.m2" intent=":literal"><semantics><msub><mi>H</mi><mi>t</mi></msub><annotation encoding="application/x-tex">H_{t}</annotation></semantics></math> share the same depth <math alttext="d_{t}" class="ltx_Math" display="inline" id="A2.Thmtheorem2.p1.m3" intent=":literal"><semantics><msub><mi>d</mi><mi>t</mi></msub><annotation encoding="application/x-tex">d_{t}</annotation></semantics></math>,
since each step corresponds to one additional serial forward pass through the Transformer block.</span></p>
</div>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>LLMs should prioritize depth scaling during prefilling.</h3>
<div class="ltx_theorem ltx_theorem_definition" id="A2.Thmtheorem3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition B.3</span></span><span class="ltx_text ltx_font_bold"> </span>(Width Scaling Variants)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para ltx_noindent" id="A2.Thmtheorem3.p1">
<p class="ltx_p">Fix a width scaling factor <math alttext="s\in\mathbb{N}" class="ltx_Math" display="inline" id="A2.Thmtheorem3.p1.m1" intent=":literal"><semantics><mrow><mi>s</mi><mo>∈</mo><mi>ℕ</mi></mrow><annotation encoding="application/x-tex">s\in\mathbb{N}</annotation></semantics></math>.
Given an input sequence of length <math alttext="L" class="ltx_Math" display="inline" id="A2.Thmtheorem3.p1.m2" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>, for each token <math alttext="i\in\{1,\dots,L\}" class="ltx_Math" display="inline" id="A2.Thmtheorem3.p1.m3" intent=":literal"><semantics><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>L</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">i\in\{1,\dots,L\}</annotation></semantics></math>
we create <math alttext="s" class="ltx_Math" display="inline" id="A2.Thmtheorem3.p1.m4" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> copies indexed by <math alttext="j\in\{1,\dots,s\}" class="ltx_Math" display="inline" id="A2.Thmtheorem3.p1.m5" intent=":literal"><semantics><mrow><mi>j</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>s</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">j\in\{1,\dots,s\}</annotation></semantics></math>.
The replicated sequence therefore has length <math alttext="L\cdot s" class="ltx_Math" display="inline" id="A2.Thmtheorem3.p1.m6" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>s</mi></mrow><annotation encoding="application/x-tex">L\cdot s</annotation></semantics></math>, with elements denoted by <math alttext="(i,j)" class="ltx_Math" display="inline" id="A2.Thmtheorem3.p1.m7" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math>,
the <math alttext="j" class="ltx_Math" display="inline" id="A2.Thmtheorem3.p1.m8" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th copy of token <math alttext="i" class="ltx_Math" display="inline" id="A2.Thmtheorem3.p1.m9" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.
The width-scaling model is obtained by applying a Transformer block
(with parameters unchanged) to this replicated sequence under a customized attention mask,
followed by a reduction step that maps the <math alttext="L\cdot s" class="ltx_Math" display="inline" id="A2.Thmtheorem3.p1.m10" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mi>s</mi></mrow><annotation encoding="application/x-tex">L\cdot s</annotation></semantics></math> outputs back to length <math alttext="L" class="ltx_Math" display="inline" id="A2.Thmtheorem3.p1.m11" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>
(e.g., by selecting the last copy or averaging over copies).</p>
</div>
<div class="ltx_para" id="A2.Thmtheorem3.p2">
<p class="ltx_p">We define two variants according to how each copy may attend:</p>
<ul class="ltx_itemize" id="A2.I3">
<li class="ltx_item" id="A2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A2.I3.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Width-NoShare.</span> The <math alttext="j" class="ltx_Math" display="inline" id="A2.I3.i1.p1.m1" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th copy of token <math alttext="i" class="ltx_Math" display="inline" id="A2.I3.i1.p1.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> may attend to all copies of tokens <math alttext="0,\dots,i-1" class="ltx_Math" display="inline" id="A2.I3.i1.p1.m3" intent=":literal"><semantics><mrow><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">0,\dots,i-1</annotation></semantics></math>,
as well as the first <math alttext="j-1" class="ltx_Math" display="inline" id="A2.I3.i1.p1.m4" intent=":literal"><semantics><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">j-1</annotation></semantics></math> copies of token <math alttext="i" class="ltx_Math" display="inline" id="A2.I3.i1.p1.m5" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="A2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A2.I3.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Width-KVShare.</span> The <math alttext="j" class="ltx_Math" display="inline" id="A2.I3.i2.p1.m1" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th copy of token <math alttext="i" class="ltx_Math" display="inline" id="A2.I3.i2.p1.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> may attend only to the last copy of tokens <math alttext="0,\dots,i-1" class="ltx_Math" display="inline" id="A2.I3.i2.p1.m3" intent=":literal"><semantics><mrow><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">0,\dots,i-1</annotation></semantics></math>,
together with the first <math alttext="j-1" class="ltx_Math" display="inline" id="A2.I3.i2.p1.m4" intent=":literal"><semantics><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">j-1</annotation></semantics></math> copies of token <math alttext="i" class="ltx_Math" display="inline" id="A2.I3.i2.p1.m5" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
</div>
<div class="ltx_theorem ltx_theorem_proposition" id="A2.Thmtheorem4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Proposition B.4</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="A2.Thmtheorem4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">During prefilling, both <span class="ltx_text ltx_font_sansserif">Width-NoShare</span> and <span class="ltx_text ltx_font_sansserif">Width-KVShare</span> are valid
width-scaling architectures with factor <math alttext="s" class="ltx_Math" display="inline" id="A2.Thmtheorem4.p1.m1" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para ltx_noindent" id="A2.SS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Depth.</span>
At any generation step, each variant performs exactly one Transformer block forward pass
on the replicated sequence.
Therefore the number of serial block forward passes needed to produce the hidden states is unchanged,
so the depth satisfies <math alttext="\tilde{d}_{t}=d_{t}" class="ltx_Math" display="inline" id="A2.SS2.p1.m1" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>d</mi><mo>~</mo></mover><mi>t</mi></msub><mo>=</mo><msub><mi>d</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\tilde{d}_{t}=d_{t}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Width.</span>
By definition, the width <math alttext="w_{t}" class="ltx_Math" display="inline" id="A2.SS2.p2.m1" intent=":literal"><semantics><msub><mi>w</mi><mi>t</mi></msub><annotation encoding="application/x-tex">w_{t}</annotation></semantics></math> is the number of hidden states produced in parallel at step <math alttext="t" class="ltx_Math" display="inline" id="A2.SS2.p2.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>.
In the original model, prefilling a sequence of length <math alttext="L" class="ltx_Math" display="inline" id="A2.SS2.p2.m3" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> produces <math alttext="L" class="ltx_Math" display="inline" id="A2.SS2.p2.m4" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> hidden states per step.
In both variants, we replicate each token <math alttext="s" class="ltx_Math" display="inline" id="A2.SS2.p2.m5" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> times, so the block computes hidden states for
all pairs <math alttext="(i,j)" class="ltx_Math" display="inline" id="A2.SS2.p2.m6" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math> with <math alttext="i\in\{1,\dots,L\}" class="ltx_Math" display="inline" id="A2.SS2.p2.m7" intent=":literal"><semantics><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>L</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">i\in\{1,\dots,L\}</annotation></semantics></math> and <math alttext="j\in\{1,\dots,s\}" class="ltx_Math" display="inline" id="A2.SS2.p2.m8" intent=":literal"><semantics><mrow><mi>j</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>s</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">j\in\{1,\dots,s\}</annotation></semantics></math>.
Hence the total number of hidden states produced in that step is</p>
<table class="ltx_equation ltx_eqn_table" id="A2.Ex11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{w}_{t}=Ls=s\cdot w_{t}." class="ltx_Math" display="block" id="A2.Ex11.m1" intent=":literal"><semantics><mrow><mrow><msub><mover accent="true"><mi>w</mi><mo>~</mo></mover><mi>t</mi></msub><mo>=</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow><mo>=</mo><mrow><mi>s</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>w</mi><mi>t</mi></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\tilde{w}_{t}=Ls=s\cdot w_{t}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">The difference between <span class="ltx_text ltx_font_sansserif">NoShare</span> and <span class="ltx_text ltx_font_sansserif">KVShare</span> lies only in the
attention pattern (which copies each query may attend to).
This affects information flow but not the number of hidden states computed.
The optional reduction back to length <math alttext="L" class="ltx_Math" display="inline" id="A2.SS2.p2.m9" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> occurs <em class="ltx_emph ltx_font_italic">after</em> the parallel computation
and thus does not change the measured width.</p>
</div>
<div class="ltx_para" id="A2.SS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Conclusion.</span>
Both variants keep serial depth fixed and enlarge width by a factor of <math alttext="s" class="ltx_Math" display="inline" id="A2.SS2.p3.m1" intent=":literal"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>,
which is precisely our notion of width scaling.
∎</p>
</div>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 16 17:53:59 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
