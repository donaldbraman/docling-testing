<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents</title>
<!--Generated on Thu Oct 16 17:56:09 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2510.14967v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S1" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S2" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Preliminaries</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S2.SS1" title="In 2 Preliminaries ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Task Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S2.SS2" title="In 2 Preliminaries ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Agentic Reinforcement Learning Pipeline</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S3" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Information Gain-based Policy Optimization</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S3.SS1" title="In 3 Information Gain-based Policy Optimization ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S3.SS2" title="In 3 Information Gain-based Policy Optimization ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Information Gain-based Turn-level Reward</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S3.SS3" title="In 3 Information Gain-based Policy Optimization ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Policy Optimization with Turn-level Advantage</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S4" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S4.SS1" title="In 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S4.SS2" title="In 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Overall Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S4.SS3" title="In 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S4.SS4" title="In 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>In-Depth Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S4.SS4.SSS0.Px1" title="In 4.4 In-Depth Analysis ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title">Ground-truth Entropy Reduction.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S4.SS4.SSS0.Px2" title="In 4.4 In-Depth Analysis ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title">Token Efficiency.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S5" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S6" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion, Limitations and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A1" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Theoretical Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A2" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Proof for Theoretical Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A2.SS1" title="In Appendix B Proof for Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Proof of Lemma <span class="ltx_text ltx_ref_tag">A.2</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A2.SS2" title="In Appendix B Proof for Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Proof of Theorem <span class="ltx_text ltx_ref_tag">A.4</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A3" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>More Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A4" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>More Discussion and
Experimental Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A4.SS1" title="In Appendix D More Discussion and Experimental Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Comparison with Other Process-reward Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A4.SS2" title="In Appendix D More Discussion and Experimental Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2 </span>Case Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A5" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Comparison between GRPO and IGPO</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A6" title="In Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Prompt template used in our experiments.</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined">\useunder</span>
<p class="ltx_p"><span class="ltx_text ltx_ulem_uline"></span><span class="ltx_ERROR undefined">\ul</span>
</p>
</div>
<h1 class="ltx_title ltx_title_document">Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guoqing Wang<sup class="ltx_sup">1*</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Sunhao Dai<sup class="ltx_sup">2*</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Guangze Ye<sup class="ltx_sup">3*</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zeyu Gan<sup class="ltx_sup">2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Wei Yao<sup class="ltx_sup">2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yong Deng<sup class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xiaofeng Wu<sup class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
and Zhenzhe Ying<sup class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy’s probability of producing the correct answer.
Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model’s own belief updates.
These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.</p>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p ltx_align_center"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/GuoqingWang1/IGPO" title="">GitHub: https://github.com/GuoqingWang1/IGPO</a></p>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>Equal contribution.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p">Large language model (LLM)–based agents are increasingly endowed with the ability to interact with external environments through tool use <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib43" title="">2025a</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib14" title="">2025</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib23" title="">2025c</a>)</cite>, a capability often regarded as a critical step toward building general-purpose autonomous intelligent systems <cite class="ltx_cite ltx_citemacro_citep">(Gutierrez et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib11" title="">2023</a>; Qu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib28" title="">2025</a>)</cite>. For example, web search <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib44" title="">2025b</a>; Qi et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib27" title="">2024</a>)</cite>, one of the most fundamental tools, enables agents to access up-to-date large-scale knowledge that substantially improves their capacity to solve complex, knowledge-intensive tasks <cite class="ltx_cite ltx_citemacro_citep">(Ning et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib25" title="">2025</a>)</cite>. Through iterative interaction with the external environment, agents can gradually acquire missing information and refine their reasoning toward solving the target query.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p">To equip general-purpose LLMs with such agentic capabilities, early efforts primarily relied on prompt-based workflows <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib22" title="">2025b</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib37" title="">2024a</a>; Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib47" title="">2024</a>)</cite>, which allowed tool use without additional training but often suffered from poor generalization. More recent studies have explored supervised fine-tuning (SFT) <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib39" title="">2024b</a>)</cite> and reinforcement learning (RL) <cite class="ltx_cite ltx_citemacro_citep">(Jin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib17" title="">2025</a>; Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib33" title="">2025a</a>; Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib48" title="">2025b</a>)</cite> to explicitly incentivize tool use, achieving markedly better performance. In particular, Group Relative Policy Optimization (GRPO) <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib31" title="">2024</a>)</cite>–style methods have emerged as the dominant approach for training agentic LLMs. In this paradigm, a group of rollouts is generated for each query under the current policy, and outcome-based rewards, typically defined by the correctness of the final answer against the ground truth, are used to construct group-relative advantages that drive policy optimization.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">Despite their simplicity and effectiveness on relatively easy tasks, outcome rewards suffer from an inherent limitation: they are <span class="ltx_text ltx_font_bold">sparse</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib45" title="">2025c</a>)</cite>, since supervision is provided only at the final answer. This sparsity becomes particularly detrimental in multi-turn agentic settings, where long trajectories exacerbate the problem in two ways.
<span class="ltx_text ltx_font_bold">First</span>, sparse rewards frequently lead to <em class="ltx_emph ltx_font_italic">advantage collapse</em>: when sampled rollouts yield the same answer (e.g., all wrong or all right),</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="165" id="S1.F1.g1" src="x1.png" width="304"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Proportion of zero-advantage groups during training—IGPO vs. GRPO on Qwen2.5-7B/3B-Instruct.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p">all rollouts in the group receive identical outcome rewards, yielding zero group-relative advantages.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, a substantial portion of training iterations suffer from this issue, especially for smaller models, which struggle more with complex queries. <span class="ltx_text ltx_font_bold">Second</span>, outcome-only supervision fails to provide fine-grained credit assignment. In multi-turn scenarios, later turns are tightly dependent on earlier ones: a reasoning or tool call of the current turn may be correct but rendered useless by prior mistakes, or conversely, early successes may be negated by subsequent errors. Such dependencies are easily obscured under outcome-only rewards, particularly in multi-hop tasks that require long-horizon reasoning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p">Several recent approaches have attempted to mitigate these issues by introducing process-level rewards. One line of work leverages external oracle knowledge or reward models to judge intermediate steps <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib38" title="">2025</a>; Feng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib7" title="">2025</a>)</cite>, but this strategy is costly to obtain and risks introducing additional bias. Another line relies on Monte Carlo simulations to estimate step values <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib36" title="">2023</a>; Zuo et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib49" title="">2025</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib45" title="">2025c</a>)</cite>, yet these methods suffer from high variance unless a large number of samples are collected. Overall, both directions face challenges in scalability and fail to provide simple and stable supervision, underscoring the need for an intrinsic and reliable process-level reward design.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p">To address these challenges, we propose Information-Gain-based Policy Optimization (IGPO), a simple but effective reinforcement learning framework that provides stable and intrinsic supervision for multi-turn agent training. The key intuition is to model each agent–environment interaction turn as an incremental process of acquiring information about the ground truth. Specifically, at every turn, IGPO computes the policy’s probability of producing the correct answer and defines the turn-level reward as the marginal increase in this probability compared to the previous state. This information gain reward offers ground-truth-aware feedback at every turn, in contrast to outcome rewards that only supervise the final answer. While turn-level rewards ensure dense and stable supervision, the outcome reward remains essential to anchor training to the final task objective. To combine these strengths, IGPO also integrates the outcome reward with the sequence of turn-level rewards, forming a dense reward trajectory for each rollout. To further stabilize training, we normalize rewards within groups and propagate them with discounted accumulation, enabling turn-level advantage estimation that captures long-horizon dependencies. Finally, IGPO optimizes the policy with a GRPO-style surrogate objective, replacing rollout-level advantages with our turn-level ones.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p">To evaluate the effectiveness of IGPO, we conduct extensive experiments on both in-domain and out-of-domain benchmarks with search-based agents. Results show that IGPO consistently outperforms strong baselines, delivering substantial gains in both answer accuracy and sample efficiency.
Our main contributions can be summarized as follows:
(1) We analyze the phenomenon of advantage collapse in outcome-reward–based optimization, and reveal the inefficiency of existing process-level rewards due to reliance on external knowledge or high-variance estimation.
(2) We propose IGPO, a simple yet effective policy optimization framework that leverages turn-level information gain to provide dense, ground-truth-aware supervision while preserving outcome-level alignment. (3) Comprehensive experiments demonstrate that IGPO outperforms strong baselines across multiple benchmarks and significantly improves sample efficiency, especially for smaller models.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminaries</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p">In this section, we present the standard multi-turn agentic RL pipeline, illustrated with a search agent as a representative example.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Task Formulation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p">Let <math alttext="\mathcal{D}=\{(q,a)\}" class="ltx_Math" display="inline" id="S2.SS1.p1.m1" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><mi>q</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}=\{(q,a)\}</annotation></semantics></math> denote a dataset of question–answer pairs, and let <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S2.SS1.p1.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation></semantics></math> represent an external tool (e.g., a web search engine). The goal of the agent is to solve question <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.p1.m3" intent=":literal"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math> by generating a rollout
<math alttext="o=(\tau_{1},\tau_{2},\ldots,\tau_{T})" class="ltx_Math" display="inline" id="S2.SS1.p1.m4" intent=":literal"><semantics><mrow><mi>o</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>τ</mi><mn>1</mn></msub><mo>,</mo><msub><mi>τ</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>τ</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">o=(\tau_{1},\tau_{2},\ldots,\tau_{T})</annotation></semantics></math> through iterative interaction with the environment via tool <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S2.SS1.p1.m5" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation></semantics></math>, where <math alttext="T" class="ltx_Math" display="inline" id="S2.SS1.p1.m6" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is the total number of interaction turns. The last turn <math alttext="\tau_{T}" class="ltx_Math" display="inline" id="S2.SS1.p1.m7" intent=":literal"><semantics><msub><mi>τ</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\tau_{T}</annotation></semantics></math> is the <span class="ltx_text ltx_font_italic">answer turn</span> that outputs a rationale-then-final answer sequence, while all previous turns involve reasoning and tool interaction.
Specifically, for <math alttext="t&lt;T" class="ltx_Math" display="inline" id="S2.SS1.p1.m8" intent=":literal"><semantics><mrow><mi>t</mi><mo>&lt;</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">t&lt;T</annotation></semantics></math>, each turn <math alttext="\tau_{t}" class="ltx_Math" display="inline" id="S2.SS1.p1.m9" intent=":literal"><semantics><msub><mi>τ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\tau_{t}</annotation></semantics></math> is defined as a triple consisting of <span class="ltx_text ltx_font_typewriter">[think]</span>, <span class="ltx_text ltx_font_typewriter">[tool call]</span>, and <span class="ltx_text ltx_font_typewriter">[tool response]</span>.
The <span class="ltx_text ltx_font_typewriter">[think]</span> step compels the agent to reason explicitly before acting, and each reasoning process is wrapped in a <span class="ltx_text ltx_font_typewriter">&lt;think&gt;&lt;/think&gt;</span> tag following the DeepSeek-R1 setting <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib10" title="">2025</a>)</cite>.
The <span class="ltx_text ltx_font_typewriter">[tool call]</span> step invokes the external tool <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S2.SS1.p1.m10" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation></semantics></math> by producing a structured request, typically JSON-formatted and wrapped in a dedicated tag (e.g., <span class="ltx_text ltx_font_typewriter">&lt;search&gt;search query&lt;/search&gt;</span> for web search).
The <span class="ltx_text ltx_font_typewriter">[tool response]</span> step then returns structured outputs from <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S2.SS1.p1.m11" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℰ</mi><annotation encoding="application/x-tex">\mathcal{E}</annotation></semantics></math>, such as webpage snippets with titles, URLs, and text when using a web search engine tool, enclosed in <span class="ltx_text ltx_font_typewriter">&lt;tool_response&gt;retrieved documents&lt;/tool_response&gt;</span> tags.
In the final turn, after a <span class="ltx_text ltx_font_typewriter">[think]</span> step, the agent generates its answer within the <span class="ltx_text ltx_font_typewriter">&lt;answer&gt;&lt;/answer&gt;</span> tag, and this content is extracted as the trajectory’s final prediction <math alttext="\hat{a}" class="ltx_Math" display="inline" id="S2.SS1.p1.m12" intent=":literal"><semantics><mover accent="true"><mi>a</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{a}</annotation></semantics></math>, which is expected to correctly address the input query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.p1.m13" intent=":literal"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math>. This agent-environment interaction is illustrated at the bottom of <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S2.F2" title="Figure 2 ‣ 2.2 Agentic Reinforcement Learning Pipeline ‣ 2 Preliminaries ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Agentic Reinforcement Learning Pipeline</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Policy Optimization.</span>
Agentic RL typically adopts policy-gradient methods to optimize the agent policy <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S2.SS2.p1.m1" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math>.
A common approach is <em class="ltx_emph ltx_font_italic">Group Relative Policy Optimization</em> (GRPO) <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib31" title="">2024</a>)</cite>, which removes the need for an explicit critic by normalizing returns within each sampled group of rollouts.
Formally, given an actor model <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S2.SS2.p1.m2" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math>, a group of <math alttext="G" class="ltx_Math" display="inline" id="S2.SS2.p1.m3" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> rollouts <math alttext="\{o_{i}\}_{i=1}^{G}" class="ltx_Math" display="inline" id="S2.SS2.p1.m4" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>o</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><annotation encoding="application/x-tex">\{o_{i}\}_{i=1}^{G}</annotation></semantics></math> is sampled from old policy <math alttext="\pi_{\theta_{\text{old}}}(\cdot\mid q)" class="ltx_math_unparsed" display="inline" id="S2.SS2.p1.m5" intent=":literal"><semantics><mrow><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>q</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}(\cdot\mid q)</annotation></semantics></math> for each input <math alttext="(q,a)\sim\mathcal{D}" class="ltx_Math" display="inline" id="S2.SS2.p1.m6" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>q</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><mo>∼</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi></mrow><annotation encoding="application/x-tex">(q,a)\sim\mathcal{D}</annotation></semantics></math>. The policy is then optimized by maximizing the clipped surrogate objective with KL regularization:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S2.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E1X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{J}_{\mathrm{GRPO}}(\theta)=" class="ltx_Math" display="inline" id="S2.E1X.m2" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒥</mi><mi>GRPO</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{J}_{\mathrm{GRPO}}(\theta)=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{(q,a)\sim\mathcal{D},\,\{o_{i}\}\sim\pi_{\theta_{\text{old}}}(\cdot|q)}\Bigg[\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_{i}|}\sum_{t=1}^{|o_{i}|}\min\!\Bigg(\frac{\pi_{\theta}(o_{i,t}\mid q,o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t}\mid q,o_{i,&lt;t})}\,\widehat{A}_{i}," class="ltx_math_unparsed" display="inline" id="S2.E1X.m3" intent=":literal"><semantics><mrow><msub><mi>𝔼</mi><mrow><mrow><mo stretchy="false">(</mo><mi>q</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><mo>∼</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo rspace="0.337em">,</mo><mrow><mo stretchy="false">{</mo><msub><mi>o</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mo>∼</mo><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mi>q</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mrow><mo maxsize="2.600em" minsize="2.600em">[</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>G</mi></mfrac></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover></mstyle><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><msub><mi>o</mi><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></mfrac></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><msub><mi>o</mi><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></munderover></mstyle><mpadded style="width:1.653em;" width="1.653em"><mi>min</mi></mpadded><mrow><mo maxsize="2.600em" minsize="2.600em">(</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi></msub><mo>,</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}_{(q,a)\sim\mathcal{D},\,\{o_{i}\}\sim\pi_{\theta_{\text{old}}}(\cdot|q)}\Bigg[\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_{i}|}\sum_{t=1}^{|o_{i}|}\min\!\Bigg(\frac{\pi_{\theta}(o_{i,t}\mid q,o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t}\mid q,o_{i,&lt;t})}\,\widehat{A}_{i},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E1Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\qquad\qquad\mathrm{clip}\!\left(\frac{\pi_{\theta}(o_{i,t}\mid q,o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t}\mid q,o_{i,&lt;t})},\,1-\epsilon,\,1+\epsilon\right)\,\widehat{A}_{i}\Bigg)-\beta\,\mathbb{D}_{\mathrm{KL}}(\pi_{\theta}\,\|\,\pi_{\mathrm{ref}})\Bigg]," class="ltx_math_unparsed" display="inline" id="S2.E1Xa.m2" intent=":literal"><semantics><mrow><mrow><mpadded style="width:1.428em;" width="1.428em"><mi>clip</mi></mpadded><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>,</mo><mn> 1</mn><mo>−</mo><mi>ϵ</mi><mo>,</mo><mn> 1</mn><mo>+</mo><mi>ϵ</mi><mo rspace="0.170em">)</mo></mrow><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi></msub><mo maxsize="2.600em" minsize="2.600em">)</mo></mrow><mo>−</mo><mi>β</mi><msub><mi>𝔻</mi><mi>KL</mi></msub><mrow><mo stretchy="false">(</mo><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0.337em">∥</mo><msub><mi>π</mi><mi>ref</mi></msub><mo stretchy="false">)</mo></mrow><mo maxsize="2.600em" minsize="2.600em">]</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\qquad\qquad\mathrm{clip}\!\left(\frac{\pi_{\theta}(o_{i,t}\mid q,o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t}\mid q,o_{i,&lt;t})},\,1-\epsilon,\,1+\epsilon\right)\,\widehat{A}_{i}\Bigg)-\beta\,\mathbb{D}_{\mathrm{KL}}(\pi_{\theta}\,\|\,\pi_{\mathrm{ref}})\Bigg],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where
<math alttext="\widehat{A}_{i}=\tfrac{r_{i}-\mathrm{mean}(r_{1},r_{2},\cdots,r_{G})}{\mathrm{std}(r_{1},r_{2},\cdots,r_{G})}" class="ltx_Math" display="inline" id="S2.SS2.p1.m7" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>−</mo><mrow><mi>mean</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mrow><mi>std</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\widehat{A}_{i}=\tfrac{r_{i}-\mathrm{mean}(r_{1},r_{2},\cdots,r_{G})}{\mathrm{std}(r_{1},r_{2},\cdots,r_{G})}</annotation></semantics></math>
is the normalized group-relative advantage for the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.p1.m8" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th rollout and <math alttext="r_{i}" class="ltx_Math" display="inline" id="S2.SS2.p1.m9" intent=":literal"><semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_{i}</annotation></semantics></math> is the outcome reward of the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.p1.m10" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th rollout. <math alttext="\epsilon" class="ltx_Math" display="inline" id="S2.SS2.p1.m11" intent=":literal"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math> is the clipping ratio, and <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS2.p1.m12" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> controls the KL penalty that regularizes the updated policy toward the reference model <math alttext="\pi_{\mathrm{ref}}" class="ltx_Math" display="inline" id="S2.SS2.p1.m13" intent=":literal"><semantics><msub><mi>π</mi><mi>ref</mi></msub><annotation encoding="application/x-tex">\pi_{\mathrm{ref}}</annotation></semantics></math>.
During optimization, gradients are applied only to decision tokens (reasoning, tool calls, answers), while tool responses from the external environment are masked out.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Reward.</span>
During training, the agent receives a scalar reward <math alttext="r" class="ltx_Math" display="inline" id="S2.SS2.p2.m1" intent=":literal"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> for each rollout <math alttext="o" class="ltx_Math" display="inline" id="S2.SS2.p2.m2" intent=":literal"><semantics><mi>o</mi><annotation encoding="application/x-tex">o</annotation></semantics></math>, which provides the optimization signal.
Prior work usually adopts an outcome-based answer reward combined with a format penalty:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r=\begin{cases}\;\;\mathrm{F1}(\hat{a},a)=\frac{2\,|\hat{a}\cap a\,|}{|\hat{a}|+|a|}\;\in[0,1],&amp;\text{if the output is in valid format},\\[6.0pt]
\;\;\lambda_{\text{fmt}},&amp;\text{otherwise},\end{cases}" class="ltx_Math" display="block" id="S2.E2.m1" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mrow><mi>F1</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mstyle displaystyle="false"><mfrac><mrow><mn>2</mn><mo lspace="0.170em" rspace="0em">​</mo><mrow><mo stretchy="false">|</mo><mrow><mover accent="true"><mi>a</mi><mo>^</mo></mover><mo>∩</mo><mi>a</mi></mrow><mo lspace="0.170em" stretchy="false">|</mo></mrow></mrow><mrow><mrow><mo stretchy="false">|</mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><mo stretchy="false">|</mo></mrow><mo>+</mo><mrow><mo stretchy="false">|</mo><mi>a</mi><mo stretchy="false">|</mo></mrow></mrow></mfrac></mstyle><mo lspace="0.558em">∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtext>if the output is in valid format</mtext><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><msub><mi>λ</mi><mtext>fmt</mtext></msub><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtext>otherwise</mtext><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">r=\begin{cases}\;\;\mathrm{F1}(\hat{a},a)=\frac{2\,|\hat{a}\cap a\,|}{|\hat{a}|+|a|}\;\in[0,1],&amp;\text{if the output is in valid format},\\[6.0pt]
\;\;\lambda_{\text{fmt}},&amp;\text{otherwise},\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\hat{a}" class="ltx_Math" display="inline" id="S2.SS2.p2.m3" intent=":literal"><semantics><mover accent="true"><mi>a</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{a}</annotation></semantics></math> is the predicted final answer for each rollout, <math alttext="a" class="ltx_Math" display="inline" id="S2.SS2.p2.m4" intent=":literal"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> is the ground-truth answer, and <math alttext="\mathrm{F1}(\hat{a},a)\in[0,1]" class="ltx_Math" display="inline" id="S2.SS2.p2.m5" intent=":literal"><semantics><mrow><mrow><mi>F1</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{F1}(\hat{a},a)\in[0,1]</annotation></semantics></math> denotes the word-level F1 score between the two.
If the output violates the required schema (e.g., missing tags or malformed JSON), a negative constant <math alttext="\lambda_{\text{fmt}}&lt;0" class="ltx_Math" display="inline" id="S2.SS2.p2.m6" intent=":literal"><semantics><mrow><msub><mi>λ</mi><mtext>fmt</mtext></msub><mo>&lt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_{\text{fmt}}&lt;0</annotation></semantics></math> is assigned as a penalty. Thus, the outcome reward provides a correctness signal aligned with evaluation metrics, while the format penalty enforces the structural validity of outputs.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="317" id="S2.F2.g1" src="x2.png" width="760"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The training pipeline of IGPO. (Upper) Turn-level information gain rewards are computed by measuring changes in ground-truth probability and combined with the outcome reward to derive discounted advantages. (Lower) Each rollout contains at most <math alttext="T-1" class="ltx_Math" display="inline" id="S2.F2.m2" intent=":literal"><semantics><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T-1</annotation></semantics></math> interaction turns, where each turn includes a reasoning step, a tool call, and the returned tool response, followed by a final answer turn. During optimization, the loss on tool response is masked out.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Information Gain-based Policy Optimization</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p">In this section, we first illustrate our motivation and then provide a detailed introduction to our proposed information gain-based policy optimization, whose overall framework is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S2.F2" title="Figure 2 ‣ 2.2 Agentic Reinforcement Learning Pipeline ‣ 2 Preliminaries ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Motivation</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p">While outcome-based reinforcement learning has been effective in single-turn tasks, directly extending it to multi-turn agentic settings such as search agents faces critical limitations.
In the standard GRPO framework (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S2.E1" title="In 2.2 Agentic Reinforcement Learning Pipeline ‣ 2 Preliminaries ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">1</span></a>), each rollout <math alttext="o_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.m1" intent=":literal"><semantics><msub><mi>o</mi><mi>i</mi></msub><annotation encoding="application/x-tex">o_{i}</annotation></semantics></math> receives a scalar reward <math alttext="r_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.m2" intent=":literal"><semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_{i}</annotation></semantics></math> computed from the final answer <math alttext="\hat{a}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.m3" intent=":literal"><semantics><msub><mover accent="true"><mi>a</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{a}_{i}</annotation></semantics></math>.
For complex queries, however, it is often the case that all <math alttext="G" class="ltx_Math" display="inline" id="S3.SS1.p1.m4" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> rollouts fail to produce the correct answer, resulting in uniformly zero rewards; conversely, for simple queries, all rollouts may produce the same correct answer, leading to the same issue.
In these cases, the normalized group-relative advantages <math alttext="\{\widehat{A}_{i}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.m5" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\widehat{A}_{i}\}</annotation></semantics></math> collapse to near zero, and the entire sample provides almost no learning signal.
We refer to this phenomenon as <em class="ltx_emph ltx_font_italic">advantage collapse</em>. Moreover, such outcome-only supervision lacks fine-grained credit assignment across turns. In multi-turn scenarios, later decisions critically depend on earlier ones: a tool call may be effective yet rendered useless by prior retrieval errors, or early reasoning may be correct but overshadowed by subsequent mistakes. Such dependencies are obscured under single outcome rewards, making it difficult for the policy to distinguish productive reasoning from uninformative or misleading turns.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p">To mitigate this issue, we introduce Information-Gain-based Policy Optimization (IGPO).
The key idea is to exploit the multi-turn structure of agentic rollouts and treat each turn as an opportunity to acquire additional evidence toward the ground truth. At every turn, IGPO measures the increase in the policy’s confidence of generating the correct answer, which we defined as the <em class="ltx_emph ltx_font_italic">information gain</em> of this turn and uses this as the turn-level reward. By rewarding turn-level information gain, IGPO supplies denser and more fine-grained supervision, especially at early training stages. We further present a theoretical analysis in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A1" title="Appendix A Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">A</span></a>, which intuitively explains why IGPO effectively addresses the limitations of sparse outcome rewards in multi-turn scenarios.
Since the information gain is defined with respect to the ground-truth answer and computed under teacher forcing, it always produces a valid signal, ensuring that every sample contributes to learning even when no rollout produces a fully correct final answer.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Information Gain-based Turn-level Reward</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Turn-level Reward.</span>
We view multi-turn agent–environment interaction as a process of <em class="ltx_emph ltx_font_italic">incrementally acquiring information about the ground truth</em>.
To capture this intuition, we propose an intrinsic <em class="ltx_emph ltx_font_italic">information gain-based reward</em>.
At each turn, we evaluate the policy’s probability of generating the ground-truth answer and define the reward as the difference between consecutive states.
We call this the <em class="ltx_emph ltx_font_italic">information gain reward</em>, as it measures the <em class="ltx_emph ltx_font_italic">marginal increase in posterior probability mass assigned to the ground truth</em> induced by the current turn.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p">Formally, let <math alttext="a=(a_{1},\ldots,a_{L})" class="ltx_Math" display="inline" id="S3.SS2.p2.m1" intent=":literal"><semantics><mrow><mi>a</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>a</mi><mi>L</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">a=(a_{1},\ldots,a_{L})</annotation></semantics></math> denote the ground-truth answer tokens.
For the <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p2.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-th turn in the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p2.m3" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th rollout, the probability of <math alttext="a" class="ltx_Math" display="inline" id="S3.SS2.p2.m4" intent=":literal"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> under the current policy <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p2.m5" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> is computed as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\pi_{\theta}(a\mid q,o_{i,\leq t})=\exp\!\left(\frac{1}{L}\sum_{j=1}^{L}\log\pi_{\theta}(a_{j}\mid q,o_{i,\leq t},a_{&lt;j})\right)," class="ltx_Math" display="block" id="S3.E3.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mpadded style="width:1.370em;" width="1.370em"><mi>exp</mi></mpadded><mo>⁡</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mi>L</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mi>θ</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>a</mi><mi>j</mi></msub><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></mrow></msub><mo>,</mo><msub><mi>a</mi><mrow><mi></mi><mo>&lt;</mo><mi>j</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\pi_{\theta}(a\mid q,o_{i,\leq t})=\exp\!\left(\frac{1}{L}\sum_{j=1}^{L}\log\pi_{\theta}(a_{j}\mid q,o_{i,\leq t},a_{&lt;j})\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="o_{i,\leq t}" class="ltx_Math" display="inline" id="S3.SS2.p2.m6" intent=":literal"><semantics><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></mrow></msub><annotation encoding="application/x-tex">o_{i,\leq t}</annotation></semantics></math> denotes the prefix of rollout <math alttext="o_{i}" class="ltx_Math" display="inline" id="S3.SS2.p2.m7" intent=":literal"><semantics><msub><mi>o</mi><mi>i</mi></msub><annotation encoding="application/x-tex">o_{i}</annotation></semantics></math> up to turn <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p2.m8" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>.
Then the immediate reward <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_tag ltx_tag_note">*</span>Due to its log-prob origin, we apply stop-gradient to the information gain–based reward.</span></span></span> for turn <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p2.m9" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> is</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{i,t}=\mathrm{IG}(a\mid q,o_{i,t})=\pi_{\theta}(a\mid q,o_{i,\leq t})-\pi_{\theta}(a\mid q,o_{i,\leq t-1}),\qquad 1\leq t&lt;T." class="ltx_Math" display="block" id="S3.E4.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><mi>IG</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>≤</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>≤</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo rspace="2.167em">,</mo><mrow><mn>1</mn><mo>≤</mo><mi>t</mi><mo>&lt;</mo><mi>T</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">r_{i,t}=\mathrm{IG}(a\mid q,o_{i,t})=\pi_{\theta}(a\mid q,o_{i,\leq t})-\pi_{\theta}(a\mid q,o_{i,\leq t-1}),\qquad 1\leq t&lt;T.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">In practice, the ground-truth answer <math alttext="a" class="ltx_Math" display="inline" id="S3.SS2.p2.m10" intent=":literal"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> is wrapped in the same schema as a predicted answer to ensure consistency with rollout formatting, e.g.,
<span class="ltx_text ltx_font_typewriter">&lt;think&gt;Now there’s enough information to answer&lt;/think&gt;&lt;answer&gt;Ground Truth <math alttext="a" class="ltx_Math" display="inline" id="S3.SS2.p2.m11" intent=":literal"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>&lt;/answer&gt;</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p">This turn-level reward has two desirable properties:
(1) <em class="ltx_emph ltx_font_italic">Ground-truth awareness</em>: the reward increases when the action raises the policy’s confidence in the correct answer, and decreases otherwise;
(2) <em class="ltx_emph ltx_font_italic">Dense supervision</em>: the reward is defined for every sample, even when no rollout yields a correct answer, thereby alleviating reward sparsity and avoiding advantage collapse.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Integrating Outcome and Turn-level Rewards.</span>
For each rollout <math alttext="o_{i}=(\tau_{i,1},\ldots,\tau_{i,T})" class="ltx_Math" display="inline" id="S3.SS2.p4.m1" intent=":literal"><semantics><mrow><msub><mi>o</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>τ</mi><mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>τ</mi><mrow><mi>i</mi><mo>,</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">o_{i}=(\tau_{i,1},\ldots,\tau_{i,T})</annotation></semantics></math> where the last turn <math alttext="\tau_{i,T}" class="ltx_Math" display="inline" id="S3.SS2.p4.m2" intent=":literal"><semantics><msub><mi>τ</mi><mrow><mi>i</mi><mo>,</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">\tau_{i,T}</annotation></semantics></math> is the answer turn producing <math alttext="\hat{a}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p4.m3" intent=":literal"><semantics><msub><mover accent="true"><mi>a</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{a}_{i}</annotation></semantics></math>, we can construct a length-<math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p4.m4" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> reward vector
<math alttext="\mathbf{r}_{i}=(r_{i,1},\,r_{i,2},\,\ldots,\,r_{i,T})." class="ltx_Math" display="inline" id="S3.SS2.p4.m5" intent=":literal"><semantics><mrow><mrow><msub><mi>𝐫</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub><mo rspace="0.337em">,</mo><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow></msub><mo rspace="0.337em">,</mo><mi mathvariant="normal">…</mi><mo rspace="0.337em">,</mo><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbf{r}_{i}=(r_{i,1},\,r_{i,2},\,\ldots,\,r_{i,T}).</annotation></semantics></math>
For <math alttext="t&lt;T" class="ltx_Math" display="inline" id="S3.SS2.p4.m6" intent=":literal"><semantics><mrow><mi>t</mi><mo>&lt;</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">t&lt;T</annotation></semantics></math>, the turn reward is the information gain <math alttext="r_{i,t}=\mathrm{IG}(a\mid q,o_{i,t})" class="ltx_Math" display="inline" id="S3.SS2.p4.m7" intent=":literal"><semantics><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mrow><mi>IG</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">r_{i,t}=\mathrm{IG}(a\mid q,o_{i,t})</annotation></semantics></math> defined in Section <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S3.SS2" title="3.2 Information Gain-based Turn-level Reward ‣ 3 Information Gain-based Policy Optimization ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
For the answer turn <math alttext="t=T" class="ltx_Math" display="inline" id="S3.SS2.p4.m8" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">t=T</annotation></semantics></math>, the reward <math alttext="r_{i,T}" class="ltx_Math" display="inline" id="S3.SS2.p4.m9" intent=":literal"><semantics><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">r_{i,T}</annotation></semantics></math> follows the outcome-based formulation in Eq. <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S2.E2" title="In 2.2 Agentic Reinforcement Learning Pipeline ‣ 2 Preliminaries ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">2</span></a>.
This yields a dense per-turn supervision signal that combines intrinsic information gains for intermediate turns with a final extrinsic correctness signal at the answer turn.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Policy Optimization with Turn-level Advantage</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Turn-level Advantage Estimation.</span>
Given a rollout <math alttext="o_{i}=(\tau_{i,1},\ldots,\tau_{i,T})" class="ltx_Math" display="inline" id="S3.SS3.p1.m1" intent=":literal"><semantics><mrow><msub><mi>o</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>τ</mi><mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>τ</mi><mrow><mi>i</mi><mo>,</mo><mi>T</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">o_{i}=(\tau_{i,1},\ldots,\tau_{i,T})</annotation></semantics></math>, each turn <math alttext="\tau_{i,t}" class="ltx_Math" display="inline" id="S3.SS3.p1.m2" intent=":literal"><semantics><msub><mi>τ</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">\tau_{i,t}</annotation></semantics></math> is associated with a reward <math alttext="r_{i,t}" class="ltx_Math" display="inline" id="S3.SS3.p1.m3" intent=":literal"><semantics><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">r_{i,t}</annotation></semantics></math> as defined in Section <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S3.SS2" title="3.2 Information Gain-based Turn-level Reward ‣ 3 Information Gain-based Policy Optimization ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
To make rewards comparable across turns and trajectories, we first aggregate all rewards in the group:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{R}=\{\,r_{i,t}:\;i=1,\ldots,G,\;t=1,\ldots,T\,\}," class="ltx_Math" display="block" id="S3.E5.m1" intent=":literal"><semantics><mrow><mrow><mi>𝐑</mi><mo>=</mo><mrow><mo rspace="0.170em" stretchy="false">{</mo><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo lspace="0.278em" rspace="0.558em">:</mo><mrow><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>G</mi></mrow></mrow><mo rspace="0.447em">,</mo><mrow><mi>t</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>T</mi></mrow></mrow></mrow><mo lspace="0.170em" stretchy="false">}</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{R}=\{\,r_{i,t}:\;i=1,\ldots,G,\;t=1,\ldots,T\,\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">and apply group-wise <math alttext="z" class="ltx_Math" display="inline" id="S3.SS3.p1.m4" intent=":literal"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>-normalization:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="A_{i,t}=\frac{r_{i,t}-\mathrm{mean}(\mathbf{R})}{\mathrm{std}(\mathbf{R})}." class="ltx_Math" display="block" id="S3.E6.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>A</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>−</mo><mrow><mi>mean</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐑</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mrow><mi>std</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐑</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">A_{i,t}=\frac{r_{i,t}-\mathrm{mean}(\mathbf{R})}{\mathrm{std}(\mathbf{R})}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p">While <math alttext="A_{i,t}" class="ltx_Math" display="inline" id="S3.SS3.p2.m1" intent=":literal"><semantics><msub><mi>A</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">A_{i,t}</annotation></semantics></math> captures the relative quality of each turn, it only reflects immediate effects and ignores the impact of current decisions on future turns.
To incorporate such long-horizon dependencies, we compute a discounted cumulative advantage to propagate outcome signals backward to earlier turns:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\widetilde{A}_{i,t}=\sum_{k=t}^{T}\gamma^{\,k-t}A_{i,k}," class="ltx_Math" display="block" id="S3.E7.m1" intent=":literal"><semantics><mrow><mrow><msub><mover accent="true"><mi>A</mi><mo>~</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo rspace="0.111em">=</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>k</mi><mo>=</mo><mi>t</mi></mrow><mi>T</mi></munderover><mrow><msup><mi>γ</mi><mrow><mi>k</mi><mo>−</mo><mi>t</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>A</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\widetilde{A}_{i,t}=\sum_{k=t}^{T}\gamma^{\,k-t}A_{i,k},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\gamma\in(0,1]" class="ltx_Math" display="inline" id="S3.SS3.p2.m2" intent=":literal"><semantics><mrow><mi>γ</mi><mo>∈</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\gamma\in(0,1]</annotation></semantics></math> is the discount factor.
During optimization, <math alttext="\widetilde{A}_{i,t}" class="ltx_Math" display="inline" id="S3.SS3.p2.m3" intent=":literal"><semantics><msub><mover accent="true"><mi>A</mi><mo>~</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">\widetilde{A}_{i,t}</annotation></semantics></math> is assigned to all decision tokens produced in turn <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p2.m4" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, while raw tool responses from the external environment are masked out.
This yields a dense and future-aware supervision signal for policy learning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Policy Optimization.</span>
With the discounted turn-level advantages <math alttext="\{\widetilde{A}_{i,j}\}" class="ltx_Math" display="inline" id="S3.SS3.p3.m1" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msub><mover accent="true"><mi>A</mi><mo>~</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\widetilde{A}_{i,j}\}</annotation></semantics></math> defined above, we optimize the agent policy using a clipped surrogate objective with KL regularization, following the same structure as GRPO but with a finer-grained credit assignment.
Formally, the IGPO objective is</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E8">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E8X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{J}_{\mathrm{IGPO}}(\theta)=" class="ltx_Math" display="inline" id="S3.E8X.m2" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒥</mi><mi>IGPO</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{J}_{\mathrm{IGPO}}(\theta)=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{(q,a)\sim\mathcal{D},\,\{o_{i}\}\sim\pi_{\theta_{\text{old}}}(\cdot\mid q)}\Bigg[\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_{i}|}\sum_{t=1}^{|o_{i}|}\min\!\Bigg(\frac{\pi_{\theta}(o_{i,t}\mid q,o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t}\mid q,o_{i,&lt;t})}\,\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\widetilde{A}_{i,\,t}," class="ltx_math_unparsed" display="inline" id="S3.E8X.m3" intent=":literal"><semantics><mrow><msub><mi>𝔼</mi><mrow><mrow><mo stretchy="false">(</mo><mi>q</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><mo>∼</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi><mo rspace="0.337em">,</mo><mrow><mo stretchy="false">{</mo><msub><mi>o</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mo>∼</mo><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>q</mi><mo stretchy="false">)</mo></mrow></mrow></msub><mrow><mo maxsize="2.600em" minsize="2.600em">[</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>G</mi></mfrac></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover></mstyle><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><msub><mi>o</mi><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></mfrac></mstyle><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><msub><mi>o</mi><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></munderover></mstyle><mpadded style="width:1.653em;" width="1.653em"><mi>min</mi></mpadded><mrow><mo maxsize="2.600em" minsize="2.600em">(</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><msub><mover accent="true"><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">A</mi><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">~</mo></mover><mrow><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">i</mi><mo mathcolor="#0000FF" rspace="0.337em" style="--ltx-fg-color:#0000FF;">,</mo><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></mrow></msub><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">,</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}_{(q,a)\sim\mathcal{D},\,\{o_{i}\}\sim\pi_{\theta_{\text{old}}}(\cdot\mid q)}\Bigg[\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_{i}|}\sum_{t=1}^{|o_{i}|}\min\!\Bigg(\frac{\pi_{\theta}(o_{i,t}\mid q,o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t}\mid q,o_{i,&lt;t})}\,\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\widetilde{A}_{i,\,t},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(8)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E8Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\qquad\qquad\mathrm{clip}\!\left(\frac{\pi_{\theta}(o_{i,t}\mid q,o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t}\mid q,o_{i,&lt;t})},\,1-\epsilon,\,1+\epsilon\right)\,\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\widetilde{A}_{i,\,t}\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\Bigg)-\beta\,\mathbb{D}_{\mathrm{KL}}(\pi_{\theta}\,\|\,\pi_{\mathrm{ref}})\Bigg]," class="ltx_math_unparsed" display="inline" id="S3.E8Xa.m2" intent=":literal"><semantics><mrow><mrow><mpadded style="width:1.428em;" width="1.428em"><mi>clip</mi></mpadded><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>∣</mo><mrow><mi>q</mi><mo>,</mo><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi></mi><mo>&lt;</mo><mi>t</mi></mrow></mrow></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>,</mo><mn> 1</mn><mo>−</mo><mi>ϵ</mi><mo>,</mo><mn> 1</mn><mo>+</mo><mi>ϵ</mi><mo rspace="0.170em">)</mo></mrow><msub><mover accent="true"><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">A</mi><mo mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">~</mo></mover><mrow><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">i</mi><mo mathcolor="#0000FF" rspace="0.337em" style="--ltx-fg-color:#0000FF;">,</mo><mi mathcolor="#0000FF" style="--ltx-fg-color:#0000FF;">t</mi></mrow></msub><mo mathcolor="#000000" maxsize="2.600em" minsize="2.600em" style="--ltx-fg-color:#000000;">)</mo></mrow><mo mathcolor="#000000" style="--ltx-fg-color:#000000;">−</mo><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">β</mi><msub><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">𝔻</mi><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">KL</mi></msub><mrow><mo mathcolor="#000000" stretchy="false" style="--ltx-fg-color:#000000;">(</mo><msub><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">π</mi><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">θ</mi></msub><mo lspace="0em" mathcolor="#000000" rspace="0.337em" style="--ltx-fg-color:#000000;">∥</mo><msub><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">π</mi><mi mathcolor="#000000" style="--ltx-fg-color:#000000;">ref</mi></msub><mo mathcolor="#000000" stretchy="false" style="--ltx-fg-color:#000000;">)</mo></mrow><mo mathcolor="#000000" maxsize="2.600em" minsize="2.600em" style="--ltx-fg-color:#000000;">]</mo><mo mathcolor="#000000" style="--ltx-fg-color:#000000;">,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\qquad\qquad\mathrm{clip}\!\left(\frac{\pi_{\theta}(o_{i,t}\mid q,o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t}\mid q,o_{i,&lt;t})},\,1-\epsilon,\,1+\epsilon\right)\,\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\widetilde{A}_{i,\,t}\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\Bigg)-\beta\,\mathbb{D}_{\mathrm{KL}}(\pi_{\theta}\,\|\,\pi_{\mathrm{ref}})\Bigg],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS3.p3.m2" intent=":literal"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math> is the clipping threshold, <math alttext="\beta" class="ltx_Math" display="inline" id="S3.SS3.p3.m3" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> controls the KL penalty strength, and <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p3.m4" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> maps token <math alttext="o_{i,t}" class="ltx_Math" display="inline" id="S3.SS3.p3.m5" intent=":literal"><semantics><msub><mi>o</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">o_{i,t}</annotation></semantics></math> to its originating turn.
During optimization, only decision tokens (reasoning, tool calls, and answers) receive gradient updates, while raw tool responses are masked out.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p">To further substantiate the simplicity and implementability of the proposed IGPO, we provide an algorithmic flow comparison between IGPO and GRPO in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#A5" title="Appendix E Comparison between GRPO and IGPO ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Appendix E</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Datasets &amp; Metrics.</span> To evaluate the effectiveness of our proposed IGPO, we conduct experiments on both in-domain (ID) and out-of-domain (OOD) QA benchmarks in an agentic search setting. Following previous work <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib48" title="">2025b</a>; Deng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib6" title="">2025</a>)</cite>, the ID setting includes four widely used datasets: NQ <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib20" title="">2019</a>)</cite>, TQ <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib18" title="">2017</a>)</cite>, HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib41" title="">2018</a>)</cite>, and 2Wiki <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib12" title="">2020</a>)</cite>, while the OOD setting includes three datasets: MusiQue <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib35" title="">2022</a>)</cite>, Bamboogle <cite class="ltx_cite ltx_citemacro_citep">(Press et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib26" title="">2022</a>)</cite>, and PopQA <cite class="ltx_cite ltx_citemacro_citep">(Mallen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib24" title="">2022</a>)</cite>.
We report word-level F1 as the evaluation metric, which is computed as the harmonic mean of precision and recall between the predicted and reference answers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Baselines.</span> To directly verify IGPO’s superiority on agentic search tasks, we compare it against a set of competitive baselines: (1) Prompt-based methods: CoT <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib40" title="">2022</a>)</cite>, CoT+RAG <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib9" title="">2023</a>)</cite>, and Search-o1 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib22" title="">2025b</a>)</cite>, which represent the baseline performance of LLMs without further training on search tasks. (2) Outcome-reward RL-based methods: Search-r1-base/Instruct <cite class="ltx_cite ltx_citemacro_citep">(Jin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib17" title="">2025</a>)</cite>, R1-searcher <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib33" title="">2025a</a>)</cite>, and DeepResearcher <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib48" title="">2025b</a>)</cite>, the representative search agents with outcome-based reward RL, yielding marked performance gains. (3) Step-reward RL-based methods: StepSearch-base/instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib38" title="">2025</a>)</cite>, ReasoningRAG <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib45" title="">2025c</a>)</cite>, and GiGPO <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib7" title="">2025</a>)</cite>, which are the latest approaches exploring step-reward RL in search-agent settings.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p">To further validate IGPO’s effectiveness, we also compare it against the following commonly used RL algorithms under the same configuration: PPO <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib30" title="">2017</a>)</cite>, a widely used actor-critic algorithm that requires an additional value model, and critic-free methods Reinforce++ <cite class="ltx_cite ltx_citemacro_citep">(Hu, <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib13" title="">2025</a>)</cite>, RLOO <cite class="ltx_cite ltx_citemacro_citep">(Kool et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib19" title="">2019</a>; Ahmadian et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib1" title="">2024</a>)</cite>, GRPO <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib31" title="">2024</a>)</cite>, and GSPO<cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib46" title="">2025a</a>)</cite> which perform advantage estimation over trajectory groups or batchs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Implementation Details.</span>
We use Qwen2.5-7B-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Qwen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib29" title="">2025</a>)</cite> as our backbone model. The training is conducted using the verl <cite class="ltx_cite ltx_citemacro_citep">(Sheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib32" title="">2025</a>)</cite> framework. The discounted factor <math alttext="\gamma" class="ltx_Math" display="inline" id="S4.SS1.p4.m1" intent=":literal"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is set to 1 with no future tuning. At each training step, we sample 32 prompts, and sample 16 rollouts for each prompt. The maximum dialogue turns are set to 10. For the environment, we use the google search API as our tool. The settings of our experiments are consistent with DeepResearcher <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib48" title="">2025b</a>)</cite>. For the other baselines in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S4.T1" title="Table 1 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Table 1</span></a>, we directly copy their reported results. All RL training methods
(including ours and the baselines) use exactly the same hyperparameter configurations. The training and inference prompt templates are shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#A6" title="Appendix F Prompt template used in our experiments. ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Appendix F</span></a>. Please refer to <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#A3" title="Appendix C More Implementation Details ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Appendix C</span></a> for more details.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Main results of IGPO compared with different agentic RL baselines across seven datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:4.6pt;padding-right:4.6pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" style="padding-left:4.6pt;padding-right:4.6pt;">In-domain</td>
<td class="ltx_td ltx_border_tt" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" style="padding-left:4.6pt;padding-right:4.6pt;">Out-of-domain</td>
<td class="ltx_td ltx_nopad_r ltx_border_tt" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">Method</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.6pt;padding-right:4.6pt;">NQ</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.6pt;padding-right:4.6pt;">TQ</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.6pt;padding-right:4.6pt;">HotpotQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.6pt;padding-right:4.6pt;">2Wiki</td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.6pt;padding-right:4.6pt;">Musique</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.6pt;padding-right:4.6pt;">Bamboogle</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.6pt;padding-right:4.6pt;">PopQA</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">Avg.</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="10" style="padding-left:4.6pt;padding-right:4.6pt;">
<span class="ltx_ERROR undefined">\cellcolor</span>[HTML]EFEFEF<span class="ltx_text ltx_font_bold">Prompt-based</span>
</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">CoT</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">19.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">45.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">24.4</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">26.4</td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">8.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">22.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">17.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">23.4</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">CoT+RAG</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">42.0</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">68.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">37.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">24.4</td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">10.0</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">25.4</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">46.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">36.4</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">Search-o1</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">32.4</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">58.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">33.0</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">30.9</td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">14.7</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">46.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">38.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">36.4</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="10" style="padding-left:4.6pt;padding-right:4.6pt;">
<span class="ltx_ERROR undefined">\cellcolor</span>[HTML]EFEFEF<span class="ltx_text ltx_font_bold">Outcome-reward RL-based</span>
</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">Search-r1-base</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">45.4</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">71.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_framed ltx_framed_underline">55.9</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">44.6</td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">26.7</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">56.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">43.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">49.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">Search-r1-instruct</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">33.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">44.7</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">45.7</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">43.4</td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">26.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">45.0</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">43.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">40.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">R1-searcher</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">35.4</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">73.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">44.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">59.4</td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">22.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">64.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">42.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">49.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">DeepResearcher</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">39.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_framed ltx_framed_underline">78.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">52.8</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_framed ltx_framed_underline">59.7</span></td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">27.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_framed ltx_framed_underline">71.0</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_framed ltx_framed_underline">48.5</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_framed ltx_framed_underline">53.9</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="10" style="padding-left:4.6pt;padding-right:4.6pt;">
<span class="ltx_ERROR undefined">\cellcolor</span>[HTML]EFEFEF<span class="ltx_text ltx_font_bold">Step-reward RL-based</span>
</th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">StepSearch-base</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">49.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">45.0</td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_font_bold">32.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">57.3</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">46.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">StepSearch-instruct</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">50.2</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">43.1</td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">31.2</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">53.4</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">44.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">ReasoningRAG</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">-</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">48.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">50.4</td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">20.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">45.5</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">46.2</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">42.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.6pt;padding-right:4.6pt;">GiGPO</th>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_framed ltx_framed_underline">46.4</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">64.7</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">41.6</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">43.6</td>
<td class="ltx_td" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">18.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">68.9</td>
<td class="ltx_td ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">46.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.6pt;padding-right:4.6pt;">47.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_font_bold">IGPO</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_font_bold">46.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_font_bold">80.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_font_bold">57.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_font_bold">68.2</span></td>
<td class="ltx_td ltx_border_bb" style="padding-left:4.6pt;padding-right:4.6pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_framed ltx_framed_underline">31.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_font_bold">74.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_font_bold">52.5</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:4.6pt;padding-right:4.6pt;"><span class="ltx_text ltx_font_bold">58.7</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Overall Performance</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p">The overall performance comparison between IGPO and the baseline methods is presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S4.T1" title="Table 1 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Table 1</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S4.T2" title="Table 2 ‣ 4.2 Overall Performance ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Table 2</span></a>. Based on these results, we can draw the following key observations:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Training-based methods consistently outperform prompt-based baselines.</span> As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S4.T1" title="Table 1 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Table 1</span></a>, all reinforcement learning–based methods, whether outcome- or step-reward driven, achieve substantially higher performance than all prompt-based approaches. This confirms that explicit policy optimization is essential for developing effective LLM-based agents, as opposed to relying on zero-shot prompting alone.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Existing step-reward methods yield competitive but unstable improvements compared to outcome-reward RL methods.</span> While step-reward baselines occasionally surpass outcome-reward ones on specific datasets (e.g., StepSearch on Musique), their overall performance still lags behind the strongest outcome-reward methods such as DeepResearcher. This suggests that existing step-reward designs, although able to provide intermediate guidance, often suffer from noisy or weak supervision signals that limit their generalizability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">IGPO achieves the best overall performance across both in-domain and out-of-domain datasets.</span> Our IGPO outperforms all baselines, with an average score of 58.7, a clear margin over the best method (+4.8 over DeepResearcher). This improvement is attributed to IGPO’s information gain-based reward design, which assigns intrinsic, ground-truth-aware credit at every turn while preserving the outcome reward at the answer step. By avoiding advantage collapse and improving sample efficiency, IGPO delivers robust gains across both in-domain and out-of-domain datasets.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Main results of IGPO compared with different RL baselines across seven datasets.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:94.9pt;vertical-align:-45.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.1pt,1.3pt) scale(0.972725025403441,0.972725025403441) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4">In-domain</td>
<td class="ltx_td ltx_border_tt"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3">Out-of-domain</td>
<td class="ltx_td ltx_border_tt"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Method</th>
<td class="ltx_td ltx_align_center ltx_border_t">NQ</td>
<td class="ltx_td ltx_align_center ltx_border_t">TQ</td>
<td class="ltx_td ltx_align_center ltx_border_t">HotpotQA</td>
<td class="ltx_td ltx_align_center ltx_border_t">2Wiki</td>
<td class="ltx_td ltx_border_t"></td>
<td class="ltx_td ltx_align_center ltx_border_t">Musique</td>
<td class="ltx_td ltx_align_center ltx_border_t">Bamboogle</td>
<td class="ltx_td ltx_align_center ltx_border_t">PopQA</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">Avg.</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">RLOO</th>
<td class="ltx_td ltx_align_center ltx_border_t">40.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">72.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">49.6</td>
<td class="ltx_td ltx_align_center ltx_border_t">55.0</td>
<td class="ltx_td ltx_border_t"></td>
<td class="ltx_td ltx_align_center ltx_border_t">24.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">62.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">43.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">49.7</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">PPO</th>
<td class="ltx_td ltx_align_center">38.7</td>
<td class="ltx_td ltx_align_center">75.4</td>
<td class="ltx_td ltx_align_center">48.6</td>
<td class="ltx_td ltx_align_center">59.7</td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>26.2</td>
<td class="ltx_td ltx_align_center">63.4</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>48.7</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">51.5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">GRPO</th>
<td class="ltx_td ltx_align_center">40.3</td>
<td class="ltx_td ltx_align_center">77.0</td>
<td class="ltx_td ltx_align_center">48.9</td>
<td class="ltx_td ltx_align_center">57.7</td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center">25.0</td>
<td class="ltx_td ltx_align_center">65.1</td>
<td class="ltx_td ltx_align_center">49.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">51.9</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Reinforce++</th>
<td class="ltx_td ltx_align_center">34.3</td>
<td class="ltx_td ltx_align_center">67.5</td>
<td class="ltx_td ltx_align_center">45.9</td>
<td class="ltx_td ltx_align_center">54.5</td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center">23.7</td>
<td class="ltx_td ltx_align_center">61.2</td>
<td class="ltx_td ltx_align_center">44.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">47.3</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">GSPO</th>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>41.5</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>77.7</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>46.3</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>60.1</td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center">25.4</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>67.6</td>
<td class="ltx_td ltx_align_center">45.4</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>52.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span class="ltx_text ltx_font_bold">IGPO</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">46.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">80.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">57.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">68.2</span></td>
<td class="ltx_td ltx_border_bb"></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">31.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">74.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">52.5</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">58.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">IGPO consistently outperforms other RL algorithms.</span> Beyond task-specific baselines, <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S4.T2" title="Table 2 ‣ 4.2 Overall Performance ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Table 2</span></a> shows that IGPO also achieves the highest overall score among standard RL methods, surpassing RLOO, PPO, Reinforce++, and GSPO. Unlike these methods, which rely solely on sparse outcome rewards, IGPO incorporates turn-level advantages to provide denser and more stable supervision, leading to stronger generalization and more efficient training.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation results of IGPO on Qwen2.5-3B/7B-Instruct with different reward designs. IGPO (w/ F1) corresponds to using only outcome rewards, reducing to standard GRPO.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:109.2pt;vertical-align:-52.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.4pt,6.4pt) scale(0.895195705261171,0.895195705261171) ;">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_tt"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4">In-domain</td>
<td class="ltx_td ltx_border_tt"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3">Out-of-domain</td>
<td class="ltx_td ltx_border_tt"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Method</td>
<td class="ltx_td ltx_align_center ltx_border_t">NQ</td>
<td class="ltx_td ltx_align_center ltx_border_t">TQ</td>
<td class="ltx_td ltx_align_center ltx_border_t">HotpotQA</td>
<td class="ltx_td ltx_align_center ltx_border_t">2Wiki</td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center ltx_border_t">Musique</td>
<td class="ltx_td ltx_align_center ltx_border_t">Bamboogle</td>
<td class="ltx_td ltx_align_center ltx_border_t">PopQA</td>
<td class="ltx_td ltx_align_left">Avg.</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="10">
<span class="ltx_ERROR undefined">\cellcolor</span>[HTML]EFEFEF<span class="ltx_text ltx_font_bold">Qwen2.5-3B-Instruct</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">IGPO (w/ F1)</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>31.0</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>55.6</td>
<td class="ltx_td ltx_align_center">27.5</td>
<td class="ltx_td ltx_align_center">29.4</td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center">12.1</td>
<td class="ltx_td ltx_align_center">35.7</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>34.9</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">32.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">IGPO (w/ IG)</td>
<td class="ltx_td ltx_align_center">29.1</td>
<td class="ltx_td ltx_align_center">53.6</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>27.9</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>36.5</td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>17.5</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>44.7</td>
<td class="ltx_td ltx_align_center">31.3</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>34.4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">IGPO (w/ F1+IG)</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">40.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">69.4</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">46.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">48.2</span></td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">23.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">57.9</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">47.4</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">47.6</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="10">
<span class="ltx_ERROR undefined">\cellcolor</span>[HTML]EFEFEF<span class="ltx_text ltx_font_bold">Qwen2.5-7B-Instruct</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">IGPO (w/ F1)</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>40.3</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>77.0</td>
<td class="ltx_td ltx_align_center">48.9</td>
<td class="ltx_td ltx_align_center">57.7</td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center">25.0</td>
<td class="ltx_td ltx_align_center">65.1</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>49.6</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">51.9</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">IGPO (w/ IG)</td>
<td class="ltx_td ltx_align_center">37.5</td>
<td class="ltx_td ltx_align_center">75.0</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>51.0</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>61.0</td>
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>28.6</td>
<td class="ltx_td ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>69.6</td>
<td class="ltx_td ltx_align_center">47.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">
<span class="ltx_ERROR undefined">\ul</span>52.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">IGPO (w/ F1+IG)</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">46.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">80.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">57.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">68.2</span></td>
<td class="ltx_td ltx_border_bb"></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">31.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">74.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">52.5</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">58.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Study</h3>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="572" id="S4.F3.sf1.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>NQ</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="574" id="S4.F3.sf2.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>TQ</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="574" id="S4.F3.sf3.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>HotpotQA</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="574" id="S4.F3.sf4.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>2Wiki</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="574" id="S4.F3.sf5.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>Musique</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="574" id="S4.F3.sf6.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Bamboogle</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="574" id="S4.F3.sf7.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>PopQA</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Training curves on Qwen2.5-7B-Instruct with different reward designs.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p">We further conduct ablation experiments to assess the contribution of different reward components. As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S4.T3" title="Table 3 ‣ 4.2 Overall Performance ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Table 3</span></a>, we observe:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">First, using only information gain (IG) turn-based reward or only outcome reward (F1) yields clearly inferior results compared to the full combination.</span> This highlights the complementary roles of turn-level and outcome-level supervision: the outcome reward enforces alignment with the final task objective but suffers from severe sparsity, whereas the information gain reward offers dense and stable guidance for intermediate steps.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Second, IGPO with only IG achieves performance comparable to or even exceeding that of standard GRPO (i.e., IGPO w/ F1).</span> This demonstrates that IGPO’s information gain reward is not subject to reward hacking. Usually, without outcome supervision, unstable reward designs would quickly collapse. In contrast, our IGPO remains robust because its turn-level signals are intrinsically defined and grounded in the ground truth.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Third, the improvements are particularly pronounced on the smaller 3B model.</span> Compared to standard GRPO, IGPO improves the 3B model by +15.3 points (32.3 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS3.p4.m1" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> 47.6) and the 7B model by +6.8 points (51.9 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS3.p4.m2" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> 58.7). This larger benefit on the 3B model arises because advantage collapse is more severe for weaker models that struggle to directly produce correct answers (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>), making them especially reliant on dense reward signals. In such cases, the information gain reward helps prune noisy reasoning paths and reinforce rollouts that progressively approach the ground truth.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Finally, IGPO demonstrates consistently faster and more stable learning dynamics.</span> As shown in  <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S4.F3" title="Figure 3 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>, IGPO steadily outperforms its two ablated variants throughout training across all seven datasets. The curves highlight two advantages: (i) IGPO converges to higher F1 scores, confirming the benefit of combining intrinsic turn-level reward and outcome rewards, and (ii) IGPO maintains stable improvements over steps, indicating robustness against reward sparsity and noisy supervision. These results further validate that IGPO provides dense and reliable training signals, thereby improving both training efficiency and final performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>In-Depth Analysis</h3>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Ground-truth Entropy Reduction.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px1.p1">
<p class="ltx_p">To better understand how IGPO improves training dynamics, we measure the change in ground-truth answer entropy from the initial query (Turn 0) to the last non-answer turn (<math alttext="T-1" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px1.p1.m1" intent=":literal"><semantics><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T-1</annotation></semantics></math>). As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S4.F5" title="Figure 5 ‣ Token Efficiency. ‣ 4.4 In-Depth Analysis ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>, IGPO consistently achieves a larger entropy reduction than GRPO throughout training. This indicates that the information gain reward effectively encourages intermediate steps to move the policy closer to the ground-truth answer distribution. In contrast, outcome-based supervision in GRPO provides no guidance for intermediate turns, resulting in weaker entropy reduction. These results highlight that IGPO’s turn-level supervision translates into more confident and grounded reasoning trajectories.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Token Efficiency.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px2.p1">
<p class="ltx_p">We further compare IGPO and GRPO in terms of token efficiency, i.e., the performance improvement per token used for gradient updates. As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S4.F5" title="Figure 5 ‣ Token Efficiency. ‣ 4.4 In-Depth Analysis ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>, performance increases more rapidly under IGPO, and the gap over GRPO widens as training progresses. In other words, IGPO achieves stronger performance with fewer tokens, indicating that its turn-level rewards deliver denser and more informative gradients than outcome-only supervision. This finding is consistent with the training dynamics observed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#S4.F3" title="Figure 3 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>, where IGPO not only converges faster but also maintains a stable advantage throughout optimization. Such improvements in token efficiency are particularly valuable in agentic RL, where training data is scarce and expensive to obtain, making efficient use of every gradient update a critical factor for scaling.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px2.p2">
<p class="ltx_p">The case study and additional analyses are provided in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#A4" title="Appendix D More Discussion and Experimental Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Appendix D</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px2.p3">
<p class="ltx_p">Beyond empirical effectiveness, our theoretical analysis in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#A1" title="Appendix A Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Appendix A</span></a> shows that maximizing turn-level information gain constrains error accumulation in multi-turn scenarios. Thus, IGPO not only alleviates advantage collapse but also reduces error accumulation in long-horizon agentic tasks.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F5.fig1" style="width:190.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="363" id="S4.F5.g1" src="x10.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Mean reduction in ground-truth answer entropy from the initial query (Turn 0) to the last non-answer turn (<math alttext="T\!-\!1" class="ltx_Math" display="inline" id="S4.F5.m2" intent=":literal"><semantics><mrow><mi>T</mi><mo lspace="0.052em" rspace="0.052em">−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T\!-\!1</annotation></semantics></math>) during training.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F5.fig2" style="width:190.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="362" id="S4.F5.g2" src="x11.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Token Efficiency: average performance with respect to the number of tokens used for gradient updates.</figcaption>
</figure>
</div>
</div>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p">The recent success of reinforcement learning (RL) methods in large reasoning models <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib2" title="">2025a</a>)</cite> such as OpenAI o1 <cite class="ltx_cite ltx_citemacro_citep">(Jaech et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib15" title="">2024</a>)</cite> and DeepSeek R1 <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib10" title="">2025</a>)</cite> has established RL as a central tool for enhancing large language models (LLMs)-based agents to solve more complex tasks. A growing body of work has explored different RL algorithms such as PPO <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib30" title="">2017</a>)</cite>, Reinforce++ <cite class="ltx_cite ltx_citemacro_citep">(Hu, <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib13" title="">2025</a>)</cite>, GRPO <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib31" title="">2024</a>)</cite>, RLOO <cite class="ltx_cite ltx_citemacro_citep">(Kool et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib19" title="">2019</a>; Ahmadian et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib1" title="">2024</a>)</cite>, DAPO <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib42" title="">2025</a>)</cite>, and GSPO <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib46" title="">2025a</a>)</cite>. These methods have been particularly effective in improving the capabilities of LLM-based agents <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib21" title="">2025a</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p">Building on these advances, an important line of research has focused on applying RL to search-based agents <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib6" title="">2025</a>; Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib5" title="">2025b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib4" title="">a</a>)</cite>. Early efforts such as DeepRetrieval <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib16" title="">2025</a>)</cite> demonstrated the feasibility of end-to-end optimization by applying PPO with retrieval-oriented metrics (e.g., recall) as rewards. Subsequent works, including Search-R1 <cite class="ltx_cite ltx_citemacro_citep">(Jin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib17" title="">2025</a>)</cite>, DeepResearcher <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib48" title="">2025b</a>)</cite>, and ReSearch <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib3" title="">2025b</a>)</cite>, extended this paradigm to multi-turn reasoning and search. R1-Searcher <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib33" title="">2025a</a>)</cite> and R1-Searcher++ <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib34" title="">2025b</a>)</cite> further introduced two-stage RL strategies, separately strengthening the ability to interact with retrieval systems and to utilize retrieved information effectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p">However, in multi-turn scenarios, outcome-only rewards remain sparse and often fail to provide sufficient guidance, leading to unstable optimization and inefficient sample utilization. Recent studies have explored step-wise or process-level rewards that assign credit to intermediate actions. ReasonRAG <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib45" title="">2025c</a>)</cite> adopted Monte Carlo Tree Search (MCTS) to approximate the value of each step. StepSearch <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib38" title="">2025</a>)</cite> leveraged a memory vector of retrieved documents, supervising intermediate steps based on their maximum similarity to ground-truth evidence. GiGPO <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib7" title="">2025</a>)</cite> introduced anchor-based grouping to estimate relative advantages for actions originating from the same anchor state. While these methods provide denser supervision than outcome-only rewards, they either rely on external oracle knowledge or suffer from limited stability and scalability, leaving room for more intrinsic and generalizable process-level reward designs.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion, Limitations and Future Work</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p">In this work, we propose IGPO, a simple and effective reinforcement learning framework for training multi-turn LLM-based agents. By providing intrinsic, ground-truth-aware supervision at every turn while preserving alignment with the final objective, IGPO delivers dense and stable training signals. Extensive experiments across in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines, achieving higher accuracy and better sample efficiency, particularly for smaller models where sparse rewards are most problematic.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p">However, our approach still relies on the availability of ground-truth answers, which limits its applicability in open-ended settings. In future work, we plan to extend IGPO to broader agentic scenarios beyond search, including tasks without explicit supervision.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmadian et al. (2024)</span>
<span class="ltx_bibblock">
Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker.

</span>
<span class="ltx_bibblock">Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.14740</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2025a)</span>
<span class="ltx_bibblock">
Kedi Chen, Dezhao Ruan, Yuhao Dan, Yaoting Wang, Siyu Yan, Xuecheng Wu, Yinqi Zhang, Qin Chen, Jie Zhou, Liang He, Biqing Qi, Linyang Li, Qipeng Guo, Xiaoming Shi, and Wei Zhang.

</span>
<span class="ltx_bibblock">A survey of inductive reasoning for large language models, 2025a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2510.10182" title="">https://arxiv.org/abs/2510.10182</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2025b)</span>
<span class="ltx_bibblock">
Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen Zhang, Huajun Chen, Fan Yang, et al.

</span>
<span class="ltx_bibblock">Learning to reason with search for llms via reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.19470</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2025a)</span>
<span class="ltx_bibblock">
Yuqin Dai, Guoqing Wang, Yuan Wang, Kairan Dou, Kaichen Zhou, Zhanwei Zhang, Shuo Yang, Fei Tang, Jun Yin, Pengyu Zeng, et al.

</span>
<span class="ltx_bibblock">Evinote-rag: Enhancing rag models via answer-supportive evidence notes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2509.00877</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2025b)</span>
<span class="ltx_bibblock">
Yuqin Dai, Shuo Yang, Guoqing Wang, Yong Deng, Zhanwei Zhang, Jun Yin, Pengyu Zeng, Zhenzhe Ying, Changhua Meng, Can Yi, et al.

</span>
<span class="ltx_bibblock">Careful queries, credible results: Teaching rag models advanced web search tools with reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2508.07956</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2025)</span>
<span class="ltx_bibblock">
Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, Yang Qin, Yuan Wang, Quanxing Zha, Sunhao Dai, and Changhua Meng.

</span>
<span class="ltx_bibblock">Atom-searcher: Enhancing agentic deep research via fine-grained atomic thought reward.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2508.12800</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2025)</span>
<span class="ltx_bibblock">
Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An.

</span>
<span class="ltx_bibblock">Group-in-group policy optimization for llm agent training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.10978</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. (2025)</span>
<span class="ltx_bibblock">
Zeyu Gan, Yun Liao, and Yong Liu.

</span>
<span class="ltx_bibblock">Rethinking external slow-thinking: From snowball errors to probability of correct reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Forty-second International Conference on Machine Learning</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.10997</em>, 2(1), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2025)</span>
<span class="ltx_bibblock">
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al.

</span>
<span class="ltx_bibblock">Deepseek-r1 incentivizes reasoning in llms through reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature</em>, 645(8081):633–638, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gutierrez et al. (2023)</span>
<span class="ltx_bibblock">
Carlos I Gutierrez, Anthony Aguirre, Risto Uuk, Claire C Boine, and Matija Franklin.

</span>
<span class="ltx_bibblock">A proposal for a definition of general purpose artificial intelligence systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Digital Society</em>, 2(3):36, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2020)</span>
<span class="ltx_bibblock">
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa.

</span>
<span class="ltx_bibblock">Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 28th International Conference on Computational Linguistics</em>, pp.  6609–6625, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu (2025)</span>
<span class="ltx_bibblock">
Jian Hu.

</span>
<span class="ltx_bibblock">Reinforce++: A simple and efficient approach for aligning large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2501.03262</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2025)</span>
<span class="ltx_bibblock">
Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, et al.

</span>
<span class="ltx_bibblock">Deep research agents: A systematic examination and roadmap.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2506.18096</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaech et al. (2024)</span>
<span class="ltx_bibblock">
Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al.

</span>
<span class="ltx_bibblock">Openai o1 system card.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2412.16720</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2025)</span>
<span class="ltx_bibblock">
Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han.

</span>
<span class="ltx_bibblock">Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.00223</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2025)</span>
<span class="ltx_bibblock">
Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han.

</span>
<span class="ltx_bibblock">Search-r1: Training llms to reason and leverage search engines with reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.09516</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1705.03551</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kool et al. (2019)</span>
<span class="ltx_bibblock">
Wouter Kool, Herke van Hoof, and Max Welling.

</span>
<span class="ltx_bibblock">Buy 4 reinforce samples, get a baseline for free!

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 7:453–466, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2025a)</span>
<span class="ltx_bibblock">
Long Li, Jiaran Hao, Jason Klein Liu, Zhijian Zhou, Xiaoyu Tan, Wei Chu, Zhe Wang, Shirui Pan, Chao Qu, and Yuan Qi.

</span>
<span class="ltx_bibblock">The choice of divergence: A neglected key to mitigating diversity collapse in reinforcement learning with verifiable reward.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2509.07430</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2025b)</span>
<span class="ltx_bibblock">
Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou.

</span>
<span class="ltx_bibblock">Search-o1: Agentic search-enhanced large reasoning models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2501.05366</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2025c)</span>
<span class="ltx_bibblock">
Xuefeng Li, Haoyang Zou, and Pengfei Liu.

</span>
<span class="ltx_bibblock">Torl: Scaling tool-integrated rl.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.23383</em>, 2025c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen et al. (2022)</span>
<span class="ltx_bibblock">
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi.

</span>
<span class="ltx_bibblock">When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.10511</em>, 7, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ning et al. (2025)</span>
<span class="ltx_bibblock">
Liangbo Ning, Ziran Liang, Zhuohang Jiang, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei, Shanru Lin, Hui Liu, Philip S Yu, et al.

</span>
<span class="ltx_bibblock">A survey of webagents: Towards next-generation ai agents for web automation with large foundation models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2</em>, pp.  6140–6150, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. (2022)</span>
<span class="ltx_bibblock">
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.

</span>
<span class="ltx_bibblock">Measuring and narrowing the compositionality gap in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.03350</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. (2024)</span>
<span class="ltx_bibblock">
Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al.

</span>
<span class="ltx_bibblock">Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2411.02337</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. (2025)</span>
<span class="ltx_bibblock">
Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Tool learning with large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Frontiers of Computer Science</em>, 19(8):198343, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qwen et al. (2025)</span>
<span class="ltx_bibblock">
Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu.

</span>
<span class="ltx_bibblock">Qwen2.5 technical report, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman et al. (2017)</span>
<span class="ltx_bibblock">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.

</span>
<span class="ltx_bibblock">Proximal policy optimization algorithms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1707.06347</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2024)</span>
<span class="ltx_bibblock">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.03300</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et al. (2025)</span>
<span class="ltx_bibblock">
Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu.

</span>
<span class="ltx_bibblock">Hybridflow: A flexible and efficient rlhf framework.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Twentieth European Conference on Computer Systems</em>, EuroSys ’25, pp.  1279–1297. ACM, March 2025.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3689031.3696075</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2025a)</span>
<span class="ltx_bibblock">
Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">R1-searcher: Incentivizing the search capability in llms via reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.05592</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2025b)</span>
<span class="ltx_bibblock">
Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">R1-searcher++: Incentivizing the dynamic knowledge acquisition of llms via reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.17005</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al. (2022)</span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Musique: Multihop questions via single-hop question composition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 10:539–554, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.

</span>
<span class="ltx_bibblock">Math-shepherd: Verify and reinforce llms step-by-step without human annotations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.08935</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024a)</span>
<span class="ltx_bibblock">
Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, et al.

</span>
<span class="ltx_bibblock">Searching for best practices in retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.01219</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2025)</span>
<span class="ltx_bibblock">
Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, and Yichao Wu.

</span>
<span class="ltx_bibblock">Stepsearch: Igniting llms search ability via step-wise proximal policy optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.15107</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024b)</span>
<span class="ltx_bibblock">
Ziting Wang, Haitao Yuan, Wei Dong, Gao Cong, and Feifei Li.

</span>
<span class="ltx_bibblock">Corag: A cost-constrained retrieval optimization system for retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2411.00744</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 35:24824–24837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning.

</span>
<span class="ltx_bibblock">Hotpotqa: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.09600</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2025)</span>
<span class="ltx_bibblock">
Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al.

</span>
<span class="ltx_bibblock">Dapo: An open-source llm reinforcement learning system at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.14476</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2025a)</span>
<span class="ltx_bibblock">
Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, et al.

</span>
<span class="ltx_bibblock">The landscape of agentic reinforcement learning for llms: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2509.02547</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2025b)</span>
<span class="ltx_bibblock">
Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, et al.

</span>
<span class="ltx_bibblock">From web search towards agentic deep research: Incentivizing search with reasoning agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2506.18959</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2025c)</span>
<span class="ltx_bibblock">
Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, et al.

</span>
<span class="ltx_bibblock">Process vs. outcome reward: Which is better for agentic rag reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.14069</em>, 2025c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2025a)</span>
<span class="ltx_bibblock">
Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al.

</span>
<span class="ltx_bibblock">Group sequence policy optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2507.18071</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2024)</span>
<span class="ltx_bibblock">
Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, et al.

</span>
<span class="ltx_bibblock">Openresearcher: Unleashing ai for accelerated scientific research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2408.06941</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2025b)</span>
<span class="ltx_bibblock">
Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu.

</span>
<span class="ltx_bibblock">Deepresearcher: Scaling deep research via reinforcement learning in real-world environments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2504.03160</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuo et al. (2025)</span>
<span class="ltx_bibblock">
Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al.

</span>
<span class="ltx_bibblock">Ttrl: Test-time reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2504.16084</em>, 2025.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Theoretical Analysis</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p">The theoretical analysis here provides an intuitive support for the efficacy of our proposed method by addressing the limitations of sparse outcome rewards in multi-turn agents.
Specifically, the theory establishes a crucial link: maximizing the process reward (IGPO’s objective) is equivalent to minimizing the upper bound on the undesirable accumulation of snowball errors during the reasoning process.
This minimization, in turn, systematically lowers the theoretical minimum for the final answer error rate, thus providing a fundamental guarantee that IGPO’s dense, turn-level signals lead to more confident and successful reasoning trajectories.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Notations</span>.
Let <math alttext="E_{\text{final}}" class="ltx_Math" display="inline" id="A1.p2.m1" intent=":literal"><semantics><msub><mi>E</mi><mtext>final</mtext></msub><annotation encoding="application/x-tex">E_{\text{final}}</annotation></semantics></math> be the event that the agent’s generated final answer does not match the ground truth answer.
Its probability is denoted by <math alttext="\mathbb{P}(E_{\text{final}})" class="ltx_Math" display="inline" id="A1.p2.m2" intent=":literal"><semantics><mrow><mi>ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>E</mi><mtext>final</mtext></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{P}(E_{\text{final}})</annotation></semantics></math>, i.e., the error rate.
For each turn <math alttext="t" class="ltx_Math" display="inline" id="A1.p2.m3" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> , denote the observed response <span class="ltx_text ltx_font_typewriter">[think]</span>, <span class="ltx_text ltx_font_typewriter">[tool call]</span> as <math alttext="\mathcal{R}_{t}" class="ltx_Math" display="inline" id="A1.p2.m4" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathcal{R}_{t}</annotation></semantics></math>. We also posit that there is an unobservable, abstract thinking step <math alttext="\mathcal{I}_{t}" class="ltx_Math" display="inline" id="A1.p2.m5" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathcal{I}_{t}</annotation></semantics></math> that underlies the generation of <math alttext="\mathcal{R}_{t}" class="ltx_Math" display="inline" id="A1.p2.m6" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathcal{R}_{t}</annotation></semantics></math>.
Let <math alttext="R_{\text{process}}^{(t)}" class="ltx_Math" display="inline" id="A1.p2.m7" intent=":literal"><semantics><msubsup><mi>R</mi><mtext>process</mtext><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">R_{\text{process}}^{(t)}</annotation></semantics></math> be the process reward, which is a dense reward signal received at each turn of the interaction. It is defined as the information gain about the ground truth answer, which is calculated as the increase in the log-probability of the correct answer from the previous state to the current state.
Then, the total process reward <math alttext="R_{\text{total}}=\sum_{t=1}^{T-1}\mathbb{E}[R_{\text{process}}^{(t)}]" class="ltx_Math" display="inline" id="A1.p2.m8" intent=":literal"><semantics><mrow><msub><mi>R</mi><mtext>total</mtext></msub><mo rspace="0.111em">=</mo><mrow><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msubsup><mi>R</mi><mtext>process</mtext><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">R_{\text{total}}=\sum_{t=1}^{T-1}\mathbb{E}[R_{\text{process}}^{(t)}]</annotation></semantics></math> is the cumulative sum of all process rewards over a complete trajectory or episode. The expectation is taken over the thinking step and observed response.
The training objective of the policy is to maximize this total reward.</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="A1.Thmtheorem1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Definition A.1</span></span><span class="ltx_text ltx_font_bold"> </span>(Snowball Error in Multi-turn Agentic RL)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="A1.Thmtheorem1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Consistent with <cite class="ltx_cite ltx_citemacro_cite">Gan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib8" title="">2025</a>)</cite>, we define the information loss at turn <math alttext="T" class="ltx_Math" display="inline" id="A1.Thmtheorem1.p1.m1" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> as the conditional entropy <math alttext="\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})" class="ltx_Math" display="inline" id="A1.Thmtheorem1.p1.m2" intent=":literal"><semantics><mrow><mi>Ent</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})</annotation></semantics></math>.
Consider the non-trivial case where <math alttext="\left|\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})\right|" class="ltx_Math" display="inline" id="A1.Thmtheorem1.p1.m3" intent=":literal"><semantics><mrow><mo>|</mo><mrow><mi>Ent</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>t</mi></msub><mo lspace="0em" rspace="0.167em" separator="true">∣</mo><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>|</mo></mrow><annotation encoding="application/x-tex">\left|\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})\right|</annotation></semantics></math> is bounded.
The cumulative snowball error up to turn <math alttext="T" class="ltx_Math" display="inline" id="A1.Thmtheorem1.p1.m4" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> is the sum of these losses:</span></p>
<table class="ltx_equation ltx_eqn_table" id="A1.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})\triangleq\sum_{t=1}^{T-1}\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})" class="ltx_Math" display="block" id="A1.E9.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Ent</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo fence="false">|</mo><mi class="ltx_font_mathcaligraphic">ℛ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">≜</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mi>Ent</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})\triangleq\sum_{t=1}^{T-1}\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A1.p3">
<p class="ltx_p">This quantity measures the aggregate uncertainty and ambiguity accumulated throughout the reasoning trajectory before the final answer is produced.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p4">
<p class="ltx_p">Next, we connect the cumulative snowball error to the agent’s final performance. It indicates the fundamental limitation of multi-turn agentic RL pipeline caused by snowball error.</p>
</div>
<div class="ltx_theorem ltx_theorem_lemma" id="A1.Thmtheorem2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Lemma A.2</span></span><span class="ltx_text ltx_font_bold"> </span>(Lower bound of error rate)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="A1.Thmtheorem2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The probability of a final answer error, <math alttext="\mathbb{P}(E_{\text{final}})" class="ltx_Math" display="inline" id="A1.Thmtheorem2.p1.m1" intent=":literal"><semantics><mrow><mi>ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>E</mi><mtext class="ltx_mathvariant_italic">final</mtext></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{P}(E_{\text{final}})</annotation></semantics></math>, is lower-bounded by the cumulative snowball error accumulated during the reasoning process:</span></p>
<table class="ltx_equation ltx_eqn_table" id="A1.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{P}(E_{\text{final}})=\Omega\left(\frac{\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})}{T-1}\right)-C_{\text{const}}." class="ltx_Math" display="block" id="A1.E10.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>E</mi><mtext class="ltx_mathvariant_italic">final</mtext></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi mathvariant="normal">Ω</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mfrac><mrow><msub><mi>Ent</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo fence="false">|</mo><mi class="ltx_font_mathcaligraphic">ℛ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>)</mo></mrow></mrow><mo>−</mo><msub><mi>C</mi><mtext class="ltx_mathvariant_italic">const</mtext></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbb{P}(E_{\text{final}})=\Omega\left(\frac{\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})}{T-1}\right)-C_{\text{const}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_italic">where
<math alttext="C_{\text{const}}" class="ltx_Math" display="inline" id="A1.Thmtheorem2.p1.m2" intent=":literal"><semantics><msub><mi>C</mi><mtext class="ltx_mathvariant_italic">const</mtext></msub><annotation encoding="application/x-tex">C_{\text{const}}</annotation></semantics></math> is a small positive constant.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof Sketch.</h6>
<div class="ltx_para" id="A1.p5">
<p class="ltx_p">This result is strongly motivated by Theorem 3.3 from <cite class="ltx_cite ltx_citemacro_cite">Gan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib8" title="">2025</a>)</cite>. We treat the generation of the final answer at turn <math alttext="T" class="ltx_Math" display="inline" id="A1.p5.m1" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> as the final step of a multi-step reasoning process. The quality of this final step is conditioned on the information accumulated over the previous <math alttext="T-1" class="ltx_Math" display="inline" id="A1.p5.m2" intent=":literal"><semantics><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T-1</annotation></semantics></math> turns. The theorem from (Gan et al., 2025) states that the error probability of any step is lower-bounded by the average snowball error accumulated up to that point. Applying this principle to the final step (<math alttext="t=T" class="ltx_Math" display="inline" id="A1.p5.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">t=T</annotation></semantics></math>) yields the stated result.
∎</p>
</div>
</div>
<div class="ltx_theorem ltx_theorem_assumption" id="A1.Thmtheorem3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Assumption A.3</span></span><span class="ltx_text ltx_font_bold"> </span>(Monotonic Reward-Information Loss Link)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="A1.Thmtheorem3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">The expected process reward at any turn <math alttext="H" class="ltx_Math" display="inline" id="A1.Thmtheorem3.p1.m1" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>, <math alttext="\mathbb{E}[R_{\text{process}}^{(t)}]" class="ltx_Math" display="inline" id="A1.Thmtheorem3.p1.m2" intent=":literal"><semantics><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msubsup><mi>R</mi><mtext class="ltx_mathvariant_italic">process</mtext><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}[R_{\text{process}}^{(t)}]</annotation></semantics></math>, is monotonically non-increasing with respect to the information loss at that turn, <math alttext="\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})" class="ltx_Math" display="inline" id="A1.Thmtheorem3.p1.m3" intent=":literal"><semantics><mrow><mi>Ent</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})</annotation></semantics></math>.
We assume there exists a bounded and monotonically non-increasing convex function <math alttext="f:\mathbb{R}_{+}\to\mathbb{R}" class="ltx_Math" display="inline" id="A1.Thmtheorem3.p1.m4" intent=":literal"><semantics><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msub><mi>ℝ</mi><mo>+</mo></msub><mo stretchy="false">→</mo><mi>ℝ</mi></mrow></mrow><annotation encoding="application/x-tex">f:\mathbb{R}_{+}\to\mathbb{R}</annotation></semantics></math> such that:</span></p>
<table class="ltx_equation ltx_eqn_table" id="A1.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{E}[R_{\text{process}}^{(t)}|\mathcal{I}_{t},\mathcal{R}_{t}]\leq f\left(\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})\right)." class="ltx_Math" display="block" id="A1.E11.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msubsup><mi>R</mi><mtext class="ltx_mathvariant_italic">process</mtext><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo fence="false">|</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>t</mi></msub><mo>,</mo><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>≤</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>Ent</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}[R_{\text{process}}^{(t)}|\mathcal{I}_{t},\mathcal{R}_{t}]\leq f\left(\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_theorem ltx_theorem_remark" id="Thmremarkx1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Remark</span></span><span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="Thmremarkx1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">As the information loss <math alttext="\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})" class="ltx_Math" display="inline" id="Thmremarkx1.p1.m1" intent=":literal"><semantics><mrow><mi>Ent</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})</annotation></semantics></math> at turn <math alttext="t" class="ltx_Math" display="inline" id="Thmremarkx1.p1.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> increases, the expected total information loss tends to decreases and asymptotically approaches a relatively small value, which is characterized by the convex nature of function <math alttext="f" class="ltx_Math" display="inline" id="Thmremarkx1.p1.m3" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A1.p6">
<p class="ltx_p">This assumption leads to the following result, demonstrating that optimizing for process rewards implicitly constrains the accumulation of snowball errors.
We first formalize the intuition that a clearer reasoning step (lower information loss) is a prerequisite for a high-quality query, which in turn yields a higher expected process reward.</p>
</div>
<div class="ltx_theorem ltx_theorem_theorem" id="A1.Thmtheorem4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold">Theorem A.4</span></span><span class="ltx_text ltx_font_bold"> </span>(Process Reward as a Bound on Snowball Error)<span class="ltx_text ltx_font_bold">.</span>
</h6>
<div class="ltx_para" id="A1.Thmtheorem4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Under Assumption <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A1.Thmtheorem3" title="Assumption A.3 (Monotonic Reward-Information Loss Link). ‣ Appendix A Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">A.3</span></a>, the expected cumulative snowball error is upper bounded by</span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A6.EGx1">
<tbody id="A1.E12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}[\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})]=\mathcal{O}(1)-\Omega\left(R_{\text{total}}\right)." class="ltx_Math" display="inline" id="A1.E12.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>Ent</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo fence="false">|</mo><mi class="ltx_font_mathcaligraphic">ℛ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒪</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><mi mathvariant="normal">Ω</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msub><mi>R</mi><mtext class="ltx_mathvariant_italic">total</mtext></msub><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}[\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})]=\mathcal{O}(1)-\Omega\left(R_{\text{total}}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A1.p7">
<p class="ltx_p">Theorem <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A1.Thmtheorem4" title="Theorem A.4 (Process Reward as a Bound on Snowball Error). ‣ Appendix A Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">A.4</span></a> establishes that maximizing the process reward is mathematically coupled with minimizing an upper bound on the cumulative snowball error.
The combination of Theorem <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A1.Thmtheorem4" title="Theorem A.4 (Process Reward as a Bound on Snowball Error). ‣ Appendix A Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">A.4</span></a> and Lemma <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A1.Thmtheorem2" title="Lemma A.2 (Lower bound of error rate). ‣ Appendix A Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">A.2</span></a> provides a complete, end-to-end theoretical justification for the efficacy of our proposed process reward mechanism. The logical chain is as follows:</p>
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Maximizing the process reward</span> (our algorithm’s objective) forces the agent to <span class="ltx_text ltx_font_bold">minimize an upper bound on the cumulative snowball error</span> (Theorem <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A1.Thmtheorem4" title="Theorem A.4 (Process Reward as a Bound on Snowball Error). ‣ Appendix A Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">A.4</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A1.I1.i2.p1">
<p class="ltx_p">Minimizing the cumulative snowball error, in turn, <span class="ltx_text ltx_font_bold">lowers the theoretical minimum for the final error rate</span>, thereby systematically increasing the probability of task success (Lemma <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A1.Thmtheorem2" title="Lemma A.2 (Lower bound of error rate). ‣ Appendix A Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">A.2</span></a>).</p>
</div>
</li>
</ul>
<p class="ltx_p">In conclusion, the turn-level process reward is not merely an engineering heuristic; it is a theoretically grounded mechanism that fundamentally addresses the problem of error accumulation in multi-step reasoning. By providing a dense, immediate signal for reasoning clarity, it transforms the intractable problem of sparse-reward, long-horizon exploration into a series of manageable, short-horizon sub-problems, each aimed at maximizing immediate information gain. This explains the significant gains in training efficiency and final performance observed in our experiments.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Proof for Theoretical Analysis</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Proof of Lemma <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A1.Thmtheorem2" title="Lemma A.2 (Lower bound of error rate). ‣ Appendix A Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">A.2</span></a>
</h3>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p">We achieve this by applying Theorem 3.3 from <cite class="ltx_cite ltx_citemacro_cite">Gan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib8" title="">2025</a>)</cite> to the final decision-making step of the agent.
In particular,</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{P}(E_{\text{final}})\geq\frac{\frac{\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})}{T-1}-C_{1}}{\log(|\mathcal{A}_{\text{final}}|-1)}," class="ltx_Math" display="block" id="A2.E13.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>ℙ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>E</mi><mtext>final</mtext></msub><mo stretchy="false">)</mo></mrow></mrow><mo>≥</mo><mfrac><mrow><mfrac><mrow><msub><mi>Ent</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo fence="false">|</mo><mi class="ltx_font_mathcaligraphic">ℛ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>−</mo><msub><mi>C</mi><mn>1</mn></msub></mrow><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mtext>final</mtext></msub><mo stretchy="false">|</mo></mrow><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbb{P}(E_{\text{final}})\geq\frac{\frac{\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})}{T-1}-C_{1}}{\log(|\mathcal{A}_{\text{final}}|-1)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="|\mathcal{A}_{\text{final}}|" class="ltx_Math" display="inline" id="A2.SS1.p1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mtext>final</mtext></msub><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|\mathcal{A}_{\text{final}}|</annotation></semantics></math> is the cardinality of the final answer space and <math alttext="C_{1}" class="ltx_Math" display="inline" id="A2.SS1.p1.m2" intent=":literal"><semantics><msub><mi>C</mi><mn>1</mn></msub><annotation encoding="application/x-tex">C_{1}</annotation></semantics></math> is a small positive constant analogous to <math alttext="\operatorname{Ent}_{b}(e_{t})" class="ltx_Math" display="inline" id="A2.SS1.p1.m3" intent=":literal"><semantics><mrow><msub><mi>Ent</mi><mi>b</mi></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>e</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Ent}_{b}(e_{t})</annotation></semantics></math> in <cite class="ltx_cite ltx_citemacro_cite">Gan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib8" title="">2025</a>)</cite>.
Since <math alttext="\log(|\mathcal{A}_{\text{final}}|-1)" class="ltx_Math" display="inline" id="A2.SS1.p1.m4" intent=":literal"><semantics><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mtext>final</mtext></msub><mo stretchy="false">|</mo></mrow><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log(|\mathcal{A}_{\text{final}}|-1)</annotation></semantics></math> and <math alttext="C_{1}" class="ltx_Math" display="inline" id="A2.SS1.p1.m5" intent=":literal"><semantics><msub><mi>C</mi><mn>1</mn></msub><annotation encoding="application/x-tex">C_{1}</annotation></semantics></math> are constant, <math alttext="\frac{\frac{\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})}{T-1}-C_{1}}{\log(|\mathcal{A}_{\text{final}}|-1)}" class="ltx_Math" display="inline" id="A2.SS1.p1.m6" intent=":literal"><semantics><mfrac><mrow><mfrac><mrow><msub><mi>Ent</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo fence="false">|</mo><mi class="ltx_font_mathcaligraphic">ℛ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>−</mo><msub><mi>C</mi><mn>1</mn></msub></mrow><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mtext>final</mtext></msub><mo stretchy="false">|</mo></mrow><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac><annotation encoding="application/x-tex">\frac{\frac{\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})}{T-1}-C_{1}}{\log(|\mathcal{A}_{\text{final}}|-1)}</annotation></semantics></math> simplifies to a form that is asymptotically dominated by the variable term.
Therefore, the right-hand side of the inequality can be expressed in terms of the lower bound symbol <math alttext="\Omega" class="ltx_Math" display="inline" id="A2.SS1.p1.m7" intent=":literal"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math> as <math alttext="\Omega\left(\frac{\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})}{T-1}\right)-C_{\text{const}}" class="ltx_Math" display="inline" id="A2.SS1.p1.m8" intent=":literal"><semantics><mrow><mrow><mi mathvariant="normal">Ω</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mfrac><mrow><msub><mi>Ent</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo fence="false">|</mo><mi class="ltx_font_mathcaligraphic">ℛ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>)</mo></mrow></mrow><mo>−</mo><msub><mi>C</mi><mtext>const</mtext></msub></mrow><annotation encoding="application/x-tex">\Omega\left(\frac{\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})}{T-1}\right)-C_{\text{const}}</annotation></semantics></math>, which completes the proof.
∎</p>
</div>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Proof of Theorem <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A1.Thmtheorem4" title="Theorem A.4 (Process Reward as a Bound on Snowball Error). ‣ Appendix A Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">A.4</span></a>
</h3>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p">According to the nature of <math alttext="f" class="ltx_Math" display="inline" id="A2.SS2.p1.m1" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> and the fact that there exist constants <math alttext="C_{\max}" class="ltx_Math" display="inline" id="A2.SS2.p1.m2" intent=":literal"><semantics><msub><mi>C</mi><mi>max</mi></msub><annotation encoding="application/x-tex">C_{\max}</annotation></semantics></math> and <math alttext="\beta" class="ltx_Math" display="inline" id="A2.SS2.p1.m3" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> such that for all non-negative bounded <math alttext="x" class="ltx_Math" display="inline" id="A2.SS2.p1.m4" intent=":literal"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, there holds <math alttext="f(x)\leq C_{\max}-\beta x" class="ltx_Math" display="inline" id="A2.SS2.p1.m5" intent=":literal"><semantics><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≤</mo><mrow><msub><mi>C</mi><mi>max</mi></msub><mo>−</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><mi>x</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">f(x)\leq C_{\max}-\beta x</annotation></semantics></math>.
Therefore, by taking the expectation over Assumption <a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#A1.Thmtheorem3" title="Assumption A.3 (Monotonic Reward-Information Loss Link). ‣ Appendix A Theoretical Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">A.3</span></a> and summing across all turns from <math alttext="t=1" class="ltx_Math" display="inline" id="A2.SS2.p1.m6" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t=1</annotation></semantics></math> to <math alttext="T-1" class="ltx_Math" display="inline" id="A2.SS2.p1.m7" intent=":literal"><semantics><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">T-1</annotation></semantics></math>, we have</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A6.EGx2">
<tbody id="A2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle R_{\text{total}}=\sum_{t=1}^{T-1}\mathbb{E}[R_{\text{process}}^{(t)}]" class="ltx_Math" display="inline" id="A2.Ex1.m1" intent=":literal"><semantics><mrow><msub><mi>R</mi><mtext>total</mtext></msub><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msubsup><mi>R</mi><mtext>process</mtext><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle R_{\text{total}}=\sum_{t=1}^{T-1}\mathbb{E}[R_{\text{process}}^{(t)}]</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\leq\sum_{t=1}^{T-1}\mathbb{E}[f\left(\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})\right)]" class="ltx_Math" display="inline" id="A2.Ex1.m2" intent=":literal"><semantics><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>Ent</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\leq\sum_{t=1}^{T-1}\mathbb{E}[f\left(\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})\right)]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\leq\sum_{t=1}^{T-1}\mathbb{E}[C_{\max}-\beta\cdot\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})]" class="ltx_Math" display="inline" id="A2.Ex2.m1" intent=":literal"><semantics><mrow><mi></mi><mo>≤</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>C</mi><mi>max</mi></msub><mo>−</mo><mrow><mi>β</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><mrow><mi>Ent</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\leq\sum_{t=1}^{T-1}\mathbb{E}[C_{\max}-\beta\cdot\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(T-1)C_{\max}-\beta\sum_{t=1}^{T-1}\mathbb{E}[\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})]" class="ltx_Math" display="inline" id="A2.Ex3.m1" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>C</mi><mi>max</mi></msub></mrow><mo>−</mo><mrow><mi>β</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover></mstyle><mrow><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><mi>Ent</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>t</mi></msub><mo fence="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=(T-1)C_{\max}-\beta\sum_{t=1}^{T-1}\mathbb{E}[\operatorname{Ent}(\mathcal{I}_{t}|\mathcal{R}_{t})]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="A2.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(T-1)C_{\max}-\beta\ \mathbb{E}[\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})]." class="ltx_Math" display="inline" id="A2.Ex4.m1" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msub><mi>C</mi><mi>max</mi></msub></mrow><mo>−</mo><mrow><mi>β</mi><mo lspace="0.500em" rspace="0em">​</mo><mi>𝔼</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>Ent</mi><mrow><mi></mi><mo>&lt;</mo><mi>T</mi></mrow></msub><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mrow><mi class="ltx_font_mathcaligraphic">ℐ</mi><mo fence="false">|</mo><mi class="ltx_font_mathcaligraphic">ℛ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=(T-1)C_{\max}-\beta\ \mathbb{E}[\operatorname{Ent}_{&lt;T}(\mathcal{I}|\mathcal{R})].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Rearranging terms yields the final result.
∎</p>
</div>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>More Implementation Details</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p">All our training experiments are conducted on 8 × NVIDIA A100-80G GPUs. The detailed hyperparameter settings are provided in  <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#A3.T4" title="Table 4 ‣ Appendix C More Implementation Details ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Table 4</span></a>. Unless otherwise specified, all experiments are based on this configuration.</p>
</div>
<figure class="ltx_table" id="A3.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Training hyperparameters.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Training hyperparameters</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Value</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t">Training Batch Size</td>
<td class="ltx_td ltx_align_center ltx_border_t">32</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Mini-Batch Size</td>
<td class="ltx_td ltx_align_center">32</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Infer Tensor Model Parallel Size</td>
<td class="ltx_td ltx_align_center">1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Sequence Parallel Size</td>
<td class="ltx_td ltx_align_center">4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Max Prompt Length</td>
<td class="ltx_td ltx_align_center">30767</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Max Response Length</td>
<td class="ltx_td ltx_align_center">2000</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Actor Learning Rate</td>
<td class="ltx_td ltx_align_center">1e-6</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Rollout Temperature</td>
<td class="ltx_td ltx_align_center">1.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Rollout Group Size</td>
<td class="ltx_td ltx_align_center">16</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Max Turn Call Turns</td>
<td class="ltx_td ltx_align_center">10</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b">KL-Divergence loss coefficient</td>
<td class="ltx_td ltx_align_center ltx_border_b">0.001</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>More Discussion and
Experimental Analysis</h2>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Comparison with Other Process-reward Methods</h3>
<div class="ltx_para ltx_noindent" id="A4.SS1.p1">
<p class="ltx_p">In addition to its obvious performance advantages, we also conduct a deeper analysis of IGPO’s superiority in terms of algorithmic characteristics compared to other process-reward-based agentic RL algorithms. We first introduce other existing process-reward-based agentic RL algorithms:</p>
<ul class="ltx_itemize" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">ReasoningRAG</span>. The main contribution of this work is the proposal of a step-level data labeling strategy based on MCTS. Subsequently, the DPO algorithm is used to optimize the agent’s policy on the labeled step-level dataset. The main limitations of this method are: (1) the data labeling process relies on MCTS, which is inefficient, and when the number of samples is insufficient, it is difficult to accurately estimate the value of each step; (2) the off-policy optimization based on DPO is less effective than on-policy algorithms.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">StepSearch</span>. StepSearch constructs turn-level supervision signals by pre-defining golden search keywords and golden tool responses, and adopts an on-policy optimization approach. Although it shifts from off-policy to on-policy, the annotation process is resource-intensive and prone to annotator bias (whether from humans or LLMs).</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A4.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">GiGPO</span>. GiGPO introduces a step-level grouping strategy based on anchor states and performs fine-grained advantage estimation within each step-level group. Although this provides a novel solution, it essentially still relies on the Monte Carlo assumption. When the number of anchor states is insufficient, it is often difficult to accurately estimate their value, which in turn leads to biased advantage estimation.</p>
</div>
</li>
</ul>
<p class="ltx_p">The proposed IGPO effectively addresses the aforementioned limitations. Starting from the on-policy GRPO setting (where rollout data are used for a single parameter update), it employs an information-gain–based incremental reward construction strategy that requires no annotation and does not rely on Monte Carlo. Moreover, the incorporation of ground-truth awareness substantially reduces bias. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#A4.T5" title="Table 5 ‣ D.1 Comparison with Other Process-reward Methods ‣ Appendix D More Discussion and Experimental Analysis ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Table 5</span></a> provides a detailed comparison highlighting the advantages of IGPO over other algorithms.</p>
</div>
<figure class="ltx_table" id="A4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison between various process reward methods.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Algorithm</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">On-Policy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Explicit Labeling-Free</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Monte Carlo–Free</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Introduces No Bias</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">ReasoningRAG</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">No</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">No</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.4pt;padding-right:3.4pt;">Sample-size Dependent</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.4pt;padding-right:3.4pt;">StepSearch</th>
<td class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">Yes</td>
<td class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">No</td>
<td class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">Yes</td>
<td class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">No</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.4pt;padding-right:3.4pt;">GiGPO</th>
<td class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">Yes</td>
<td class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">Yes</td>
<td class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">No</td>
<td class="ltx_td ltx_align_center" style="padding-left:3.4pt;padding-right:3.4pt;">Sample-size Dependent</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">IGPO</th>
<td class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.4pt;padding-right:3.4pt;">Yes</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Case Study</h3>
<figure class="ltx_figure" id="A4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="575" id="A4.F6.g1" src="x12.png" width="760"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Case study showing a scenario where the final answer is incorrect but contains a single correct retrieval turn. IGPO provides a process reward for this turn, improving token utilization.</figcaption>
</figure>
<figure class="ltx_figure" id="A4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="801" id="A4.F7.g1" src="x13.png" width="760"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Case study illustrating a situation where the first round of retrieval failed, but the second and third rounds successfully located the correct evidence and produced the right answer. In this case, IGPO imposes a penalty on the erroneous retrieval in the first round.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Comparison between GRPO and IGPO</h2>
<div class="ltx_para ltx_noindent" id="A5.p1">
<p class="ltx_p">Algorithm <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#alg1" title="Algorithm 1 ‣ Appendix E Comparison between GRPO and IGPO ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the algorithmic flow of IGPO (right) and GRPO (left). The key steps corresponding to each algorithm are highlighted with the same color font to visually highlight the differences: yellow for <span class="ltx_text" style="--ltx-fg-color:#F0AD00;">reward calculation</span>, green for <span class="ltx_text" style="--ltx-fg-color:#009E73;">advantage estimation</span>, blue for <span class="ltx_text" style="--ltx-fg-color:#0072B2;">advantage accumulation and assignment</span>, and purple for <span class="ltx_text" style="--ltx-fg-color:#AA3377;">policy optimization</span>. In terms of reward calculation, IGPO constructs dense turn-level rewards through incremental information gain. For advantage estimation, both IGPO and GRPO use Group Normalization. Regarding advantage accumulation and allocation, GRPO directly assigns the outcome-based advantage to all tokens of the current output, while IGPO further computes the cumulative discounted advantage, capturing long-horizon information and performing turn-level reward assignment. In policy optimization, IGPO achieves more efficient and effective optimization by maximizing the turn-level cumulative discounted advantages.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 1</span> </span> <span class="ltx_text ltx_font_bold">GRPO</span> vs. <span class="ltx_text ltx_font_bold">IGPO</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:212.5pt;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">GRPO</span></p>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg1.l0">
<span class="ltx_tag ltx_tag_listingline">0:</span> initial policy <math alttext="\pi_{\theta_{\text{init}}}" class="ltx_Math" display="inline" id="alg1.l0.m1" intent=":literal"><semantics><msub><mi>π</mi><msub><mi>θ</mi><mtext>init</mtext></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{\text{init}}}</annotation></semantics></math>; task prompts <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="alg1.l0.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation></semantics></math>; hyperparameters <math alttext="\epsilon" class="ltx_Math" display="inline" id="alg1.l0.m3" intent=":literal"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>, <math alttext="\beta" class="ltx_Math" display="inline" id="alg1.l0.m4" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, <math alttext="\mu" class="ltx_Math" display="inline" id="alg1.l0.m5" intent=":literal"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span> <math alttext="\pi_{\theta}\leftarrow\pi_{\theta_{\text{init}}}" class="ltx_Math" display="inline" id="alg1.l1.m1" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">←</mo><msub><mi>π</mi><msub><mi>θ</mi><mtext>init</mtext></msub></msub></mrow><annotation encoding="application/x-tex">\pi_{\theta}\leftarrow\pi_{\theta_{\text{init}}}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span> <span class="ltx_text ltx_font_bold">for</span> iteration <math alttext="=1,\ldots,I" class="ltx_Math" display="inline" id="alg1.l2.m1" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>I</mi></mrow></mrow><annotation encoding="application/x-tex">=1,\ldots,I</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span>  <math alttext="\pi_{\text{ref}}\leftarrow\pi_{\theta}" class="ltx_Math" display="inline" id="alg1.l3.m1" intent=":literal"><semantics><mrow><msub><mi>π</mi><mtext>ref</mtext></msub><mo stretchy="false">←</mo><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{\text{ref}}\leftarrow\pi_{\theta}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span>  <span class="ltx_text ltx_font_bold">for</span> step <math alttext="=1,\ldots,M" class="ltx_Math" display="inline" id="alg1.l4.m1" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>M</mi></mrow></mrow><annotation encoding="application/x-tex">=1,\ldots,M</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>   Sample a batch <math alttext="\mathcal{D}_{b}" class="ltx_Math" display="inline" id="alg1.l5.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>b</mi></msub><annotation encoding="application/x-tex">\mathcal{D}_{b}</annotation></semantics></math> from <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="alg1.l5.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>   <math alttext="\pi_{\theta_{\text{old}}}\leftarrow\pi_{\theta}" class="ltx_Math" display="inline" id="alg1.l6.m1" intent=":literal"><semantics><mrow><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mo stretchy="false">←</mo><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}\leftarrow\pi_{\theta}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>   For each <math alttext="q\in\mathcal{D}_{b}" class="ltx_Math" display="inline" id="alg1.l7.m1" intent=":literal"><semantics><mrow><mi>q</mi><mo>∈</mo><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>b</mi></msub></mrow><annotation encoding="application/x-tex">q\in\mathcal{D}_{b}</annotation></semantics></math>, sample <math alttext="G" class="ltx_Math" display="inline" id="alg1.l7.m2" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> outputs <math alttext="\{y_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot\mid q)" class="ltx_math_unparsed" display="inline" id="alg1.l7.m3" intent=":literal"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><mo>∼</mo><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>q</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\{y_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot\mid q)</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">   Compute outcome reward </span><math alttext="\{r_{i}\}_{i=1}^{G}" class="ltx_Math" display="inline" id="alg1.l8.m1" intent=":literal"><semantics><msubsup><mrow><mo mathcolor="#F0AD00" stretchy="false" style="--ltx-fg-color:#F0AD00;">{</mo><msub><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">r</mi><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">i</mi></msub><mo mathcolor="#F0AD00" stretchy="false" style="--ltx-fg-color:#F0AD00;">}</mo></mrow><mrow><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">i</mi><mo mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">=</mo><mn mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">1</mn></mrow><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">G</mi></msubsup><annotation encoding="application/x-tex">\{r_{i}\}_{i=1}^{G}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#F0AD00;"> from the final answer in each </span><math alttext="y_{i}" class="ltx_Math" display="inline" id="alg1.l8.m2" intent=":literal"><semantics><msub><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">y</mi><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">i</mi></msub><annotation encoding="application/x-tex">y_{i}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span><span class="ltx_text" style="--ltx-fg-color:#009E73;">   Compute each </span><math alttext="y_{i}" class="ltx_Math" display="inline" id="alg1.l9.m1" intent=":literal"><semantics><msub><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">y</mi><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">i</mi></msub><annotation encoding="application/x-tex">y_{i}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#009E73;">’s advantage </span><math alttext="\{A_{i}\}_{i=1}^{G}" class="ltx_Math" display="inline" id="alg1.l9.m2" intent=":literal"><semantics><msubsup><mrow><mo mathcolor="#009E73" stretchy="false" style="--ltx-fg-color:#009E73;">{</mo><msub><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">A</mi><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">i</mi></msub><mo mathcolor="#009E73" stretchy="false" style="--ltx-fg-color:#009E73;">}</mo></mrow><mrow><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">i</mi><mo mathcolor="#009E73" style="--ltx-fg-color:#009E73;">=</mo><mn mathcolor="#009E73" style="--ltx-fg-color:#009E73;">1</mn></mrow><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">G</mi></msubsup><annotation encoding="application/x-tex">\{A_{i}\}_{i=1}^{G}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#009E73;"> via group normalization of </span><math alttext="\{r_{i}\}_{i=1}^{G}" class="ltx_Math" display="inline" id="alg1.l9.m3" intent=":literal"><semantics><msubsup><mrow><mo mathcolor="#009E73" stretchy="false" style="--ltx-fg-color:#009E73;">{</mo><msub><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">r</mi><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">i</mi></msub><mo mathcolor="#009E73" stretchy="false" style="--ltx-fg-color:#009E73;">}</mo></mrow><mrow><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">i</mi><mo mathcolor="#009E73" style="--ltx-fg-color:#009E73;">=</mo><mn mathcolor="#009E73" style="--ltx-fg-color:#009E73;">1</mn></mrow><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">G</mi></msubsup><annotation encoding="application/x-tex">\{r_{i}\}_{i=1}^{G}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#009E73;"> (Eq.in Sec </span><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S2.SS2" style="--ltx-fg-color:#009E73;" title="2.2 Agentic Reinforcement Learning Pipeline ‣ 2 Preliminaries ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">2.2</span></a><span class="ltx_text" style="--ltx-fg-color:#009E73;">)</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span><span class="ltx_text" style="--ltx-fg-color:#0072B2;">   Assign </span><math alttext="A_{i}" class="ltx_Math" display="inline" id="alg1.l10.m1" intent=":literal"><semantics><msub><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">A</mi><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">i</mi></msub><annotation encoding="application/x-tex">A_{i}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#0072B2;"> to all tokens of </span><math alttext="y_{i}" class="ltx_Math" display="inline" id="alg1.l10.m2" intent=":literal"><semantics><msub><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">y</mi><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">i</mi></msub><annotation encoding="application/x-tex">y_{i}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline">11:</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;">   </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#AA3377;">for</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;"> GRPO iter </span><math alttext="=1,\ldots,\mu" class="ltx_Math" display="inline" id="alg1.l11.m1" intent=":literal"><semantics><mrow><mi></mi><mo mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">=</mo><mrow><mn mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">1</mn><mo mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">,</mo><mi mathcolor="#AA3377" mathvariant="normal" style="--ltx-fg-color:#AA3377;">…</mi><mo mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">,</mo><mi mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">μ</mi></mrow></mrow><annotation encoding="application/x-tex">=1,\ldots,\mu</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#AA3377;"> </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#AA3377;">do</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline">12:</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;">    Update </span><math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="alg1.l12.m1" intent=":literal"><semantics><msub><mi mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">π</mi><mi mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#AA3377;"> by maximizing the GRPO objective (Eq. </span><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S2.E1" style="--ltx-fg-color:#AA3377;" title="In 2.2 Agentic Reinforcement Learning Pipeline ‣ 2 Preliminaries ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" style="--ltx-fg-color:#AA3377;">)
</span>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline">13:</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;">   </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#AA3377;">end</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;"> </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#AA3377;">for</span>
</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline">14:</span>  <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline">15:</span> <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:212.5pt;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">IGPO</span></p>
<div class="ltx_listing ltx_listing">
<div class="ltx_listingline" id="alg1.l0a">
<span class="ltx_tag ltx_tag_listingline">0:</span> initial policy <math alttext="\pi_{\theta_{\text{init}}}" class="ltx_Math" display="inline" id="alg1.l0a.m1" intent=":literal"><semantics><msub><mi>π</mi><msub><mi>θ</mi><mtext>init</mtext></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{\text{init}}}</annotation></semantics></math>; task prompts <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="alg1.l0a.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation></semantics></math>; max turns <math alttext="H" class="ltx_Math" display="inline" id="alg1.l0a.m3" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>; hyperparameters <math alttext="\epsilon" class="ltx_Math" display="inline" id="alg1.l0a.m4" intent=":literal"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>, <math alttext="\beta" class="ltx_Math" display="inline" id="alg1.l0a.m5" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, <math alttext="\gamma" class="ltx_Math" display="inline" id="alg1.l0a.m6" intent=":literal"><semantics><mi>γ</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>, <math alttext="\mu" class="ltx_Math" display="inline" id="alg1.l0a.m7" intent=":literal"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l1a">
<span class="ltx_tag ltx_tag_listingline">1:</span> <math alttext="\pi_{\theta}\leftarrow\pi_{\theta_{\text{init}}}" class="ltx_Math" display="inline" id="alg1.l1a.m1" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">←</mo><msub><mi>π</mi><msub><mi>θ</mi><mtext>init</mtext></msub></msub></mrow><annotation encoding="application/x-tex">\pi_{\theta}\leftarrow\pi_{\theta_{\text{init}}}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l2a">
<span class="ltx_tag ltx_tag_listingline">2:</span> <span class="ltx_text ltx_font_bold">for</span> iteration <math alttext="=1,\ldots,I" class="ltx_Math" display="inline" id="alg1.l2a.m1" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>I</mi></mrow></mrow><annotation encoding="application/x-tex">=1,\ldots,I</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l3a">
<span class="ltx_tag ltx_tag_listingline">3:</span>  <math alttext="\pi_{\text{ref}}\leftarrow\pi_{\theta}" class="ltx_Math" display="inline" id="alg1.l3a.m1" intent=":literal"><semantics><mrow><msub><mi>π</mi><mtext>ref</mtext></msub><mo stretchy="false">←</mo><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{\text{ref}}\leftarrow\pi_{\theta}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l4a">
<span class="ltx_tag ltx_tag_listingline">4:</span>  <span class="ltx_text ltx_font_bold">for</span> step <math alttext="=1,\ldots,M" class="ltx_Math" display="inline" id="alg1.l4a.m1" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>M</mi></mrow></mrow><annotation encoding="application/x-tex">=1,\ldots,M</annotation></semantics></math> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline" id="alg1.l5a">
<span class="ltx_tag ltx_tag_listingline">5:</span>   Sample a batch <math alttext="\mathcal{D}_{b}" class="ltx_Math" display="inline" id="alg1.l5a.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>b</mi></msub><annotation encoding="application/x-tex">\mathcal{D}_{b}</annotation></semantics></math> from <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="alg1.l5a.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒟</mi><annotation encoding="application/x-tex">\mathcal{D}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l6a">
<span class="ltx_tag ltx_tag_listingline">6:</span>   <math alttext="\pi_{\theta_{\text{old}}}\leftarrow\pi_{\theta}" class="ltx_Math" display="inline" id="alg1.l6a.m1" intent=":literal"><semantics><mrow><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mo stretchy="false">←</mo><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}\leftarrow\pi_{\theta}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l7a">
<span class="ltx_tag ltx_tag_listingline">7:</span>   For each <math alttext="q\in\mathcal{D}_{b}" class="ltx_Math" display="inline" id="alg1.l7a.m1" intent=":literal"><semantics><mrow><mi>q</mi><mo>∈</mo><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>b</mi></msub></mrow><annotation encoding="application/x-tex">q\in\mathcal{D}_{b}</annotation></semantics></math>, sample <math alttext="G" class="ltx_Math" display="inline" id="alg1.l7a.m2" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> outputs <math alttext="\{y_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot\mid q)" class="ltx_math_unparsed" display="inline" id="alg1.l7a.m3" intent=":literal"><semantics><mrow><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><mo>∼</mo><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><mi>q</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\{y_{i}\}_{i=1}^{G}\sim\pi_{\theta_{\text{old}}}(\cdot\mid q)</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l8a">
<span class="ltx_tag ltx_tag_listingline">8:</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">   </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#F0AD00;">for</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;"> iteration </span><math alttext="=1,\ldots,T" class="ltx_Math" display="inline" id="alg1.l8a.m1" intent=":literal"><semantics><mrow><mi></mi><mo mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">=</mo><mrow><mn mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">1</mn><mo mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">,</mo><mi mathcolor="#F0AD00" mathvariant="normal" style="--ltx-fg-color:#F0AD00;">…</mi><mo mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">,</mo><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">T</mi></mrow></mrow><annotation encoding="application/x-tex">=1,\ldots,T</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#F0AD00;"> </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#F0AD00;">do</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l9a">
<span class="ltx_tag ltx_tag_listingline">9:</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">    </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#F0AD00;">if</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;"> </span><math alttext="\text{t}&lt;\text{T}" class="ltx_Math" display="inline" id="alg1.l9a.m1" intent=":literal"><semantics><mrow><mtext mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">t</mtext><mo mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">&lt;</mo><mtext mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">T</mtext></mrow><annotation encoding="application/x-tex">\text{t}&lt;\text{T}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#F0AD00;"> </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#F0AD00;">then</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l10a">
<span class="ltx_tag ltx_tag_listingline">10:</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">     Compute the infomation gain-based turn-level rewards </span><math alttext="\{r_{i,t}\}_{i=1}^{G}" class="ltx_Math" display="inline" id="alg1.l10a.m1" intent=":literal"><semantics><msubsup><mrow><mo mathcolor="#F0AD00" stretchy="false" style="--ltx-fg-color:#F0AD00;">{</mo><msub><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">r</mi><mrow><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">i</mi><mo mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">,</mo><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">t</mi></mrow></msub><mo mathcolor="#F0AD00" stretchy="false" style="--ltx-fg-color:#F0AD00;">}</mo></mrow><mrow><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">i</mi><mo mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">=</mo><mn mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">1</mn></mrow><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">G</mi></msubsup><annotation encoding="application/x-tex">\{r_{i,t}\}_{i=1}^{G}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#F0AD00;"> for each </span><math alttext="y_{i}" class="ltx_Math" display="inline" id="alg1.l10a.m2" intent=":literal"><semantics><msub><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">y</mi><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">i</mi></msub><annotation encoding="application/x-tex">y_{i}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#F0AD00;"> (Eq. </span><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S3.E4" style="--ltx-fg-color:#F0AD00;" title="In 3.2 Information Gain-based Turn-level Reward ‣ 3 Information Gain-based Policy Optimization ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">)
</span>
</div>
<div class="ltx_listingline" id="alg1.l11a">
<span class="ltx_tag ltx_tag_listingline">11:</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">    </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#F0AD00;">else</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l12a">
<span class="ltx_tag ltx_tag_listingline">12:</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">     Compute the final-turn rewards </span><math alttext="\{r_{i,T}\}_{i=1}^{G}" class="ltx_Math" display="inline" id="alg1.l12a.m1" intent=":literal"><semantics><msubsup><mrow><mo mathcolor="#F0AD00" stretchy="false" style="--ltx-fg-color:#F0AD00;">{</mo><msub><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">r</mi><mrow><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">i</mi><mo mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">,</mo><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">T</mi></mrow></msub><mo mathcolor="#F0AD00" stretchy="false" style="--ltx-fg-color:#F0AD00;">}</mo></mrow><mrow><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">i</mi><mo mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">=</mo><mn mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">1</mn></mrow><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">G</mi></msubsup><annotation encoding="application/x-tex">\{r_{i,T}\}_{i=1}^{G}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#F0AD00;"> based on the answer in each </span><math alttext="y_{i}" class="ltx_Math" display="inline" id="alg1.l12a.m2" intent=":literal"><semantics><msub><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">y</mi><mi mathcolor="#F0AD00" style="--ltx-fg-color:#F0AD00;">i</mi></msub><annotation encoding="application/x-tex">y_{i}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#F0AD00;"> (Eq. </span><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S2.E2" style="--ltx-fg-color:#F0AD00;" title="In 2.2 Agentic Reinforcement Learning Pipeline ‣ 2 Preliminaries ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">)
</span>
</div>
<div class="ltx_listingline" id="alg1.l13a">
<span class="ltx_tag ltx_tag_listingline">13:</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">    </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#F0AD00;">end</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;"> </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#F0AD00;">if</span>
</div>
<div class="ltx_listingline" id="alg1.l14a">
<span class="ltx_tag ltx_tag_listingline">14:</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;">   </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#F0AD00;">end</span><span class="ltx_text" style="--ltx-fg-color:#F0AD00;"> </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#F0AD00;">for</span>
</div>
<div class="ltx_listingline" id="alg1.l15a">
<span class="ltx_tag ltx_tag_listingline">15:</span><span class="ltx_text" style="--ltx-fg-color:#009E73;">   Compute the per turn advantages </span><math alttext="\{A_{i,1\leq t\leq T}\}_{i=1}^{G}" class="ltx_Math" display="inline" id="alg1.l15a.m1" intent=":literal"><semantics><msubsup><mrow><mo mathcolor="#009E73" stretchy="false" style="--ltx-fg-color:#009E73;">{</mo><msub><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">A</mi><mrow><mrow><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">i</mi><mo mathcolor="#009E73" style="--ltx-fg-color:#009E73;">,</mo><mn mathcolor="#009E73" style="--ltx-fg-color:#009E73;">1</mn></mrow><mo mathcolor="#009E73" style="--ltx-fg-color:#009E73;">≤</mo><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">t</mi><mo mathcolor="#009E73" style="--ltx-fg-color:#009E73;">≤</mo><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">T</mi></mrow></msub><mo mathcolor="#009E73" stretchy="false" style="--ltx-fg-color:#009E73;">}</mo></mrow><mrow><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">i</mi><mo mathcolor="#009E73" style="--ltx-fg-color:#009E73;">=</mo><mn mathcolor="#009E73" style="--ltx-fg-color:#009E73;">1</mn></mrow><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">G</mi></msubsup><annotation encoding="application/x-tex">\{A_{i,1\leq t\leq T}\}_{i=1}^{G}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#009E73;"> in </span><math alttext="y_{i}" class="ltx_Math" display="inline" id="alg1.l15a.m2" intent=":literal"><semantics><msub><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">y</mi><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">i</mi></msub><annotation encoding="application/x-tex">y_{i}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#009E73;"> via group normalization of </span><math alttext="\{r_{i,1\leq t\leq T}\}_{i=1}^{G}" class="ltx_Math" display="inline" id="alg1.l15a.m3" intent=":literal"><semantics><msubsup><mrow><mo mathcolor="#009E73" stretchy="false" style="--ltx-fg-color:#009E73;">{</mo><msub><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">r</mi><mrow><mrow><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">i</mi><mo mathcolor="#009E73" style="--ltx-fg-color:#009E73;">,</mo><mn mathcolor="#009E73" style="--ltx-fg-color:#009E73;">1</mn></mrow><mo mathcolor="#009E73" style="--ltx-fg-color:#009E73;">≤</mo><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">t</mi><mo mathcolor="#009E73" style="--ltx-fg-color:#009E73;">≤</mo><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">T</mi></mrow></msub><mo mathcolor="#009E73" stretchy="false" style="--ltx-fg-color:#009E73;">}</mo></mrow><mrow><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">i</mi><mo mathcolor="#009E73" style="--ltx-fg-color:#009E73;">=</mo><mn mathcolor="#009E73" style="--ltx-fg-color:#009E73;">1</mn></mrow><mi mathcolor="#009E73" style="--ltx-fg-color:#009E73;">G</mi></msubsup><annotation encoding="application/x-tex">\{r_{i,1\leq t\leq T}\}_{i=1}^{G}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#009E73;"> (Eq. </span><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S3.E6" style="--ltx-fg-color:#009E73;" title="In 3.3 Policy Optimization with Turn-level Advantage ‣ 3 Information Gain-based Policy Optimization ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text" style="--ltx-fg-color:#009E73;">)</span>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline">16:</span><span class="ltx_text" style="--ltx-fg-color:#0072B2;">   Compute the per turn cumulative discounted advantages </span><math alttext="\{\hat{A}_{i,1\leq t\leq T}\}_{i=1}^{G}" class="ltx_Math" display="inline" id="alg1.l16.m1" intent=":literal"><semantics><msubsup><mrow><mo mathcolor="#0072B2" stretchy="false" style="--ltx-fg-color:#0072B2;">{</mo><msub><mover accent="true"><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">A</mi><mo mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">^</mo></mover><mrow><mrow><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">i</mi><mo mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">,</mo><mn mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">1</mn></mrow><mo mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">≤</mo><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">t</mi><mo mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">≤</mo><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">T</mi></mrow></msub><mo mathcolor="#0072B2" stretchy="false" style="--ltx-fg-color:#0072B2;">}</mo></mrow><mrow><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">i</mi><mo mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">=</mo><mn mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">1</mn></mrow><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">G</mi></msubsup><annotation encoding="application/x-tex">\{\hat{A}_{i,1\leq t\leq T}\}_{i=1}^{G}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#0072B2;"> in each </span><math alttext="y_{i}" class="ltx_Math" display="inline" id="alg1.l16.m2" intent=":literal"><semantics><msub><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">y</mi><mi mathcolor="#0072B2" style="--ltx-fg-color:#0072B2;">i</mi></msub><annotation encoding="application/x-tex">y_{i}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#0072B2;"> (Eq. </span><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S3.E7" style="--ltx-fg-color:#0072B2;" title="In 3.3 Policy Optimization with Turn-level Advantage ‣ 3 Information Gain-based Policy Optimization ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">7</span></a><span class="ltx_text" style="--ltx-fg-color:#0072B2;">), then assign them to the tokens in each turn</span>
</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline">17:</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;">   </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#AA3377;">for</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;"> IGPO iteration </span><math alttext="=1,\ldots,\mu" class="ltx_Math" display="inline" id="alg1.l17.m1" intent=":literal"><semantics><mrow><mi></mi><mo mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">=</mo><mrow><mn mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">1</mn><mo mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">,</mo><mi mathcolor="#AA3377" mathvariant="normal" style="--ltx-fg-color:#AA3377;">…</mi><mo mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">,</mo><mi mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">μ</mi></mrow></mrow><annotation encoding="application/x-tex">=1,\ldots,\mu</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#AA3377;"> </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#AA3377;">do</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;">
</span>
</div>
<div class="ltx_listingline" id="alg1.l18">
<span class="ltx_tag ltx_tag_listingline">18:</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;">    Update </span><math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="alg1.l18.m1" intent=":literal"><semantics><msub><mi mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">π</mi><mi mathcolor="#AA3377" style="--ltx-fg-color:#AA3377;">θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math><span class="ltx_text" style="--ltx-fg-color:#AA3377;"> by maximizing the IGPO objective (Eq. </span><a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#S3.E8" style="--ltx-fg-color:#AA3377;" title="In 3.3 Policy Optimization with Turn-level Advantage ‣ 3 Information Gain-based Policy Optimization ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text" style="--ltx-fg-color:#AA3377;">)
</span>
</div>
<div class="ltx_listingline" id="alg1.l19">
<span class="ltx_tag ltx_tag_listingline">19:</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;">   </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#AA3377;">end</span><span class="ltx_text" style="--ltx-fg-color:#AA3377;"> </span><span class="ltx_text ltx_font_bold" style="--ltx-fg-color:#AA3377;">for</span>
</div>
<div class="ltx_listingline" id="alg1.l20">
<span class="ltx_tag ltx_tag_listingline">20:</span>  <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
<div class="ltx_listingline" id="alg1.l21">
<span class="ltx_tag ltx_tag_listingline">21:</span> <span class="ltx_text ltx_font_bold">end</span> <span class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</div>
</div>
</div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Prompt template used in our experiments.</h2>
<div class="ltx_para ltx_noindent" id="A6.p1">
<p class="ltx_p">Our prompt follows the style of DeepResearcher <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14967v1#bib.bib48" title="">2025b</a>)</cite>, and the same template is used for training, validation, and testing. The prompt template is shown in the <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2510.14967v1#A6.F8" title="Figure 8 ‣ Appendix F Prompt template used in our experiments. ‣ Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>, where <span class="ltx_text ltx_font_typewriter">{today}</span> represents the current date to ensure the relevance of the model’s response. <span class="ltx_text ltx_font_typewriter">{{ tool.name }}: {{ tool.description }}</span> indicates the available tools, while the <span class="ltx_text ltx_font_typewriter">#Rollout section</span> controls the model’s output format. The <span class="ltx_text ltx_font_typewriter">#Tools</span> section provides the model with the tool invocation method.</p>
</div>
<figure class="ltx_figure" id="A6.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="918" id="A6.F8.g1" src="x14.png" width="760"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Prompt template used in our experiments.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 16 17:56:09 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
