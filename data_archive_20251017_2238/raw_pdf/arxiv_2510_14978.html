<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Learning an Image Editing Model without Image Editing Pairs</title>
<!--Generated on Thu Oct 16 17:50:04 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2510.14978v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S1" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S2" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S3" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S3.SS1" title="In 3 Background ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Diffusion Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S3.SS2" title="In 3 Background ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Vision Language Models (VLMs)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S4" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S4.SS1" title="In 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Edit Instruction Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S4.SS2" title="In 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Training Objective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S4.SS3" title="In 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Training details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.SS1" title="In 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Local image-editing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.SS2" title="In 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Free-form Editing: Customization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.SS3" title="In 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S6" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion and Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A1" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional Comparison with Baseline Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A2" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A3" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Limitation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A4" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Dataset Construction Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A5" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Training Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A5.SS1" title="In Appendix E Training Implementation Details ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.1 </span>Local-image editing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A5.SS2" title="In Appendix E Training Implementation Details ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.2 </span>Free-form editing (Customization)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A6" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Other Baseline Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A7" title="In Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Societal Impact</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Learning an Image Editing Model
<br class="ltx_break"/>without Image Editing Pairs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_tabular ltx_align_middle">
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_left" style="padding-bottom:5.0pt;"><span class="ltx_text ltx_font_bold">Nupur Kumari<sup class="ltx_sup"><span class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>   
<span class="ltx_text ltx_font_bold">Sheng-Yu Wang<sup class="ltx_sup"><span class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>   
<span class="ltx_text ltx_font_bold">Nanxuan Zhao<sup class="ltx_sup"><span class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span>   
<span class="ltx_text ltx_font_bold">Yotam Nitzan<sup class="ltx_sup"><span class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_left" style="padding-bottom:5.0pt;"><span class="ltx_text ltx_font_bold">Yuheng Li<sup class="ltx_sup"><span class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span>   
<span class="ltx_text ltx_font_bold">Krishna Kumar Singh<sup class="ltx_sup"><span class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span>   
<span class="ltx_text ltx_font_bold">Richard Zhang<sup class="ltx_sup"><span class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_nopad_r ltx_align_left" style="padding-bottom:11.99998pt;"><span class="ltx_text ltx_font_bold">Eli Shechtman<sup class="ltx_sup"><span class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span>   
<span class="ltx_text ltx_font_bold">Jun-Yan Zhu<sup class="ltx_sup"><span class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>   
<span class="ltx_text ltx_font_bold">Xun Huang<sup class="ltx_sup"><span class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span></span></span>
</span>
<br class="ltx_break"/><sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1</span></sup>Carnegie Mellon University  <sup class="ltx_sup"><span class="ltx_text ltx_font_italic">2</span></sup>Adobe
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale.
Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p">Large-scale text-to-image models have achieved remarkable success, generating images of high fidelity that closely align with textual descriptions <cite class="ltx_cite ltx_citemacro_cite">Ramesh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib59" title="">2022</a>); Peebles &amp; Xie (<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib55" title="">2023</a>); Kang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib31" title="">2023</a>)</cite>. Despite these advances, text-only conditioning offers limited user control and falls short for many downstream applications <cite class="ltx_cite ltx_citemacro_citep">(Meng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib52" title="">2022</a>; Gal et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib18" title="">2023</a>)</cite>. In practice, users often wish to start with an existing image to perform tasks like adjusting local attributes, changing the style, or placing an object in a new context. These <em class="ltx_emph ltx_font_italic">image editing</em> operations require precise, image-guided control that text-only prompts cannot provide.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p">While collecting large-scale text–image pairs is relatively straightforward <cite class="ltx_cite ltx_citemacro_citep">(Schuhmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib64" title="">2021</a>)</cite>, constructing supervised datasets for editing tasks is far more challenging. As one requires a <span class="ltx_text ltx_font_italic">pair</span> of images, the input and its edited counterpart, along with the text instruction, and such data is rarely available online. Early methods addressed this by synthetically generating editing pairs <cite class="ltx_cite ltx_citemacro_citep">(Brooks et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib5" title="">2023</a>)</cite> from a pretrained model, using zero-shot editing techniques <cite class="ltx_cite ltx_citemacro_citep">(Hertz et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib24" title="">2023</a>)</cite>. However, synthetic datasets can quickly become outdated with new and improved base models, and they risk amplifying and propagating artifacts of the synthetic editing process. More recent approaches extract frames from videos and annotate their differences <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib10" title="">2025</a>; Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib70" title="">2023b</a>; Krojer et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib36" title="">2024</a>)</cite>. Although promising, the applicability of this strategy is constrained by the diversity of transformations present in natural video sequences, where obtaining pixel-aligned before–and–after edited pairs is nearly impossible. A final alternative is to manually create training pairs <cite class="ltx_cite ltx_citemacro_citep">(Winter et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib76" title="">2024</a>; Magar et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib51" title="">2025</a>)</cite>, but this can be quite laborious and does not scale as easily.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p">In this work, we explore the possibility of training an image editing model <span class="ltx_text ltx_font_italic">without any training pairs</span>. Our key idea is to leverage supervision from Vision Language Models (VLMs) <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib44" title="">2023a</a>)</cite>, relying on their general image-understanding capabilities to check whether the generated images satisfy the editing instructions. Prior works have studied the use of specialized models or general-purpose VLMs in improving generative models along dimensions such as text-alignment and aesthetic quality, primarily using reinforcement learning <cite class="ltx_cite ltx_citemacro_citep">(Black et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib4" title="">2024</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib45" title="">2025a</a>)</cite>. In contrast, our method is the first to explore using gradient feedback from VLMs for general instruction-following, and we distill this feedback into a lightweight generative model that can generalize to arbitrary images and edit instructions. Our final method combines the VLM-feedback with a distribution matching loss to ensure that generated outputs remain in the realistic image domain while following the edit instructions. In summary, our contributions are threefold:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p">We propose NP-Edit (No-Pair Edit), a framework for training image editing models using gradient feedback from a Vision–Language Model (VLM), requiring <em class="ltx_emph ltx_font_italic">no paired supervision</em>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p">For efficient training and effective VLM feedback, our formulation combines it with distribution matching loss to learn a <em class="ltx_emph ltx_font_italic">few-step</em> image editing model. The final model remains competitive with existing baselines trained on supervised data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p">We conduct a comprehensive empirical study analyzing the impact of (i) different VLM backbones, (ii) dataset scale and diversity, and (iii) VLM loss formulation. Our findings show that performance improves directly with more powerful VLMs and larger datasets, demonstrating its strong potential and scalability.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Diffusion-based image editing.</span>   Development of large-scale text-to-image models has enabled a wide range of downstream applications, including local image editing <cite class="ltx_cite ltx_citemacro_citep">(Hertz et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib24" title="">2023</a>; Meng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib52" title="">2022</a>)</cite>, stylization <cite class="ltx_cite ltx_citemacro_citep">(Sohn et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib66" title="">2023</a>; Hertz et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib25" title="">2024</a>; Jones et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib30" title="">2024</a>)</cite>, personalization and customization <cite class="ltx_cite ltx_citemacro_citep">(Gal et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib18" title="">2023</a>; Ruiz et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib60" title="">2023</a>)</cite>. These can broadly be viewed as different forms of image-editing capabilities. Early approaches often relied on zero-shot inference-time methods <cite class="ltx_cite ltx_citemacro_citep">(Hertz et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib24" title="">2023</a>; Parmar et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib54" title="">2023</a>; Cao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib7" title="">2023</a>; Avrahami et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib3" title="">2023</a>; Kim et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib34" title="">2023</a>)</cite> or flexible but slow optimization-based techniques <cite class="ltx_cite ltx_citemacro_citep">(Gal et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib18" title="">2023</a>; Ruiz et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib60" title="">2023</a>; Kumari et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib38" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p">To improve efficiency and robustness, subsequent works introduced training-based approaches <cite class="ltx_cite ltx_citemacro_citep">(Brooks et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib5" title="">2023</a>; Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib78" title="">2025</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib9" title="">2023</a>; Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib17" title="">2024</a>; Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib71" title="">2024</a>)</cite>. However, obtaining large datasets of image pairs remains challenging: synthetic curation <cite class="ltx_cite ltx_citemacro_citep">(Brooks et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib5" title="">2023</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib90" title="">2023</a>; Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib93" title="">2024</a>; Hui et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib29" title="">2024</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib83" title="">2024b</a>; Tan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib73" title="">2025</a>; Cai et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib6" title="">2025</a>; Kumari et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib39" title="">2025</a>)</cite> risks becoming outdated as generative models improve, while human annotation <cite class="ltx_cite ltx_citemacro_citep">(Winter et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib76" title="">2024</a>; Magar et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib51" title="">2025</a>; Ge et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib19" title="">2024</a>; Sushko et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib72" title="">2025</a>)</cite> is costly and labor-intensive. Recent efforts have explored constructing paired data from videos <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib10" title="">2025</a>; Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib70" title="">2023b</a>)</cite> or simulation environments <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib88" title="">2025</a>)</cite>, although these remain limited in either annotation diversity or visual realism. We also target similar image-editing capabilities but remove the need for paired data <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib96" title="">2017</a>)</cite>, by using differentiable feedback from vision–language models instead of ground-truth edits.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Post-training for image generation.</span>
Post-training methods typically align image generators with human preferences using either Direct Preference Optimization (DPO) <cite class="ltx_cite ltx_citemacro_citep">(Wallace et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib74" title="">2024</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib82" title="">2024a</a>)</cite> or Reinforcement Learning (RL) <cite class="ltx_cite ltx_citemacro_citep">(Black et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib4" title="">2024</a>)</cite>. While early RL-based works use feedback from a simple scalar reward model <cite class="ltx_cite ltx_citemacro_citep">(Kirstain et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib35" title="">2023</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib79" title="">2023</a>)</cite>, the paradigm has recently been enhanced by employing sophisticated Vision-Language Models (VLMs) as “judges” to provide more generic and accurate reward signals <cite class="ltx_cite ltx_citemacro_citep">(Ku et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib37" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p">Although post-training has been successfully applied to text-to-image generation, its use for image editing models has been less explored. Concurrently to our work, EARL <cite class="ltx_cite ltx_citemacro_citep">(Ahmadi et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib2" title="">2025</a>)</cite> begins to address this by using a VLM-as-a-judge framework to post-train an image-editing model with RL. However, RL-based approaches often depend heavily on good initialization, typically requiring a Supervised Fine-Tuning (SFT) phase with paired editing data.
In contrast, our method leverages differentiable feedback from the VLM model, thereby obviating the need for an initial SFT stage and enables the learning of image editing models without the use of synthetically generated data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p">Related to our work, <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib49" title="">2025</a>)</cite> recently introduced a method that incorporates gradient feedback from VLMs to satisfy various criteria, including the horizon line, style, and layout, in generated images. However, their framework operates in a per-example optimization setting, requiring costly LoRA fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib27" title="">2022</a>)</cite> for each criterion and prompt pair, and also does not consider image editing tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p6">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Few-step diffusion models.</span>
Standard diffusion (or flow-matching) models require many sampling steps to generate high-quality images. Many prior works reduce the number of denoising steps for faster sampling by predicting larger denoising steps, including consistency models <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib33" title="">2024</a>; Geng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib21" title="">2024</a>; Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib69" title="">2023a</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib84" title="">2024c</a>; Song &amp; Dhariwal, <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib68" title="">2024</a>; Lu &amp; Song, <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib48" title="">2025</a>; Heek et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib23" title="">2024</a>)</cite>, shortcut models <cite class="ltx_cite ltx_citemacro_citep">(Frans et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib16" title="">2024</a>)</cite>, meanflow <cite class="ltx_cite ltx_citemacro_citep">(Geng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib22" title="">2025</a>)</cite>, and inductive moment matching <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib94" title="">2025</a>)</cite>. Another line distills a pre-trained multi-step teacher into a few-step student by matching ODE trajectories <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib69" title="">2023a</a>; Salimans &amp; Ho, <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib61" title="">2022</a>; Geng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib20" title="">2023</a>)</cite>, using adversarial loss <cite class="ltx_cite ltx_citemacro_citep">(Sauer et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib63" title="">2024b</a>; Kang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib32" title="">2024</a>; Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib86" title="">2024a</a>; Sauer et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib62" title="">2024a</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib80" title="">2024</a>)</cite>, or applying score distillation <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib50" title="">2023</a>; Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib87" title="">2024b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib86" title="">a</a>; Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib95" title="">2024</a>)</cite>. In our framework, we adopt DMD <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib87" title="">2024b</a>)</cite> as a distribution matching objective. This ensures that our few-step editing model’s output remains in the real-image manifold defined by the pre-trained text-to-image teacher, while using VLM-feedback to ensure the model follows the editing instructions.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Background</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Diffusion Models</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p">Diffusion or flow-based models are a class of generating models that learn the data distribution by denoising samples corrupted by different levels of Gaussian Noise <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib26" title="">2020</a>; Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib67" title="">2021</a>; Lipman et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib43" title="">2023</a>)</cite>. Given a real sample <math alttext="{\mathbf{x}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m1" intent=":literal"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">{\mathbf{x}}</annotation></semantics></math>, a forward diffusion process creates noisy samples <math alttext="{\mathbf{x}}^{t}=\alpha^{t}{\mathbf{x}}+\sigma^{t}\epsilon" class="ltx_Math" display="inline" id="S3.SS1.p1.m2" intent=":literal"><semantics><mrow><msup><mi>𝐱</mi><mi>t</mi></msup><mo>=</mo><mrow><mrow><msup><mi>α</mi><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>𝐱</mi></mrow><mo>+</mo><mrow><msup><mi>σ</mi><mi>t</mi></msup><mo lspace="0em" rspace="0em">​</mo><mi>ϵ</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">{\mathbf{x}}^{t}=\alpha^{t}{\mathbf{x}}+\sigma^{t}\epsilon</annotation></semantics></math> over time <math alttext="t\in(0,1]" class="ltx_Math" display="inline" id="S3.SS1.p1.m3" intent=":literal"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">t\in(0,1]</annotation></semantics></math>, where <math alttext="\epsilon\sim\mathcal{N}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="S3.SS1.p1.m4" intent=":literal"><semantics><mrow><mi>ϵ</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\epsilon\sim\mathcal{N}(\bm{0},\bm{I})</annotation></semantics></math>, and <math alttext="\alpha^{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.m5" intent=":literal"><semantics><msup><mi>α</mi><mi>t</mi></msup><annotation encoding="application/x-tex">\alpha^{t}</annotation></semantics></math>, <math alttext="\sigma^{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.m6" intent=":literal"><semantics><msup><mi>σ</mi><mi>t</mi></msup><annotation encoding="application/x-tex">\sigma^{t}</annotation></semantics></math> define a noise schedule such that <math alttext="{\mathbf{x}}^{T}\sim\mathcal{N}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="S3.SS1.p1.m7" intent=":literal"><semantics><mrow><msup><mi>𝐱</mi><mi>T</mi></msup><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">{\mathbf{x}}^{T}\sim\mathcal{N}(\bm{0},\bm{I})</annotation></semantics></math> and <math alttext="{\mathbf{x}}^{0}={\mathbf{x}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m8" intent=":literal"><semantics><mrow><msup><mi>𝐱</mi><mn>0</mn></msup><mo>=</mo><mi>𝐱</mi></mrow><annotation encoding="application/x-tex">{\mathbf{x}}^{0}={\mathbf{x}}</annotation></semantics></math>. The denoising model is parameterized to reverse the forward diffusion process by either predicting the noise <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS1.p1.m9" intent=":literal"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib26" title="">2020</a>)</cite> added to the sample or velocity <math alttext="{\mathbf{v}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m10" intent=":literal"><semantics><mi>𝐯</mi><annotation encoding="application/x-tex">{\mathbf{v}}</annotation></semantics></math> towards the clean sample <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib47" title="">2023b</a>; Salimans &amp; Ho, <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib61" title="">2022</a>)</cite>. In our work, we follow the flow-based formulation, with the forward denoising process being a linear interpolation, i.e., <math alttext="\alpha^{t}=1-t" class="ltx_Math" display="inline" id="S3.SS1.p1.m11" intent=":literal"><semantics><mrow><msup><mi>α</mi><mi>t</mi></msup><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mi>t</mi></mrow></mrow><annotation encoding="application/x-tex">\alpha^{t}=1-t</annotation></semantics></math> and <math alttext="\sigma^{t}=t" class="ltx_Math" display="inline" id="S3.SS1.p1.m12" intent=":literal"><semantics><mrow><msup><mi>σ</mi><mi>t</mi></msup><mo>=</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">\sigma^{t}=t</annotation></semantics></math>. The training objective for a flow-based model, with parameters <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p1.m13" intent=":literal"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>, can be simplified to the following:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{{\mathbf{x}}^{t},t,\mathbf{c},\epsilon\sim\mathcal{N}(\bm{0},\bm{I})}w_{t}||\mathbf{v}-\mathbf{v}_{\theta}({\mathbf{x}}^{t},t,\mathbf{c})||," class="ltx_Math" display="inline" id="S3.E1X.m2" intent=":literal"><semantics><mrow><mrow><msub><mi>𝔼</mi><mrow><mrow><msup><mi>𝐱</mi><mi>t</mi></msup><mo>,</mo><mi>t</mi><mo>,</mo><mi>𝐜</mi><mo>,</mo><mi>ϵ</mi></mrow><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>w</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">‖</mo><mrow><mi>𝐯</mi><mo>−</mo><mrow><msub><mi>𝐯</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝐱</mi><mi>t</mi></msup><mo>,</mo><mi>t</mi><mo>,</mo><mi>𝐜</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">‖</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\mathbb{E}_{{\mathbf{x}}^{t},t,\mathbf{c},\epsilon\sim\mathcal{N}(\bm{0},\bm{I})}w_{t}||\mathbf{v}-\mathbf{v}_{\theta}({\mathbf{x}}^{t},t,\mathbf{c})||,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="{\mathbf{v}}=\epsilon-{\mathbf{x}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m14" intent=":literal"><semantics><mrow><mi>𝐯</mi><mo>=</mo><mrow><mi>ϵ</mi><mo>−</mo><mi>𝐱</mi></mrow></mrow><annotation encoding="application/x-tex">{\mathbf{v}}=\epsilon-{\mathbf{x}}</annotation></semantics></math> and <math alttext="w_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.m15" intent=":literal"><semantics><msub><mi>w</mi><mi>t</mi></msub><annotation encoding="application/x-tex">w_{t}</annotation></semantics></math> is a time dependent weighting factor. The denoising network can be conditioned on other inputs <math alttext="{\mathbf{c}}" class="ltx_Math" display="inline" id="S3.SS1.p1.m16" intent=":literal"><semantics><mi>𝐜</mi><annotation encoding="application/x-tex">{\mathbf{c}}</annotation></semantics></math>, such as a text prompt, a reference image, or both, as in our case.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Vision Language Models (VLMs)</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p">Vision Language Models (VLMs) trained from multimodal image-text data have shown exemplary visual understanding and reasoning capabilities and can serve as a general-purpose visual model.
A common strategy for training such large-scale VLMs is via visual instruction tuning <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib44" title="">2023a</a>)</cite>, which aligns a pre-trained Vision Encoder output with the input word embedding space of a pretrained Large Language Model (LLM). More specifically, the image <math alttext="{\mathbf{x}}" class="ltx_Math" display="inline" id="S3.SS2.p1.m1" intent=":literal"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">{\mathbf{x}}</annotation></semantics></math> is encoded into a set of tokens using the vision encoder, <math alttext="\mathbf{X}_{v}=g({\mathbf{x}})" class="ltx_Math" display="inline" id="S3.SS2.p1.m2" intent=":literal"><semantics><mrow><msub><mi>𝐗</mi><mi>v</mi></msub><mo>=</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐱</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{X}_{v}=g({\mathbf{x}})</annotation></semantics></math>. The input question regarding the image and its ground truth answer are tokenized in the LLM input embedding space as <math alttext="\mathbf{X}_{q}" class="ltx_Math" display="inline" id="S3.SS2.p1.m3" intent=":literal"><semantics><msub><mi>𝐗</mi><mi>q</mi></msub><annotation encoding="application/x-tex">\mathbf{X}_{q}</annotation></semantics></math> and <math alttext="\mathbf{X}_{a}" class="ltx_Math" display="inline" id="S3.SS2.p1.m4" intent=":literal"><semantics><msub><mi>𝐗</mi><mi>a</mi></msub><annotation encoding="application/x-tex">\mathbf{X}_{a}</annotation></semantics></math>, respectively. A projector module, <math alttext="f_{\phi}" class="ltx_Math" display="inline" id="S3.SS2.p1.m5" intent=":literal"><semantics><msub><mi>f</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">f_{\phi}</annotation></semantics></math>, projects the vision-encoded tokens into the LLM word embedding space and is trained via standard autoregressive loss to maximize the probability of predicting the correct answer:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle p(\mathbf{X}_{a}|\mathbf{X}_{v},\mathbf{X}_{q})=\prod_{i=1}^{L}p\Big(a_{i}|f_{\phi}(\mathbf{X}_{v}),\mathbf{X}_{q},\mathbf{X}_{a&lt;i}\Big)," class="ltx_Math" display="inline" id="S3.E2X.m2" intent=":literal"><semantics><mrow><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝐗</mi><mi>a</mi></msub><mo fence="false">|</mo><mrow><msub><mi>𝐗</mi><mi>v</mi></msub><mo>,</mo><msub><mi>𝐗</mi><mi>q</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo movablelimits="false">∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover></mstyle><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.600em" minsize="1.600em">(</mo><mrow><msub><mi>a</mi><mi>i</mi></msub><mo fence="false">|</mo><mrow><mrow><msub><mi>f</mi><mi>ϕ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐗</mi><mi>v</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><msub><mi>𝐗</mi><mi>q</mi></msub><mo>,</mo><msub><mi>𝐗</mi><mrow><mi>a</mi><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow></mrow><mo maxsize="1.600em" minsize="1.600em">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle p(\mathbf{X}_{a}|\mathbf{X}_{v},\mathbf{X}_{q})=\prod_{i=1}^{L}p\Big(a_{i}|f_{\phi}(\mathbf{X}_{v}),\mathbf{X}_{q},\mathbf{X}_{a&lt;i}\Big),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="\mathbf{X}_{a}=[a_{1}\cdots a_{L}]" class="ltx_Math" display="inline" id="S3.SS2.p1.m6" intent=":literal"><semantics><mrow><msub><mi>𝐗</mi><mi>a</mi></msub><mo>=</mo><mrow><mo stretchy="false">[</mo><mrow><msub><mi>a</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">​</mo><mi mathvariant="normal">⋯</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>a</mi><mi>L</mi></msub></mrow><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{X}_{a}=[a_{1}\cdots a_{L}]</annotation></semantics></math> is of token length <math alttext="L" class="ltx_Math" display="inline" id="S3.SS2.p1.m7" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>, and <math alttext="\mathbf{X}_{a&lt;i}" class="ltx_Math" display="inline" id="S3.SS2.p1.m8" intent=":literal"><semantics><msub><mi>𝐗</mi><mrow><mi>a</mi><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">\mathbf{X}_{a&lt;i}</annotation></semantics></math> denotes all the tokens before the current prediction token index. The final loss simplifies to a cross-entropy over the total vocabulary length. In our experiments, we use LLaVa-OneVision-7B <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib41" title="">2024</a>)</cite> as the VLM that uses SigLIP <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib89" title="">2023</a>)</cite> vision encoder and Qwen-2 LLM <cite class="ltx_cite ltx_citemacro_citep">(Qwen-Team, <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib56" title="">2024</a>)</cite>, and is among the state-of-the-art VLMs of this scale.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p">Given a pretrained text-to-image diffusion model <math alttext="G_{\text{init}}" class="ltx_Math" display="inline" id="S4.p1.m1" intent=":literal"><semantics><msub><mi>G</mi><mtext>init</mtext></msub><annotation encoding="application/x-tex">G_{\text{init}}</annotation></semantics></math> and a dataset <math alttext="\mathcal{X}=\{({\mathbf{y}}_{i},{\mathbf{c}}_{i},{\mathbf{c}}^{{\mathbf{y}}}_{i},{\mathbf{c}}^{{\mathbf{x}}}_{i})\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S4.p1.m2" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐲</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐜</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>𝐜</mi><mi>i</mi><mi>𝐲</mi></msubsup><mo>,</mo><msubsup><mi>𝐜</mi><mi>i</mi><mi>𝐱</mi></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{X}=\{({\mathbf{y}}_{i},{\mathbf{c}}_{i},{\mathbf{c}}^{{\mathbf{y}}}_{i},{\mathbf{c}}^{{\mathbf{x}}}_{i})\}_{i=1}^{N}</annotation></semantics></math> of reference image <math alttext="{\mathbf{y}}" class="ltx_Math" display="inline" id="S4.p1.m3" intent=":literal"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">{\mathbf{y}}</annotation></semantics></math>, corresponding edit instruction <math alttext="{\mathbf{c}}" class="ltx_Math" display="inline" id="S4.p1.m4" intent=":literal"><semantics><mi>𝐜</mi><annotation encoding="application/x-tex">{\mathbf{c}}</annotation></semantics></math>, and captions <math alttext="{\mathbf{c}}^{{\mathbf{y}}}" class="ltx_Math" display="inline" id="S4.p1.m5" intent=":literal"><semantics><msup><mi>𝐜</mi><mi>𝐲</mi></msup><annotation encoding="application/x-tex">{\mathbf{c}}^{{\mathbf{y}}}</annotation></semantics></math> and <math alttext="{\mathbf{c}}^{{\mathbf{x}}}" class="ltx_Math" display="inline" id="S4.p1.m6" intent=":literal"><semantics><msup><mi>𝐜</mi><mi>𝐱</mi></msup><annotation encoding="application/x-tex">{\mathbf{c}}^{{\mathbf{x}}}</annotation></semantics></math> that describe the reference and edited image respectively, we fine-tune <math alttext="G_{\text{init}}" class="ltx_Math" display="inline" id="S4.p1.m7" intent=":literal"><semantics><msub><mi>G</mi><mtext>init</mtext></msub><annotation encoding="application/x-tex">G_{\text{init}}</annotation></semantics></math> into a <em class="ltx_emph ltx_font_italic">few-step</em> image editing model <math alttext="G_{\theta}" class="ltx_Math" display="inline" id="S4.p1.m8" intent=":literal"><semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">G_{\theta}</annotation></semantics></math> without requiring ground truth edited image <math alttext="{\mathbf{x}}" class="ltx_Math" display="inline" id="S4.p1.m9" intent=":literal"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">{\mathbf{x}}</annotation></semantics></math> according to the edit instruction. Our approach, No-Pair (NP)-Edit, introduces a VLM-based loss to evaluate edit success and combines it with a distribution matching loss to ensure outputs remain within the natural image domain. Below, we first detail the construction of the dataset, then our training objective, and finally other implementation details.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Edit Instruction Dataset</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p">Each dataset sample consists of a real image <math alttext="{\mathbf{y}}" class="ltx_Math" display="inline" id="S4.SS1.p1.m1" intent=":literal"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">{\mathbf{y}}</annotation></semantics></math> as reference and an associated edit instruction, <math alttext="{\mathbf{c}}" class="ltx_Math" display="inline" id="S4.SS1.p1.m2" intent=":literal"><semantics><mi>𝐜</mi><annotation encoding="application/x-tex">{\mathbf{c}}</annotation></semantics></math>. Following prior works <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib46" title="">2025b</a>; Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib85" title="">2025</a>)</cite>, we focus on several categories of local editing operations, such as <em class="ltx_emph ltx_font_italic">Add</em>, <em class="ltx_emph ltx_font_italic">Replace</em>, <em class="ltx_emph ltx_font_italic">Remove</em>, <em class="ltx_emph ltx_font_italic">Adjust shape</em>, <em class="ltx_emph ltx_font_italic">Action</em>, <em class="ltx_emph ltx_font_italic">Stylization</em>, <em class="ltx_emph ltx_font_italic">Text editing</em>, <em class="ltx_emph ltx_font_italic">Color</em>, <em class="ltx_emph ltx_font_italic">Material</em>, and <em class="ltx_emph ltx_font_italic">Background</em> change, as well as more free-form editing tasks such as <em class="ltx_emph ltx_font_italic">Customization</em> or <em class="ltx_emph ltx_font_italic">Personalization</em> <cite class="ltx_cite ltx_citemacro_citep">(Gal et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib18" title="">2023</a>; Ruiz et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib60" title="">2023</a>; Kumari et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib38" title="">2023</a>)</cite>. Candidate instructions for each type are generated using Qwen2.5-32B VLM <cite class="ltx_cite ltx_citemacro_citep">(Qwen-Team, <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib57" title="">2025</a>)</cite>. Given an image-instruction pair, we further query the VLM to assess its validity and to suggest the caption, <math alttext="{\mathbf{c}}^{{\mathbf{x}}}" class="ltx_Math" display="inline" id="S4.SS1.p1.m3" intent=":literal"><semantics><msup><mi>𝐜</mi><mi>𝐱</mi></msup><annotation encoding="application/x-tex">{\mathbf{c}}^{{\mathbf{x}}}</annotation></semantics></math>, for the edited image. For the customization task, we restrict reference images to those showing a prominent central object, either filtered from a real image corpus or generated via the pretrained model <cite class="ltx_cite ltx_citemacro_citep">(Tan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib73" title="">2025</a>; Kumari et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib39" title="">2025</a>)</cite>, and prompt the VLM to generate a caption that places the object in a novel background or context. In total, for local and free-form editing instructions, our dataset consists of <math alttext="\sim 3" class="ltx_Math" display="inline" id="S4.SS1.p1.m4" intent=":literal"><semantics><mrow><mi></mi><mo>∼</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\sim 3</annotation></semantics></math>M and <math alttext="\sim 600" class="ltx_Math" display="inline" id="S4.SS1.p1.m5" intent=":literal"><semantics><mrow><mi></mi><mo>∼</mo><mn>600</mn></mrow><annotation encoding="application/x-tex">\sim 600</annotation></semantics></math>K reference images, respectively. The input prompt to the Qwen2.5-32B VLM for each setup is shown in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A4" title="Appendix D Dataset Construction Details ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training Objective</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p">Training a diffusion or flow-based model <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib26" title="">2020</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib47" title="">2023b</a>)</cite> for image editing without pairs presents a unique challenge. Standard diffusion training takes as input noised versions of a ground-truth image. In our setting, no such ground-truth edited image exists; thus, we cannot construct these intermediate noisy inputs. On the other hand, directly mapping noise to the edited image in a single step is naturally challenging and yields poor fidelity (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A2" title="Appendix B Ablation Study ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">B</span></a>). To address this, during training, we propose to unroll the backward diffusion trajectory starting from noise using a two-step sampling procedure <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib69" title="">2023a</a>)</cite>. Specifically, given the reference image–instruction pair <math alttext="({\mathbf{y}},{\mathbf{c}})" class="ltx_Math" display="inline" id="S4.SS2.p1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>𝐲</mi><mo>,</mo><mi>𝐜</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">({\mathbf{y}},{\mathbf{c}})</annotation></semantics></math>, the editing model <math alttext="G_{\theta}" class="ltx_Math" display="inline" id="S4.SS2.p1.m2" intent=":literal"><semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">G_{\theta}</annotation></semantics></math> first predicts a provisional clean image <math alttext="\hat{{\mathbf{x}}}_{\theta}^{0}" class="ltx_Math" display="inline" id="S4.SS2.p1.m3" intent=":literal"><semantics><msubsup><mover accent="true"><mi>𝐱</mi><mo>^</mo></mover><mi>θ</mi><mn>0</mn></msubsup><annotation encoding="application/x-tex">\hat{{\mathbf{x}}}_{\theta}^{0}</annotation></semantics></math> from noise <math alttext="\epsilon" class="ltx_Math" display="inline" id="S4.SS2.p1.m4" intent=":literal"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>. Then, a second step refines this estimate by feeding an interpolated noisy input back into the model:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E3">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E3X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{{\mathbf{x}}}_{\theta}^{0}" class="ltx_Math" display="inline" id="S4.E3X.m2" intent=":literal"><semantics><msubsup><mover accent="true"><mi>𝐱</mi><mo>^</mo></mover><mi>θ</mi><mn>0</mn></msubsup><annotation encoding="application/x-tex">\displaystyle\hat{{\mathbf{x}}}_{\theta}^{0}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\epsilon-\hat{{\mathbf{v}}}_{\theta}," class="ltx_Math" display="inline" id="S4.E3X.m3" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mi>ϵ</mi><mo>−</mo><msub><mover accent="true"><mi>𝐯</mi><mo>^</mo></mover><mi>θ</mi></msub></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\epsilon-\hat{{\mathbf{v}}}_{\theta},</annotation></semantics></math></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\text{where }\hat{{\mathbf{v}}}_{\theta}\equiv G_{\theta}(\epsilon,t=1,{\mathbf{c}},{\mathbf{y}}),\quad\epsilon\sim\mathcal{N}(\bm{0},\bm{I})," class="ltx_math_unparsed" display="inline" id="S4.E3X.m5" intent=":literal"><semantics><mrow><mtext>where </mtext><msub><mover accent="true"><mi>𝐯</mi><mo>^</mo></mover><mi>θ</mi></msub><mo>≡</mo><msub><mi>G</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><mi>ϵ</mi><mo>,</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>𝐜</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="false">)</mo></mrow><mo rspace="1.167em">,</mo><mi>ϵ</mi><mo>∼</mo><mi class="ltx_font_mathcaligraphic">𝒩</mi><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\text{where }\hat{{\mathbf{v}}}_{\theta}\equiv G_{\theta}(\epsilon,t=1,{\mathbf{c}},{\mathbf{y}}),\quad\epsilon\sim\mathcal{N}(\bm{0},\bm{I}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E3Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle{\mathbf{x}}_{\theta}^{0}" class="ltx_Math" display="inline" id="S4.E3Xa.m2" intent=":literal"><semantics><msubsup><mi>𝐱</mi><mi>θ</mi><mn>0</mn></msubsup><annotation encoding="application/x-tex">\displaystyle{\mathbf{x}}_{\theta}^{0}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\hat{{\mathbf{x}}}_{\theta}^{t}-t{\mathbf{v}}_{\theta}," class="ltx_Math" display="inline" id="S4.E3Xa.m3" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><msubsup><mover accent="true"><mi>𝐱</mi><mo>^</mo></mover><mi>θ</mi><mi>t</mi></msubsup><mo>−</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝐯</mi><mi>θ</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\hat{{\mathbf{x}}}_{\theta}^{t}-t{\mathbf{v}}_{\theta},</annotation></semantics></math></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\text{where }{\mathbf{v}}_{\theta}\equiv G_{\theta}(\hat{{\mathbf{x}}}_{\theta}^{t},t,{\mathbf{c}},{\mathbf{y}}),\quad\hat{{\mathbf{x}}}_{\theta}^{t}=(1-t)\hat{{\mathbf{x}}}_{\theta}^{0}+t\epsilon;\epsilon\sim\mathcal{N}(\bm{0},\bm{I}),t\sim(0,1)." class="ltx_Math" display="inline" id="S4.E3Xa.m5" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mtext>where </mtext><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝐯</mi><mi>θ</mi></msub></mrow><mo>≡</mo><mrow><msub><mi>G</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>𝐱</mi><mo>^</mo></mover><mi>θ</mi><mi>t</mi></msubsup><mo>,</mo><mi>t</mi><mo>,</mo><mi>𝐜</mi><mo>,</mo><mi>𝐲</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msubsup><mover accent="true"><mi>𝐱</mi><mo>^</mo></mover><mi>θ</mi><mi>t</mi></msubsup><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>t</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mover accent="true"><mi>𝐱</mi><mo>^</mo></mover><mi>θ</mi><mn>0</mn></msubsup></mrow><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>ϵ</mi></mrow></mrow></mrow><mo>;</mo><mrow><mrow><mi>ϵ</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mi>t</mi><mo>∼</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\text{where }{\mathbf{v}}_{\theta}\equiv G_{\theta}(\hat{{\mathbf{x}}}_{\theta}^{t},t,{\mathbf{c}},{\mathbf{y}}),\quad\hat{{\mathbf{x}}}_{\theta}^{t}=(1-t)\hat{{\mathbf{x}}}_{\theta}^{0}+t\epsilon;\epsilon\sim\mathcal{N}(\bm{0},\bm{I}),t\sim(0,1).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p">With the second step, the model is now trained on noisy intermediate states at timesteps determined by <math alttext="t" class="ltx_Math" display="inline" id="S4.SS2.p2.m1" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, while being more efficient than a full backward unroll. In our method, we focus on <em class="ltx_emph ltx_font_italic">few-step</em> generation—specifically four steps—and restrict <math alttext="t\in[0.25,0.5,0.75]" class="ltx_Math" display="inline" id="S4.SS2.p2.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">t\in[0.25,0.5,0.75]</annotation></semantics></math> in the second step. Few-step generator provides a better estimate of the denoised image, <math alttext="{\mathbf{x}}^{0}_{\theta}" class="ltx_Math" display="inline" id="S4.SS2.p2.m3" intent=":literal"><semantics><msubsup><mi>𝐱</mi><mi>θ</mi><mn>0</mn></msubsup><annotation encoding="application/x-tex">{\mathbf{x}}^{0}_{\theta}</annotation></semantics></math>, at intermediate steps, which in turn enables effective VLM-based feedback. VLMs tend to give unreliable judgments when inputs are noisy or blurry (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A2" title="Appendix B Ablation Study ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">B</span></a>). This also enables faster inference and lowers training costs.</p>
</div>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="245" id="S4.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold">Method.</span> We fine-tune a pretrained text-to-image model into a few-step image-editing model using differentiable VLM-feedback regarding edit success. In addition, we use distribution matching loss (DMD <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib86" title="">2024a</a>)</cite>) to ensure output images remain in the natural image manifold.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">VLM-based editing loss.</span>
To evaluate whether an edit is successfully applied in <math alttext="{\mathbf{x}}^{0}_{\theta}" class="ltx_Math" display="inline" id="S4.SS2.p3.m1" intent=":literal"><semantics><msubsup><mi>𝐱</mi><mi>θ</mi><mn>0</mn></msubsup><annotation encoding="application/x-tex">{\mathbf{x}}^{0}_{\theta}</annotation></semantics></math>, we define a set of template questions with corresponding ground truth answers, <math alttext="\mathcal{D}_{\text{QA}}=\{(\mathbf{X}_{q_{j}},\mathbf{X}_{a_{j}})\}_{j}" class="ltx_Math" display="inline" id="S4.SS2.p3.m2" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>QA</mtext></msub><mo>=</mo><msub><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐗</mi><msub><mi>q</mi><mi>j</mi></msub></msub><mo>,</mo><msub><mi>𝐗</mi><msub><mi>a</mi><mi>j</mi></msub></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{D}_{\text{QA}}=\{(\mathbf{X}_{q_{j}},\mathbf{X}_{a_{j}})\}_{j}</annotation></semantics></math>, tailored to each edit category.
The VLM is instructed to answer with a binary yes and no answer, i.e., <math alttext="\mathbf{X}_{a_{j}}\in\{\text{\emph{Yes}},\text{\emph{No}}\}" class="ltx_Math" display="inline" id="S4.SS2.p3.m3" intent=":literal"><semantics><mrow><msub><mi>𝐗</mi><msub><mi>a</mi><mi>j</mi></msub></msub><mo>∈</mo><mrow><mo stretchy="false">{</mo><mtext class="ltx_mathvariant_italic"><em class="ltx_emph ltx_font_italic">Yes</em></mtext><mo>,</mo><mtext class="ltx_mathvariant_italic"><em class="ltx_emph ltx_font_italic">No</em></mtext><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{X}_{a_{j}}\in\{\text{\emph{Yes}},\text{\emph{No}}\}</annotation></semantics></math>, and <math alttext="\mathbf{X}_{\bar{a}_{j}}" class="ltx_Math" display="inline" id="S4.SS2.p3.m4" intent=":literal"><semantics><msub><mi>𝐗</mi><msub><mover accent="true"><mi>a</mi><mo>¯</mo></mover><mi>j</mi></msub></msub><annotation encoding="application/x-tex">\mathbf{X}_{\bar{a}_{j}}</annotation></semantics></math> denotes the opposite response.
The loss is then a binary cross-entropy over the predicted logit difference between the tokens corresponding to the correct and opposite responses, respectively.</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E4">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E4X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{VLM}}" class="ltx_Math" display="inline" id="S4.E4X.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>VLM</mtext></msub><annotation encoding="application/x-tex">\displaystyle\mathcal{L}_{\text{VLM}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=-\sum_{j}\log p(a_{j}),\text{ where }p(a_{j})=\sigma(\ell^{(j)}_{a_{j}}-\ell^{(j)}_{\bar{a}_{j}})" class="ltx_Math" display="inline" id="S4.E4X.m3" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>j</mi></munder></mstyle><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><mi>p</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>a</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mtext> where </mtext><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>a</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>σ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi mathvariant="normal">ℓ</mi><msub><mi>a</mi><mi>j</mi></msub><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>−</mo><msubsup><mi mathvariant="normal">ℓ</mi><msub><mover accent="true"><mi>a</mi><mo>¯</mo></mover><mi>j</mi></msub><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=-\sum_{j}\log p(a_{j}),\text{ where }p(a_{j})=\sigma(\ell^{(j)}_{a_{j}}-\ell^{(j)}_{\bar{a}_{j}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="\ell^{(j)}_{a_{j}}" class="ltx_Math" display="inline" id="S4.SS2.p3.m5" intent=":literal"><semantics><msubsup><mi mathvariant="normal">ℓ</mi><msub><mi>a</mi><mi>j</mi></msub><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msubsup><annotation encoding="application/x-tex">\ell^{(j)}_{a_{j}}</annotation></semantics></math> is the logit corresponding to the token <math alttext="\mathbf{X}_{a_{j}}" class="ltx_Math" display="inline" id="S4.SS2.p3.m6" intent=":literal"><semantics><msub><mi>𝐗</mi><msub><mi>a</mi><mi>j</mi></msub></msub><annotation encoding="application/x-tex">\mathbf{X}_{a_{j}}</annotation></semantics></math>, <math alttext="\sigma" class="ltx_Math" display="inline" id="S4.SS2.p3.m7" intent=":literal"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math> is the sigmoid function, and <math alttext="p(a_{j})" class="ltx_Math" display="inline" id="S4.SS2.p3.m8" intent=":literal"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>a</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(a_{j})</annotation></semantics></math> is the probability of correct answer, while restricting normalization to only the <em class="ltx_emph ltx_font_italic">Yes</em> and <em class="ltx_emph ltx_font_italic">No</em> tokens, which we observe to be more effective during training <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib91" title="">2024</a>)</cite>. Computing this loss is relatively fast, as it only requires a single forward call to the VLM per question, as opposed to autoregressive token prediction.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p">For each edit instruction, we use two complementary questions to compute the editing loss: (1) <em class="ltx_emph ltx_font_italic">Edit-verification</em> question to assess whether the intended edit is applied, and (2) <em class="ltx_emph ltx_font_italic">Identity-preservation</em> question to ensure the image is not over-edited and is consistent with the reference image. Specifically, for the local image-editing instructions, we verify edit success with the following question “The objective is to evaluate if the editing instruction has been executed in the second image. Editing instruction: <math alttext="\{" class="ltx_Math" display="inline" id="S4.SS2.p4.m1" intent=":literal"><semantics><mo stretchy="false">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math>edit instruction<math alttext="\}" class="ltx_Math" display="inline" id="S4.SS2.p4.m2" intent=":literal"><semantics><mo stretchy="false">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math>. Answer with a Yes or No.” except <em class="ltx_emph ltx_font_italic">removal</em> edit-type, where we directly evaluate if the intended object is removed by asking “Answer with a Yes or No if the image has <math alttext="\{" class="ltx_Math" display="inline" id="S4.SS2.p4.m3" intent=":literal"><semantics><mo stretchy="false">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math>object name<math alttext="\}" class="ltx_Math" display="inline" id="S4.SS2.p4.m4" intent=":literal"><semantics><mo stretchy="false">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math>”. For the identity-preservation question, we ask the following: “Answer with a Yes or No if the second image is exactly the same as the first image. IGNORE the changes in the second image because of the edit: <math alttext="\{" class="ltx_Math" display="inline" id="S4.SS2.p4.m5" intent=":literal"><semantics><mo stretchy="false">{</mo><annotation encoding="application/x-tex">\{</annotation></semantics></math>edit instruction<math alttext="\}" class="ltx_Math" display="inline" id="S4.SS2.p4.m6" intent=":literal"><semantics><mo stretchy="false">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math>”. We provide the list of all questions along with their system and user prompts for all editing types, including free-form editing in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A5" title="Appendix E Training Implementation Details ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Distribution matching with text-to-image teacher model.</span>   While VLM feedback evaluates the efficacy of instruction following, it does not enforce the generated outputs to remain in the real image domain. To ensure this and keep the output distribution of the generator aligned with the pre-trained model, we apply Distribution Matching Distillation (DMD) <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib87" title="">2024b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib86" title="">a</a>)</cite> between the fine-tuned model, <math alttext="G_{\theta}" class="ltx_Math" display="inline" id="S4.SS2.p5.m1" intent=":literal"><semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">G_{\theta}</annotation></semantics></math>, and the pre-trained text-to-image (teacher) model, <math alttext="G_{\text{init}}" class="ltx_Math" display="inline" id="S4.SS2.p5.m2" intent=":literal"><semantics><msub><mi>G</mi><mtext>init</mtext></msub><annotation encoding="application/x-tex">G_{\text{init}}</annotation></semantics></math>. DMD minimizes the Kullback–Leibler (KL) divergence between the real image distribution, as estimated by the teacher model, and the output distribution of the fine-tuned model. The gradient of this KL-divergence loss with respect to the generator parameters can be simplified to:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E5">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E5X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\nabla_{\theta}D_{KL}" class="ltx_Math" display="inline" id="S4.E5X.m2" intent=":literal"><semantics><mrow><msub><mo>∇</mo><mi>θ</mi></msub><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\displaystyle\nabla_{\theta}D_{KL}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\mathbb{E}_{\begin{subarray}{c}\epsilon\sim\mathcal{N}(\bm{0},\bm{I}),t\in(0,1),{\mathbf{x}}^{0}_{\theta}\end{subarray}}\Big[-\big({\mathbf{v}}_{\text{real}}({\mathbf{x}}^{t}_{\theta},t,{\mathbf{c}}^{{\mathbf{x}}})-{\mathbf{v}}_{\text{gen}}({\mathbf{x}}^{t}_{\theta},t,{\mathbf{c}}^{{\mathbf{x}}})\big)\hskip 1.42262pt\frac{dG}{d\theta}\Big]," class="ltx_Math" display="inline" id="S4.E5X.m3" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><msub><mi>𝔼</mi><mtable><mtr><mtd><mrow><mrow><mi>ϵ</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mi>t</mi><mo>∈</mo><mrow><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo>,</mo><msubsup><mi>𝐱</mi><mi>θ</mi><mn>0</mn></msubsup></mrow></mrow></mrow></mtd></mtr></mtable></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.600em" minsize="1.600em">[</mo><mrow><mo>−</mo><mrow><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><mrow><msub><mi>𝐯</mi><mtext>real</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝐱</mi><mi>θ</mi><mi>t</mi></msubsup><mo>,</mo><mi>t</mi><mo>,</mo><msup><mi>𝐜</mi><mi>𝐱</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>𝐯</mi><mtext>gen</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝐱</mi><mi>θ</mi><mi>t</mi></msubsup><mo>,</mo><mi>t</mi><mo>,</mo><msup><mi>𝐜</mi><mi>𝐱</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow><mo lspace="0.140em" rspace="0em">​</mo><mstyle displaystyle="true"><mfrac><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>G</mi></mrow><mrow><mi>d</mi><mo lspace="0em" rspace="0em">​</mo><mi>θ</mi></mrow></mfrac></mstyle></mrow></mrow><mo maxsize="1.600em" minsize="1.600em">]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\mathbb{E}_{\begin{subarray}{c}\epsilon\sim\mathcal{N}(\bm{0},\bm{I}),t\in(0,1),{\mathbf{x}}^{0}_{\theta}\end{subarray}}\Big[-\big({\mathbf{v}}_{\text{real}}({\mathbf{x}}^{t}_{\theta},t,{\mathbf{c}}^{{\mathbf{x}}})-{\mathbf{v}}_{\text{gen}}({\mathbf{x}}^{t}_{\theta},t,{\mathbf{c}}^{{\mathbf{x}}})\big)\hskip 1.42262pt\frac{dG}{d\theta}\Big],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(5)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p">where <math alttext="{\mathbf{c}}^{{\mathbf{x}}}" class="ltx_Math" display="inline" id="S4.SS2.p5.m3" intent=":literal"><semantics><msup><mi>𝐜</mi><mi>𝐱</mi></msup><annotation encoding="application/x-tex">{\mathbf{c}}^{{\mathbf{x}}}</annotation></semantics></math> is the text caption describing the noisy edited image <math alttext="{\mathbf{x}}^{t}_{\theta}" class="ltx_Math" display="inline" id="S4.SS2.p5.m4" intent=":literal"><semantics><msubsup><mi>𝐱</mi><mi>θ</mi><mi>t</mi></msubsup><annotation encoding="application/x-tex">{\mathbf{x}}^{t}_{\theta}</annotation></semantics></math> and <math alttext="{\mathbf{v}}_{\text{real}}" class="ltx_Math" display="inline" id="S4.SS2.p5.m5" intent=":literal"><semantics><msub><mi>𝐯</mi><mtext>real</mtext></msub><annotation encoding="application/x-tex">{\mathbf{v}}_{\text{real}}</annotation></semantics></math>, <math alttext="{\mathbf{v}}_{\text{gen}}" class="ltx_Math" display="inline" id="S4.SS2.p5.m6" intent=":literal"><semantics><msub><mi>𝐯</mi><mtext>gen</mtext></msub><annotation encoding="application/x-tex">{\mathbf{v}}_{\text{gen}}</annotation></semantics></math> represents the predicted velocity from the teacher and a trainable auxiliary model, <math alttext="A_{\phi}" class="ltx_Math" display="inline" id="S4.SS2.p5.m7" intent=":literal"><semantics><msub><mi>A</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">A_{\phi}</annotation></semantics></math> respectively. The auxiliary model is trained along with <math alttext="G_{\theta}" class="ltx_Math" display="inline" id="S4.SS2.p5.m8" intent=":literal"><semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">G_{\theta}</annotation></semantics></math> to learn the current output distribution of <math alttext="G_{\theta}" class="ltx_Math" display="inline" id="S4.SS2.p5.m9" intent=":literal"><semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">G_{\theta}</annotation></semantics></math> using a flow-based denoising objective. This loss ensures that the edited images not only satisfy the instruction but also remain faithful to the text conditioned distribution of real images modeled by the pretrained teacher.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Training details</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p">The pretrained model <math alttext="G_{\theta}" class="ltx_Math" display="inline" id="S4.SS3.p1.m1" intent=":literal"><semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">G_{\theta}</annotation></semantics></math> is originally designed to generate an image, <math alttext="{\mathbf{x}}" class="ltx_Math" display="inline" id="S4.SS3.p1.m2" intent=":literal"><semantics><mi>𝐱</mi><annotation encoding="application/x-tex">{\mathbf{x}}</annotation></semantics></math>, conditioned only on text <math alttext="{\mathbf{c}}" class="ltx_Math" display="inline" id="S4.SS3.p1.m3" intent=":literal"><semantics><mi>𝐜</mi><annotation encoding="application/x-tex">{\mathbf{c}}</annotation></semantics></math>. To adapt it to our editing task, we extend its conditioning to include the reference image <math alttext="{\mathbf{y}}" class="ltx_Math" display="inline" id="S4.SS3.p1.m4" intent=":literal"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">{\mathbf{y}}</annotation></semantics></math>. Following recent works <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib78" title="">2025</a>; Tan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib73" title="">2025</a>)</cite>, we concatenate the VAE encoding of the reference image to the noisy target image encoding along the token sequence dimension, similar to text embedding, thereby enabling the model to attend to both text and visual conditions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p">To stabilize training, in the initial few iterations, we train the model with the objective of simply reconstructing the concatenated reference image. This encourages the network to propagate content from the reference input, aligning it toward producing realistic images under joint text–image conditioning. After this, we introduce our main training objective as explained in the previous section.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p">The final loss for the generator is a weighted combination of the VLM-based editing loss and DMD loss. The auxiliary network, <math alttext="A_{\phi}" class="ltx_Math" display="inline" id="S4.SS3.p3.m1" intent=":literal"><semantics><msub><mi>A</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">A_{\phi}</annotation></semantics></math>, is updated <math alttext="N_{\text{aux}}" class="ltx_Math" display="inline" id="S4.SS3.p3.m2" intent=":literal"><semantics><msub><mi>N</mi><mtext>aux</mtext></msub><annotation encoding="application/x-tex">N_{\text{aux}}</annotation></semantics></math> times for every generator, <math alttext="G_{\theta}" class="ltx_Math" display="inline" id="S4.SS3.p3.m3" intent=":literal"><semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">G_{\theta}</annotation></semantics></math>, update <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib86" title="">2024a</a>)</cite>. Our pre-trained generative model is a 2B parameter internal DiT-based <cite class="ltx_cite ltx_citemacro_citep">(Peebles &amp; Xie, <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib55" title="">2023</a>)</cite> latent space diffusion model. The overall training pipeline is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S4.F1" title="Figure 1 ‣ 4.2 Training Objective ‣ 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">1</span></a> and is detailed more formally in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#alg1" title="In 4.3 Training details ‣ 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">1</span></a> below. Other training hyperparameters are detailed in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A5" title="Appendix E Training Implementation Details ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<figure class="ltx_float ltx_algorithm" id="alg1">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div class="ltx_listingline">
<span class="ltx_text ltx_font_bold">Input:</span> Pretrained <span class="ltx_text ltx_markedasmath">VLM</span> and text-to-image model <math alttext="G_{\text{init}}" class="ltx_Math" display="inline" id="alg1.m2" intent=":literal"><semantics><msub><mi>G</mi><mtext>init</mtext></msub><annotation encoding="application/x-tex">G_{\text{init}}</annotation></semantics></math>, Dataset <math alttext="\mathcal{X}=\{({\mathbf{y}}_{i},{\mathbf{c}}_{i},{\mathbf{c}}^{\mathbf{y}}_{i},{\mathbf{c}}^{\mathbf{x}}_{i})\}" class="ltx_Math" display="inline" id="alg1.m3" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐲</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐜</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>𝐜</mi><mi>i</mi><mi>𝐲</mi></msubsup><mo>,</mo><msubsup><mi>𝐜</mi><mi>i</mi><mi>𝐱</mi></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{X}=\{({\mathbf{y}}_{i},{\mathbf{c}}_{i},{\mathbf{c}}^{\mathbf{y}}_{i},{\mathbf{c}}^{\mathbf{x}}_{i})\}</annotation></semantics></math>.
</div>
<div class="ltx_listingline">
<span class="ltx_text ltx_font_bold">Output:</span> Few-step image-editing model <math alttext="G_{\theta}" class="ltx_Math" display="inline" id="alg1.m4" intent=":literal"><semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">G_{\theta}</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span>
<math alttext="G_{\theta}\leftarrow\text{copyWeights}(G_{\text{init}})" class="ltx_Math" display="inline" id="alg1.m5" intent=":literal"><semantics><mrow><msub><mi>G</mi><mi>θ</mi></msub><mo stretchy="false">←</mo><mrow><mtext>copyWeights</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>G</mi><mtext>init</mtext></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">G_{\theta}\leftarrow\text{copyWeights}(G_{\text{init}})</annotation></semantics></math>;  <math alttext="A_{\phi}\leftarrow\text{copyWeights}(G_{\text{init}})" class="ltx_Math" display="inline" id="alg1.m6" intent=":literal"><semantics><mrow><msub><mi>A</mi><mi>ϕ</mi></msub><mo stretchy="false">←</mo><mrow><mtext>copyWeights</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>G</mi><mtext>init</mtext></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">A_{\phi}\leftarrow\text{copyWeights}(G_{\text{init}})</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span><span class="ltx_text ltx_font_typewriter">// </span><span class="ltx_text ltx_font_typewriter">Warmup with identity loss </span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span>
<span class="ltx_text ltx_font_bold">for</span> <em class="ltx_emph ltx_font_italic"><math alttext="\text{step}=1" class="ltx_Math" display="inline" id="alg1.m7" intent=":literal"><semantics><mrow><mtext class="ltx_mathvariant_italic">step</mtext><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\text{step}=1</annotation></semantics></math> <span class="ltx_text ltx_font_bold ltx_font_upright">to</span> <math alttext="N_{\text{warmup}}" class="ltx_Math" display="inline" id="alg1.m8" intent=":literal"><semantics><msub><mi>N</mi><mtext class="ltx_mathvariant_italic"><em class="ltx_emph" style="font-size:70%;">warmup</em></mtext></msub><annotation encoding="application/x-tex">N_{\text{warmup}}</annotation></semantics></math></em> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 
<math alttext="({\mathbf{y}},{\mathbf{c}}^{\mathbf{y}})\sim\mathcal{X}" class="ltx_Math" display="inline" id="alg1.m9" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><mi>𝐲</mi><mo>,</mo><msup><mi>𝐜</mi><mi>𝐲</mi></msup><mo stretchy="false">)</mo></mrow><mo>∼</mo><mi class="ltx_font_mathcaligraphic">𝒳</mi></mrow><annotation encoding="application/x-tex">({\mathbf{y}},{\mathbf{c}}^{\mathbf{y}})\sim\mathcal{X}</annotation></semantics></math>, <math alttext="\epsilon\sim\mathcal{N}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="alg1.m10" intent=":literal"><semantics><mrow><mi>ϵ</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\epsilon\sim\mathcal{N}(\bm{0},\bm{I})</annotation></semantics></math>, <math alttext="t\sim(0,1]" class="ltx_Math" display="inline" id="alg1.m11" intent=":literal"><semantics><mrow><mi>t</mi><mo>∼</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">t\sim(0,1]</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="{\mathbf{x}}\leftarrow{\mathbf{y}}" class="ltx_Math" display="inline" id="alg1.m12" intent=":literal"><semantics><mrow><mi>𝐱</mi><mo stretchy="false">←</mo><mi>𝐲</mi></mrow><annotation encoding="application/x-tex">{\mathbf{x}}\leftarrow{\mathbf{y}}</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="{\mathbf{x}}^{t}\leftarrow(1-t){\mathbf{x}}+t\epsilon" class="ltx_Math" display="inline" id="alg1.m13" intent=":literal"><semantics><mrow><msup><mi>𝐱</mi><mi>t</mi></msup><mo stretchy="false">←</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>t</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>𝐱</mi></mrow><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>ϵ</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">{\mathbf{x}}^{t}\leftarrow(1-t){\mathbf{x}}+t\epsilon</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="{\mathbf{v}}_{\theta}\leftarrow G_{\theta}({\mathbf{x}}^{t},t,{\mathbf{y}},{\mathbf{c}}^{\mathbf{y}})" class="ltx_Math" display="inline" id="alg1.m14" intent=":literal"><semantics><mrow><msub><mi>𝐯</mi><mi>θ</mi></msub><mo stretchy="false">←</mo><mrow><msub><mi>G</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>𝐱</mi><mi>t</mi></msup><mo>,</mo><mi>t</mi><mo>,</mo><mi>𝐲</mi><mo>,</mo><msup><mi>𝐜</mi><mi>𝐲</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">{\mathbf{v}}_{\theta}\leftarrow G_{\theta}({\mathbf{x}}^{t},t,{\mathbf{y}},{\mathbf{c}}^{\mathbf{y}})</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="\mathcal{L}_{\text{id}}\leftarrow||{\mathbf{v}}-{\mathbf{v}}_{\theta}||\;\text{where}\;{\mathbf{v}}=\epsilon-{\mathbf{x}}" class="ltx_Math" display="inline" id="alg1.m15" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>id</mtext></msub><mo stretchy="false">←</mo><mrow><mrow><mo stretchy="false">‖</mo><mrow><mi>𝐯</mi><mo>−</mo><msub><mi>𝐯</mi><mi>θ</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mtext>where</mtext><mo lspace="0.280em" rspace="0em">​</mo><mi>𝐯</mi></mrow><mo>=</mo><mrow><mi>ϵ</mi><mo>−</mo><mi>𝐱</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{id}}\leftarrow||{\mathbf{v}}-{\mathbf{v}}_{\theta}||\;\text{where}\;{\mathbf{v}}=\epsilon-{\mathbf{x}}</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="\theta_{G}\leftarrow\theta_{G}-\eta_{G}\nabla_{\theta_{G}}\,\mathcal{L}_{\text{id}}" class="ltx_Math" display="inline" id="alg1.m16" intent=":literal"><semantics><mrow><msub><mi>θ</mi><mi>G</mi></msub><mo stretchy="false">←</mo><mrow><msub><mi>θ</mi><mi>G</mi></msub><mo>−</mo><mrow><msub><mi>η</mi><mi>G</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mo rspace="0.167em">∇</mo><msub><mi>θ</mi><mi>G</mi></msub></msub><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>id</mtext></msub></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta_{G}\leftarrow\theta_{G}-\eta_{G}\nabla_{\theta_{G}}\,\mathcal{L}_{\text{id}}</annotation></semantics></math>.

</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11</span> end for
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">12</span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">13</span> 

</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">14</span><span class="ltx_text ltx_font_typewriter">// </span><span class="ltx_text ltx_font_typewriter">Main training loop </span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">15</span>
<span class="ltx_text ltx_font_bold">while</span> <em class="ltx_emph ltx_font_typewriter ltx_font_italic">train</em> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">16</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 
<math alttext="\{({\mathbf{y}},{\mathbf{c}},{\mathbf{c}}^{{\mathbf{x}}})\}\sim\mathcal{X},\epsilon\sim\mathcal{N}(0,I),t\in[0.25,0.5,0.75,1]" class="ltx_Math" display="inline" id="alg1.m17" intent=":literal"><semantics><mrow><mrow><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><mi>𝐲</mi><mo>,</mo><mi>𝐜</mi><mo>,</mo><msup><mi>𝐜</mi><mi>𝐱</mi></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mo>∼</mo><mi class="ltx_font_mathcaligraphic">𝒳</mi></mrow><mo>,</mo><mrow><mrow><mi>ϵ</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0.25</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.75</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\{({\mathbf{y}},{\mathbf{c}},{\mathbf{c}}^{{\mathbf{x}}})\}\sim\mathcal{X},\epsilon\sim\mathcal{N}(0,I),t\in[0.25,0.5,0.75,1]</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">17</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="{\mathbf{v}}_{\theta}\leftarrow G_{\theta}(\epsilon,t=1,{\mathbf{y}},{\mathbf{c}})" class="ltx_math_unparsed" display="inline" id="alg1.m18" intent=":literal"><semantics><mrow><msub><mi>𝐯</mi><mi>θ</mi></msub><mo stretchy="false">←</mo><msub><mi>G</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><mi>ϵ</mi><mo>,</mo><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>𝐲</mi><mo>,</mo><mi>𝐜</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">{\mathbf{v}}_{\theta}\leftarrow G_{\theta}(\epsilon,t=1,{\mathbf{y}},{\mathbf{c}})</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">18</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="{\mathbf{x}}^{0}_{\theta}\leftarrow\epsilon-{\mathbf{v}}_{\theta}\quad" class="ltx_Math" display="inline" id="alg1.m19" intent=":literal"><semantics><mrow><mrow><msubsup><mi>𝐱</mi><mi>θ</mi><mn>0</mn></msubsup><mo stretchy="false">←</mo><mrow><mi>ϵ</mi><mo>−</mo><msub><mi>𝐯</mi><mi>θ</mi></msub></mrow></mrow><mspace style="width:1em;" width="1em"></mspace></mrow><annotation encoding="application/x-tex">{\mathbf{x}}^{0}_{\theta}\leftarrow\epsilon-{\mathbf{v}}_{\theta}\quad</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">19</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <span class="ltx_text ltx_font_bold">if</span> <em class="ltx_emph ltx_font_italic"><math alttext="t&lt;1" class="ltx_Math" display="inline" id="alg1.m20" intent=":literal"><semantics><mrow><mi>t</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t&lt;1</annotation></semantics></math></em> <span class="ltx_text ltx_font_bold">then</span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">20</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 

</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">21</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="{\mathbf{v}}_{\theta}\leftarrow G_{\theta}({\mathbf{x}}^{t}_{\theta},t,{\mathbf{y}},{\mathbf{c}});\quad{\mathbf{x}}^{t}_{\theta}\leftarrow(1-t){\mathbf{x}}^{0}_{\theta}\ +t\epsilon\;;\;\;\;\epsilon\sim\mathcal{N}(\bm{0},\bm{I})" class="ltx_Math" display="inline" id="alg1.m21" intent=":literal"><semantics><mrow><mrow><msub><mi>𝐯</mi><mi>θ</mi></msub><mo stretchy="false">←</mo><mrow><msub><mi>G</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝐱</mi><mi>θ</mi><mi>t</mi></msubsup><mo>,</mo><mi>t</mi><mo>,</mo><mi>𝐲</mi><mo>,</mo><mi>𝐜</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">;</mo><mrow><mrow><msubsup><mi>𝐱</mi><mi>θ</mi><mi>t</mi></msubsup><mo stretchy="false">←</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>t</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝐱</mi><mi>θ</mi><mn>0</mn></msubsup></mrow><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>ϵ</mi></mrow></mrow></mrow><mo lspace="0.280em" rspace="0.997em">;</mo><mrow><mi>ϵ</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">{\mathbf{v}}_{\theta}\leftarrow G_{\theta}({\mathbf{x}}^{t}_{\theta},t,{\mathbf{y}},{\mathbf{c}});\quad{\mathbf{x}}^{t}_{\theta}\leftarrow(1-t){\mathbf{x}}^{0}_{\theta}\ +t\epsilon\;;\;\;\;\epsilon\sim\mathcal{N}(\bm{0},\bm{I})</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">22</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="{\mathbf{x}}^{0}_{\theta}\leftarrow{\mathbf{x}}^{t}_{\theta}-t{\mathbf{v}}_{\theta}" class="ltx_Math" display="inline" id="alg1.m22" intent=":literal"><semantics><mrow><msubsup><mi>𝐱</mi><mi>θ</mi><mn>0</mn></msubsup><mo stretchy="false">←</mo><mrow><msubsup><mi>𝐱</mi><mi>θ</mi><mi>t</mi></msubsup><mo>−</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><msub><mi>𝐯</mi><mi>θ</mi></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">{\mathbf{x}}^{0}_{\theta}\leftarrow{\mathbf{x}}^{t}_{\theta}-t{\mathbf{v}}_{\theta}</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">23</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  end if
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">24</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">25</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="\text{Compute }\mathcal{L}_{\text{VLM}}" class="ltx_Math" display="inline" id="alg1.m23" intent=":literal"><semantics><mrow><mtext>Compute </mtext><mo lspace="0em" rspace="0em">​</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>VLM</mtext></msub></mrow><annotation encoding="application/x-tex">\text{Compute }\mathcal{L}_{\text{VLM}}</annotation></semantics></math> <span class="ltx_text ltx_font_typewriter">// </span><span class="ltx_text ltx_font_typewriter">Eqn. <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S4.E4" title="In 4.2 Training Objective ‣ 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">4</span></a> </span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">26</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 
<math alttext="\text{Compute }\nabla_{\theta}D_{KL}" class="ltx_Math" display="inline" id="alg1.m24" intent=":literal"><semantics><mrow><mtext>Compute </mtext><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">\text{Compute }\nabla_{\theta}D_{KL}</annotation></semantics></math> <span class="ltx_text ltx_font_typewriter">// </span><span class="ltx_text ltx_font_typewriter">Eqn. <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S4.E5" title="In 4.2 Training Objective ‣ 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">5</span></a> </span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">27</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 

</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">28</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="\theta_{G}\leftarrow\theta_{G}-\eta_{G}\lambda_{\text{vlm}}\nabla_{\theta_{G}}\mathcal{L}_{\text{VLM}}-\eta_{G}\lambda_{\text{dmd}}\nabla_{\theta}D_{KL}" class="ltx_Math" display="inline" id="alg1.m25" intent=":literal"><semantics><mrow><msub><mi>θ</mi><mi>G</mi></msub><mo stretchy="false">←</mo><mrow><msub><mi>θ</mi><mi>G</mi></msub><mo>−</mo><mrow><msub><mi>η</mi><mi>G</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>λ</mi><mtext>vlm</mtext></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mo rspace="0.167em">∇</mo><msub><mi>θ</mi><mi>G</mi></msub></msub><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>VLM</mtext></msub></mrow></mrow><mo>−</mo><mrow><msub><mi>η</mi><mi>G</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>λ</mi><mtext>dmd</mtext></msub><mo lspace="0.167em" rspace="0em">​</mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><msub><mi>D</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow></msub></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta_{G}\leftarrow\theta_{G}-\eta_{G}\lambda_{\text{vlm}}\nabla_{\theta_{G}}\mathcal{L}_{\text{VLM}}-\eta_{G}\lambda_{\text{dmd}}\nabla_{\theta}D_{KL}</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">29</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 

</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">30</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <span class="ltx_text ltx_font_bold">for</span> <em class="ltx_emph ltx_font_italic"><math alttext="\text{local step}=1" class="ltx_Math" display="inline" id="alg1.m26" intent=":literal"><semantics><mrow><mtext class="ltx_mathvariant_italic">local step</mtext><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\text{local step}=1</annotation></semantics></math> <span class="ltx_text ltx_font_bold ltx_font_upright">to</span> <math alttext="N_{\text{aux}}" class="ltx_Math" display="inline" id="alg1.m27" intent=":literal"><semantics><msub><mi>N</mi><mtext class="ltx_mathvariant_italic"><em class="ltx_emph" style="font-size:70%;">aux</em></mtext></msub><annotation encoding="application/x-tex">N_{\text{aux}}</annotation></semantics></math></em> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">31</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 

</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">32</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="\epsilon\sim\mathcal{N}(\bm{0},\bm{I}),\{({\mathbf{y}},{\mathbf{c}},{\mathbf{c}}^{{\mathbf{x}}})\}\sim\mathcal{X}" class="ltx_Math" display="inline" id="alg1.m28" intent=":literal"><semantics><mrow><mrow><mi>ϵ</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><mi>𝐲</mi><mo>,</mo><mi>𝐜</mi><mo>,</mo><msup><mi>𝐜</mi><mi>𝐱</mi></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mo>∼</mo><mi class="ltx_font_mathcaligraphic">𝒳</mi></mrow></mrow><annotation encoding="application/x-tex">\epsilon\sim\mathcal{N}(\bm{0},\bm{I}),\{({\mathbf{y}},{\mathbf{c}},{\mathbf{c}}^{{\mathbf{x}}})\}\sim\mathcal{X}</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">33</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="{\mathbf{x}}_{\theta}^{0}\leftarrow G_{\theta}(\epsilon,{\mathbf{y}},{\mathbf{c}})" class="ltx_Math" display="inline" id="alg1.m29" intent=":literal"><semantics><mrow><msubsup><mi>𝐱</mi><mi>θ</mi><mn>0</mn></msubsup><mo stretchy="false">←</mo><mrow><msub><mi>G</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ϵ</mi><mo>,</mo><mi>𝐲</mi><mo>,</mo><mi>𝐜</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">{\mathbf{x}}_{\theta}^{0}\leftarrow G_{\theta}(\epsilon,{\mathbf{y}},{\mathbf{c}})</annotation></semantics></math> <span class="ltx_text ltx_font_typewriter">// </span><span class="ltx_text ltx_font_typewriter">edited image with backward unroll </span>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">34</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 

</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">35</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="{\mathbf{x}}^{t}_{\theta}\leftarrow(1-t){\mathbf{x}}_{\theta}^{0}+t\epsilon\quad\epsilon\sim\mathcal{N}(\bm{0},\bm{I})\;t\in(0,1)" class="ltx_Math" display="inline" id="alg1.m30" intent=":literal"><semantics><mrow><mrow><msubsup><mi>𝐱</mi><mi>θ</mi><mi>t</mi></msubsup><mo stretchy="false">←</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>t</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi>𝐱</mi><mi>θ</mi><mn>0</mn></msubsup></mrow><mo>+</mo><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>ϵ</mi></mrow></mrow></mrow><mspace style="width:1em;" width="1em"></mspace><mrow><mi>ϵ</mi><mo>∼</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>𝟎</mn><mo>,</mo><mi>𝑰</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.280em" rspace="0em">​</mo><mi>t</mi></mrow><mo>∈</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">{\mathbf{x}}^{t}_{\theta}\leftarrow(1-t){\mathbf{x}}_{\theta}^{0}+t\epsilon\quad\epsilon\sim\mathcal{N}(\bm{0},\bm{I})\;t\in(0,1)</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">36</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="{\mathbf{v}}_{\phi}\leftarrow A_{\phi}({\mathbf{x}}^{t}_{\theta},{\mathbf{c}}^{\mathbf{x}})" class="ltx_Math" display="inline" id="alg1.m31" intent=":literal"><semantics><mrow><msub><mi>𝐯</mi><mi>ϕ</mi></msub><mo stretchy="false">←</mo><mrow><msub><mi>A</mi><mi>ϕ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>𝐱</mi><mi>θ</mi><mi>t</mi></msubsup><mo>,</mo><msup><mi>𝐜</mi><mi>𝐱</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">{\mathbf{v}}_{\phi}\leftarrow A_{\phi}({\mathbf{x}}^{t}_{\theta},{\mathbf{c}}^{\mathbf{x}})</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">37</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> <math alttext="\phi_{A}\leftarrow\phi_{A}-\eta_{A}\nabla_{\theta}||{\mathbf{v}}-{\mathbf{v}}_{\phi}||\;\text{where}\;{\mathbf{v}}=\epsilon-{\mathbf{x}}_{\theta}^{0}" class="ltx_Math" display="inline" id="alg1.m32" intent=":literal"><semantics><mrow><msub><mi>ϕ</mi><mi>A</mi></msub><mo stretchy="false">←</mo><mrow><msub><mi>ϕ</mi><mi>A</mi></msub><mo>−</mo><mrow><msub><mi>η</mi><mi>A</mi></msub><mo lspace="0.167em" rspace="0em">​</mo><msub><mo>∇</mo><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">‖</mo><mrow><mi>𝐯</mi><mo>−</mo><msub><mi>𝐯</mi><mi>ϕ</mi></msub></mrow><mo stretchy="false">‖</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mtext>where</mtext><mo lspace="0.280em" rspace="0em">​</mo><mi>𝐯</mi></mrow></mrow><mo>=</mo><mrow><mi>ϵ</mi><mo>−</mo><msubsup><mi>𝐱</mi><mi>θ</mi><mn>0</mn></msubsup></mrow></mrow><annotation encoding="application/x-tex">\phi_{A}\leftarrow\phi_{A}-\eta_{A}\nabla_{\theta}||{\mathbf{v}}-{\mathbf{v}}_{\phi}||\;\text{where}\;{\mathbf{v}}=\epsilon-{\mathbf{x}}_{\theta}^{0}</annotation></semantics></math>
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">38</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span>  end for
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">39</span> <span class="ltx_rule" style="width:1px;height:100%;--ltx-bg-color:black;display:inline-block;"> </span> 
</div>
<div class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">40</span> end while
</div>
<div class="ltx_listingline">
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 1</span> </span> NP-Edit: our training method</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1220" id="S4.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold">Qualitative comparison on GEdit-Bench</span> under the few-step sampling setting. For an upper-bound comparison, in the <math alttext="1^{\text{st}}" class="ltx_Math" display="inline" id="S4.F2.m2" intent=":literal"><semantics><msup><mn>1</mn><mtext>st</mtext></msup><annotation encoding="application/x-tex">1^{\text{st}}</annotation></semantics></math> column we show results of the best multi-step sampling method (as measured by the quantitative metrics in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.T1" title="Table 1 ‣ 5.1 Local image-editing ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">1</span></a>). Our method performs on par or better than baseline methods across different edit types in the few-step setting. We show more samples in the Appendix Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A7.F10" title="Figure 10 ‣ Appendix G Societal Impact ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">10</span></a>
</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p">In this section, we show the results of our method on local image editing as well as more free-form image editing tasks like customization, and compare them with the state-of-the-art baseline methods.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Local image-editing</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Benchmark.</span>   For evaluation, following prior works, we use the English subset of GEdit-Benchmark <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib46" title="">2025b</a>)</cite>, which captures real-world user interactions across different edit types. We also show results on the ImgEdit <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib85" title="">2025</a>)</cite> benchmark in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A1" title="Appendix A Additional Comparison with Baseline Methods ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Evaluation metric.</span>   For quantitative evaluation, we follow prior works and use GPT4o-based VIEScore <cite class="ltx_cite ltx_citemacro_citep">(Ku et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib37" title="">2024</a>)</cite> metric. It scores each edit on: (1) Semantic Consistency (SC) score, evaluating whether the edit instruction was followed, and (2) Perceptual Quality (PQ) score, assessing realism and absence of artifacts. Following VIEScore, for the <em class="ltx_emph ltx_font_italic">Overall score</em>, we take the geometric mean between SC and PQ for each image, and average across images in the evaluation benchmark.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Baselines.</span>   We compare our method with leading baselines, including FLUX.1-Kontext <cite class="ltx_cite ltx_citemacro_citep">(Labs et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib40" title="">2025</a>)</cite>, Step1X-Edit <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib46" title="">2025b</a>)</cite>, BAGEL <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib13" title="">2025</a>)</cite>, OmniGen <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib78" title="">2025</a>)</cite>, and Qwen-Image-Edit <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib77" title="">2025</a>)</cite>. Since no prior work explicitly targets few-step editing, we simply evaluate the above baselines with few-step sampling as well as their original multi-step setting for an upper-bound comparison. We also include Turbo-Edit <cite class="ltx_cite ltx_citemacro_citep">(Deutch et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib14" title="">2024</a>)</cite>, a state-of-the-art zero-shot few-step method that requires no paired supervision (as zero-shot) and is thus closest to our setup. We use the open-source implementation of all baselines, with further details in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A6" title="Appendix F Other Baseline Details ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">F</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span> <span class="ltx_text ltx_font_bold">Quantitative evaluation on GEdit-Bench</span>. Our method performs on par or better than baselines under the few-step setting. For multi-step sampling, it still outperforms OmiGen and remains competitive with many of the larger-scale models like BAGEL and FLUX.1 Kontext. All numbers reported in <math alttext="\times 10" class="ltx_Math" display="inline" id="S5.T1.m2" intent=":literal"><semantics><mrow><mi></mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\times 10</annotation></semantics></math></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:81.9pt;vertical-align:-39.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-123.2pt,25.9pt) scale(0.612963325434994,0.612963325434994) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\#" class="ltx_Math" display="inline" id="S5.T1.m3" intent=":literal"><semantics><mi mathvariant="normal">#</mi><annotation encoding="application/x-tex">\#</annotation></semantics></math><span class="ltx_text ltx_font_bold">Param</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\#" class="ltx_Math" display="inline" id="S5.T1.m4" intent=":literal"><semantics><mi mathvariant="normal">#</mi><annotation encoding="application/x-tex">\#</annotation></semantics></math><span class="ltx_text ltx_font_bold">Step</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">SC Score<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.m5" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">PQ Score</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.m6" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Overall</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.m7" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Omni-Gen <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib78" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">50</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">5.52</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">6.14</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4.97</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">BAGEL <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib13" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.02</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.26</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.14</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">FLUX.1-Kontext <cite class="ltx_cite ltx_citemacro_citep">(Labs et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib40" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.29</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.65</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.65</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Step1X-Edit <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib46" title="">2025b</a>)</cite> v1.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.30</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.37</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.79</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Qwen-Image-Edit <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib77" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.94</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.50</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.36</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">FLUX.1-Kontext <cite class="ltx_cite ltx_citemacro_citep">(Labs et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib40" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">5.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">5.74</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">5.04</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Step1X-Edit <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib46" title="">2025b</a>)</cite> v1.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">6.61</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.43</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">6.01</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Qwen-Image-Edit <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib77" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.82</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.21</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.06</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Turbo-Edit <cite class="ltx_cite ltx_citemacro_citep">(Deutch et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib14" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.84</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">6.67</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.84</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">NP-Edit (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">2B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">6.16</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.69</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.10</span></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span> <span class="ltx_text ltx_font_bold">Free-form editing task, <em class="ltx_emph ltx_font_italic">Customization</em>, evaluation on Dreambooth</span>. We perform better than OminiControl, DSD, and SynCD, which are trained for this task on synthetic datasets. When compared to FLUX.1-Kontext and Qwen-Image-Edit, we still perform comparably in the few-step setting. All numbers are reported in <math alttext="\times 10" class="ltx_Math" display="inline" id="S5.T2.m2" intent=":literal"><semantics><mrow><mi></mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\times 10</annotation></semantics></math></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:89.3pt;vertical-align:-43.1pt;"><span class="ltx_transformed_inner" style="transform:translate(-123.2pt,28.2pt) scale(0.612963325434994,0.612963325434994) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\#" class="ltx_Math" display="inline" id="S5.T2.m3" intent=":literal"><semantics><mi mathvariant="normal">#</mi><annotation encoding="application/x-tex">\#</annotation></semantics></math><span class="ltx_text ltx_font_bold">Param</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\#" class="ltx_Math" display="inline" id="S5.T2.m4" intent=":literal"><semantics><mi mathvariant="normal">#</mi><annotation encoding="application/x-tex">\#</annotation></semantics></math><span class="ltx_text ltx_font_bold">Step</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">SC Score<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.m5" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">PQ Score</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.m6" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Overall</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.m7" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">DSD <cite class="ltx_cite ltx_citemacro_citep">(Cai et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib6" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">28</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">6.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">7.41</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">6.78</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SynCD <cite class="ltx_cite ltx_citemacro_citep">(Kumari et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib39" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">30</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.66</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.83</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.54</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">FLUX.1-Kontext <cite class="ltx_cite ltx_citemacro_citep">(Labs et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib40" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.19</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.45</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.61</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Qwen-Image-Edit <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib77" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">8.53</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.79</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">8.02</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">OminiControl <cite class="ltx_cite ltx_citemacro_citep">(Tan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib73" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">6.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.82</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">6.22</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">DSD <cite class="ltx_cite ltx_citemacro_citep">(Cai et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib6" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.37</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.78</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.29</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SynCD <cite class="ltx_cite ltx_citemacro_citep">(Kumari et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib39" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.71</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.84</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.07</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">FLUX.1-Kontext <cite class="ltx_cite ltx_citemacro_citep">(Labs et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib40" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">7.99</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.18</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">7.39</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Qwen-Image-Edit <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib77" title="">2025</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">8.08</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.44</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.62</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">NP-Edit (Ours)</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.68</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">7.56</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.33</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">NP-Edit (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">2B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">7.60</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">7.28</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">7.10</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Results</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.T1" title="Table 1 ‣ 5.1 Local image-editing ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">1</span></a> shows the quantitative result. In the few-step setting, our method achieves the best Overall and Perceptual Quality (PQ) score compared to baseline methods. When compared to their original multi-step sampling, our few-step model still outperforms OmniGen and remains competitive with BAGEL, FLUX.1-Kontext, despite being <math alttext="\times 6" class="ltx_Math" display="inline" id="S5.SS1.p4.m1" intent=":literal"><semantics><mrow><mi></mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">\times 6</annotation></semantics></math> smaller parameter-wise. While Step1X-Edit and Qwen-Image-Edit perform better, they are substantially larger models. Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S4.F2" title="Figure 2 ‣ 4.3 Training details ‣ 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">2</span></a> provides a qualitative comparison. As we can see, our method can successfully follow different editing instructions while being consistent with the input reference image. For instance, in the <math alttext="6^{\text{th}}" class="ltx_Math" display="inline" id="S5.SS1.p4.m2" intent=":literal"><semantics><msup><mn>6</mn><mtext>th</mtext></msup><annotation encoding="application/x-tex">6^{\text{th}}</annotation></semantics></math> row (sheep color change), our approach produces a more natural edit compared to baselines. It also performs comparably to the multi-step variant for edits like lighting the candle in <math alttext="4^{\text{rth}}" class="ltx_Math" display="inline" id="S5.SS1.p4.m3" intent=":literal"><semantics><msup><mn>4</mn><mtext>rth</mtext></msup><annotation encoding="application/x-tex">4^{\text{rth}}</annotation></semantics></math> row or making the person wave in <math alttext="7^{\text{th}}" class="ltx_Math" display="inline" id="S5.SS1.p4.m4" intent=":literal"><semantics><msup><mn>7</mn><mtext>th</mtext></msup><annotation encoding="application/x-tex">7^{\text{th}}</annotation></semantics></math> row.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Free-form Editing: Customization</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Benchmark.</span>   We use the widely adopted DreamBooth <cite class="ltx_cite ltx_citemacro_citep">(Ruiz et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib60" title="">2023</a>)</cite> dataset for evaluation. It consists of <math alttext="30" class="ltx_Math" display="inline" id="S5.SS2.p1.m1" intent=":literal"><semantics><mn>30</mn><annotation encoding="application/x-tex">30</annotation></semantics></math> objects and <math alttext="25" class="ltx_Math" display="inline" id="S5.SS2.p1.m2" intent=":literal"><semantics><mn>25</mn><annotation encoding="application/x-tex">25</annotation></semantics></math> prompts per object category. The goal is to generate the same object as shown in the reference image, but in a different context, as mentioned in the text prompt.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Baselines.</span>   We compare against state-of-the-art unified image-editing baselines such as FLUX.1-Kontext <cite class="ltx_cite ltx_citemacro_citep">(Labs et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib40" title="">2025</a>)</cite> and Qwen-Image-Edit <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib77" title="">2025</a>)</cite> as well as OminiControl <cite class="ltx_cite ltx_citemacro_citep">(Tan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib73" title="">2025</a>)</cite>, DSD <cite class="ltx_cite ltx_citemacro_citep">(Cai et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib6" title="">2025</a>)</cite>, and SynCD <cite class="ltx_cite ltx_citemacro_citep">(Kumari et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib39" title="">2025</a>)</cite>, which are feed-forward models trained specifically for this task on synthetic datasets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Evaluation metric.</span>   Here as well, we use the VIEScore evaluation with a similar Semantic Consistency (SC) score to evaluate identity and text alignment and the Perceptual Quality (PQ) score to measure realism, and the geometric mean of the two for the Overall score.
We also report CLIPScore <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib58" title="">2021</a>)</cite> and DINO <cite class="ltx_cite ltx_citemacro_citep">(Oquab et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib53" title="">2023</a>)</cite> similarity-based metrics in the Appendix.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Results.</span>   As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.T2" title="Table 2 ‣ 5.1 Local image-editing ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">2</span></a>, our method performs comparably to state-of-the-art methods. In the few-shot sampling setting, all the baseline methods fail to generate realistic samples at <math alttext="4" class="ltx_Math" display="inline" id="S5.SS2.p4.m1" intent=":literal"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math> steps; therefore, we compare with them at <math alttext="8" class="ltx_Math" display="inline" id="S5.SS2.p4.m2" intent=":literal"><semantics><mn>8</mn><annotation encoding="application/x-tex">8</annotation></semantics></math> sampling steps. Our method still results in higher fidelity samples as  Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.F3" title="Figure 3 ‣ 5.2 Free-form Editing: Customization ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">3</span></a> shows, while maintaining object identity with the reference image. Note that our method performs better than OminiControl, which is also a few-step (8) model for this task.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="525" id="S5.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold">Qualitative comparison on Customization task.</span> Our method can generate the object in new contexts while having better fidelity under few-step sampling. We show more samples in the Appendix Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A7.F12" title="Figure 12 ‣ Appendix G Societal Impact ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">12</span></a>.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p">In this section, we perform several ablations to analyze the role of different components of our method, dataset scale, and stronger VLMs. All ablations are done on the local image-editing task.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Training objective.</span>   We ablate our training objective across four settings: (1) using only distribution matching loss, (2) using only the VLM-based editing loss, (3) removing the identity-preservation loss <math alttext="\mathcal{D}_{QA}" class="ltx_Math" display="inline" id="S5.SS3.p2.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mi>A</mi></mrow></msub><annotation encoding="application/x-tex">\mathcal{D}_{QA}</annotation></semantics></math>, and (4) replacing the binary-cross entropy loss (Eqn. <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S4.E4" title="In 4.2 Training Objective ‣ 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">4</span></a>) with standard cross-entropy over the full vocabulary. Results are shown below in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.T3" title="Table 3 ‣ 5.3 Ablation ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">3</span></a>. Training without the VLM-based loss and relying solely on distribution matching significantly degrades the model’s capabilities at following editing instructions. We observe that VLM-based loss is essential for maintaining consistency between input and edited images and for certain editing tasks like <em class="ltx_emph ltx_font_italic">Removal</em> (Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.F4" title="Figure 4 ‣ 5.3 Ablation ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">4</span></a> and Appendix Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A2.F5" title="Figure 5 ‣ Appendix B Ablation Study ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">5</span></a>). However, only training with VLM-based loss leads to unrealistic outputs (Appendix Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A2.F6" title="Figure 6 ‣ Appendix B Ablation Study ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">6</span></a>), and the training eventually diverges, as evidenced by the low overall score in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.T3" title="Table 3 ‣ 5.3 Ablation ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">3</span></a>, underscoring the need for DMD loss. In addition, using binary cross-entropy loss and having a question to check consistency between input and edited images improves the overall performance.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span> <span class="ltx_text ltx_font_bold">Training objective ablation</span>. We compare on the GEdit-Bench using the VIEScore metric. Ablating different components of our method leads to a drop in performance, indicating its importance.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:238.5pt;height:61.6pt;vertical-align:-28.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.5pt,5.8pt) scale(0.84149150076373,0.84149150076373) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">SC Score<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">PQ Score</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Overall</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.m3" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.69</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.10</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">w/ only DMD</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.93</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.51</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.93</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">w/ only VLM</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.03</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.48</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.93</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">w/o VLM identity</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.70</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.67</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.76</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">w/ standard CE loss</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">5.95</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">7.64</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">5.89</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS3.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Dataset and VLM scale.</span>   To study the role of dataset scale, we vary the number of unique reference images in training. Our final dataset represents the maximum scale feasible under our computational resources. Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.SS3" title="5.3 Ablation ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">5.3</span></a> shows the performance across different dataset sizes and VLM backbones. We observe consistent gains with larger datasets, suggesting that further scaling of data could yield additional improvements. Similarly, a larger parameter VLM-backbone leads to better performance, across different VLMs such as InternVL <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib11" title="">2024</a>)</cite> and LLava-OneVision <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib41" title="">2024</a>)</cite>, underscoring the promise that our method can improve as more powerful VLMs are developed.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Our method vs. Reinforcement Learning (RL).</span>   RL is a common post-training strategy for improving pre-trained models without paired supervision and can also leverage VLMs as the reward model, a similar setup to ours. Thus, we benchmark our method against Flow-GRPO <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib45" title="">2025a</a>)</cite>, a widely used RL method for text-to-image diffusion.
However, since RL relies on a reasonable initialization, we need to first train an image-editing model via Supervised Fine-Tuning (SFT) on a paired dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib42" title="">2025</a>)</cite>.
We then fine-tune it with Flow-GRPO using the same Llava-OneVision reward model as in our approach. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.SS3" title="5.3 Ablation ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">5.3</span></a>, SFT alone performs poorly, likely due to the limited quality of paired data. Our method surpasses both SFT and SFT+RL, despite requiring no paired supervision.
Fine-tuning the model with some paired data before applying our approach can slightly improve the pixel-level consistency between the input reference and output edited image (as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.F4" title="Figure 4 ‣ 5.3 Ablation ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">4</span></a>), although the quantitative numbers are similar.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p5">
<p class="ltx_p">The Appendix provides additional results and more detailed discussion of the method’s limitations.</p>
</div>
<figure class="ltx_figure" id="S5.SS3.fig3">
<div class="ltx_block">
<figure class="ltx_figure ltx_minipage ltx_align_center ltx_align_middle" id="S5.T4" style="width:195.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 4: </span> <span class="ltx_text ltx_font_bold">Dataset and VLM scale and comparison with Reinforcement Learning</span> on the GEdit-Bench. Increasing dataset scale and using stronger VLMs leads to increased performance. Our method also performs better than post-training an SFT model with RL <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib45" title="">2025a</a>)</cite>.</figcaption>
<br class="ltx_break ltx_break"/>
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:226.2pt;vertical-align:-108.9pt;"><span class="ltx_transformed_inner" style="transform:translate(88.4pt,-46.1pt) scale(1.68797043997135,1.68797043997135) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">SC Score<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.SS3.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">PQ Score</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.SS3.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Overall</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.SS3.m3" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">1 <math alttext="\%" class="ltx_Math" display="inline" id="S5.SS3.m4" intent=":literal"><semantics><mo>%</mo><annotation encoding="application/x-tex">\%</annotation></semantics></math> Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">7.10</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4.66</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">50 <math alttext="\%" class="ltx_Math" display="inline" id="S5.SS3.m5" intent=":literal"><semantics><mo>%</mo><annotation encoding="application/x-tex">\%</annotation></semantics></math> Dataset</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.41</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.73</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.52</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">100 <math alttext="\%" class="ltx_Math" display="inline" id="S5.SS3.m6" intent=":literal"><semantics><mo>%</mo><annotation encoding="application/x-tex">\%</annotation></semantics></math> Dataset</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.16</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.69</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.10</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">InternVL-2B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">5.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">7.67</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">5.45</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">InternVL-14B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.88</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.74</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.89</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">LLava-0.5B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.57</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.50</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.59</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">LLava-7B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.16</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.69</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.10</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">SFT</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">3.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">5.70</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">3.64</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SFT + RL</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.55</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.47</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.19</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">SFT + Ours</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.83</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.06</span></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="S5.F4" style="width:229.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="470" id="S5.SS3.g1" src="x4.png" width="830"/>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold">Qualitative analysis of ablation experiments.</span> Our method maintains better input and edited image alignment compared to only training with DMD loss, which also fails on tasks like removal. Compared to fine-tuning an SFT model with RL, our method results in better fidelity while following the edit instruction. Please zoom in for details.</figcaption>
</figure>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion and Limitations</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p">This paper introduces a new paradigm for enabling image editing capabilities given a pre-trained text-to-image diffusion model, without paired before-and-after edit supervision. Our approach combines differentiable feedback from VLMs to ensure editing success with a distribution matching objective to maintain visual realism. This method achieves competitive performance with recent state-of-the-art baselines trained on paired data while enabling efficient few-step generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p">Despite these promising results, our method has limitations. Without pixel-level supervision, edits may deviate from the input image in fine-grained details or fail to fully preserve subject identity. We show in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A3" title="Appendix C Limitation ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">C</span></a> that adding a perceptual similarity loss (e.g., LPIPS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib92" title="">2018</a>)</cite>) between input and edited images alleviates this to some extent, though often at the cost of editing quality. Another constraint for our method is the need to keep the VLM in GPU memory, introducing VRAM overhead. We expect ongoing advances in stronger and more efficient VLMs can help address this issue. Overall, our framework scales effectively with large unpaired datasets and highlights a path toward more flexible post-training of generative models for diverse downstream tasks.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Acknowledgment.</span>
We thank Gaurav Parmar, Maxwell Jones, and Ruihan Gao for their feedback and helpful discussions. This work was partly done while Nupur Kumari was interning at Adobe Research. The project was partly supported by Adobe Inc., the Packard
Fellowship, the IITP grant funded by the Korean Government (MSIT) (No. RS-2024-00457882, National AI Research Lab Project), NSF IIS-2239076, and NSF ISS-2403303.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adam et al. (2014)</span>
<span class="ltx_bibblock">
Kingma DP Ba J Adam et al.

</span>
<span class="ltx_bibblock">A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 1412(6), 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmadi et al. (2025)</span>
<span class="ltx_bibblock">
Saba Ahmadi, Rabiul Awal, Ankur Sikarwar, Amirhossein Kazemnejad, Ge Ya Luo, Juan A Rodriguez, Sai Rajeswar, Siva Reddy, Christopher Pal, Benno Krojer, et al.

</span>
<span class="ltx_bibblock">The promise of rl for autoregressive image editing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2508.01119</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Avrahami et al. (2023)</span>
<span class="ltx_bibblock">
Omri Avrahami, Ohad Fried, and Dani Lischinski.

</span>
<span class="ltx_bibblock">Blended latent diffusion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ACM transactions on graphics (TOG)</em>, 42(4):1–11, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et al. (2024)</span>
<span class="ltx_bibblock">
Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine.

</span>
<span class="ltx_bibblock">Training diffusion models with reinforcement learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brooks et al. (2023)</span>
<span class="ltx_bibblock">
Tim Brooks, Aleksander Holynski, and Alexei A Efros.

</span>
<span class="ltx_bibblock">Instructpix2pix: Learning to follow image editing instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2025)</span>
<span class="ltx_bibblock">
Shengqu Cai, Eric Ryan Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, and Gordon. Wetzstein.

</span>
<span class="ltx_bibblock">Diffusion self-distillation for zero-shot customized image generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2023)</span>
<span class="ltx_bibblock">
Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng.

</span>
<span class="ltx_bibblock">Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision (ICCV)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cazenavette et al. (2024)</span>
<span class="ltx_bibblock">
George Cazenavette, Avneesh Sud, Thomas Leung, and Ben Usman.

</span>
<span class="ltx_bibblock">Fakeinversion: Learning to detect images from unseen text-to-image models by inverting stable diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William W Cohen.

</span>
<span class="ltx_bibblock">Subject-driven text-to-image generation via apprenticeship learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Neural Information Processing Systems (NeurIPS)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2025)</span>
<span class="ltx_bibblock">
Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al.

</span>
<span class="ltx_bibblock">Unireal: Universal image generation and editing via learning real-world dynamics.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al.

</span>
<span class="ltx_bibblock">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Corvi et al. (2023)</span>
<span class="ltx_bibblock">
Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva.

</span>
<span class="ltx_bibblock">On the detection of synthetic images generated by diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2025)</span>
<span class="ltx_bibblock">
Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al.

</span>
<span class="ltx_bibblock">Emerging properties in unified multimodal pretraining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.14683</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deutch et al. (2024)</span>
<span class="ltx_bibblock">
Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">Turboedit: Text-based image editing using few-step diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">SIGGRAPH Asia Conference Proceedings</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandez et al. (2023)</span>
<span class="ltx_bibblock">
Pierre Fernandez, Guillaume Couairon, Hervé Jégou, Matthijs Douze, and Teddy Furon.

</span>
<span class="ltx_bibblock">The stable signature: Rooting watermarks in latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision (ICCV)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frans et al. (2024)</span>
<span class="ltx_bibblock">
Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">One step diffusion via shortcut models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2410.12557</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2024)</span>
<span class="ltx_bibblock">
Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan.

</span>
<span class="ltx_bibblock">Guiding instruction-based image editing via multimodal large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gal et al. (2023)</span>
<span class="ltx_bibblock">
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">An image is worth one word: Personalizing text-to-image generation using textual inversion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. (2024)</span>
<span class="ltx_bibblock">
Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan.

</span>
<span class="ltx_bibblock">Seed-data-edit technical report: A hybrid dataset for instructional image editing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.04007</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al. (2023)</span>
<span class="ltx_bibblock">
Zhengyang Geng, Ashwini Pokle, and J Zico Kolter.

</span>
<span class="ltx_bibblock">One-step diffusion distillation via deep equilibrium models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Neural Information Processing Systems (NeurIPS)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al. (2024)</span>
<span class="ltx_bibblock">
Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and J Zico Kolter.

</span>
<span class="ltx_bibblock">Consistency models made easy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2406.14548</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al. (2025)</span>
<span class="ltx_bibblock">
Zhengyang Geng, Mingyang Deng, Xingjian Bai, J Zico Kolter, and Kaiming He.

</span>
<span class="ltx_bibblock">Mean flows for one-step generative modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.13447</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heek et al. (2024)</span>
<span class="ltx_bibblock">
Jonathan Heek, Emiel Hoogeboom, and Tim Salimans.

</span>
<span class="ltx_bibblock">Multistep consistency models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.06807</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hertz et al. (2023)</span>
<span class="ltx_bibblock">
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">Prompt-to-prompt image editing with cross attention control.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hertz et al. (2024)</span>
<span class="ltx_bibblock">
Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">Style aligned image generation via shared attention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al. (2020)</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Neural Information Processing Systems (NeurIPS)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2023)</span>
<span class="ltx_bibblock">
Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith.

</span>
<span class="ltx_bibblock">Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hui et al. (2024)</span>
<span class="ltx_bibblock">
Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie.

</span>
<span class="ltx_bibblock">Hq-edit: A high-quality dataset for instruction-based image editing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.09990</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones et al. (2024)</span>
<span class="ltx_bibblock">
Maxwell Jones, Sheng-Yu Wang, Nupur Kumari, David Bau, and Jun-Yan Zhu.

</span>
<span class="ltx_bibblock">Customizing text-to-image models with a single image pair.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">SIGGRAPH Asia Conference Proceedings</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. (2023)</span>
<span class="ltx_bibblock">
Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.

</span>
<span class="ltx_bibblock">Scaling up gans for text-to-image synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. (2024)</span>
<span class="ltx_bibblock">
Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park.

</span>
<span class="ltx_bibblock">Distilling diffusion models into conditional gans.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2024)</span>
<span class="ltx_bibblock">
Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon.

</span>
<span class="ltx_bibblock">Consistency trajectory models: Learning probability flow ode trajectory of diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2023)</span>
<span class="ltx_bibblock">
Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu.

</span>
<span class="ltx_bibblock">Dense text-to-image generation with attention modulation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision (ICCV)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirstain et al. (2023)</span>
<span class="ltx_bibblock">
Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy.

</span>
<span class="ltx_bibblock">Pick-a-pic: An open dataset of user preferences for text-to-image generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 36:36652–36663, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krojer et al. (2024)</span>
<span class="ltx_bibblock">
Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Chris Pal, and Siva Reddy.

</span>
<span class="ltx_bibblock">Learning action and reasoning-centric image editing from videos and simulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 37:38035–38078, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ku et al. (2024)</span>
<span class="ltx_bibblock">
Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen.

</span>
<span class="ltx_bibblock">Viescore: Towards explainable metrics for conditional image synthesis evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Association for Computational Linguistics (ACL)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumari et al. (2023)</span>
<span class="ltx_bibblock">
Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.

</span>
<span class="ltx_bibblock">Multi-concept customization of text-to-image diffusion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumari et al. (2025)</span>
<span class="ltx_bibblock">
Nupur Kumari, Xi Yin, Jun-Yan Zhu, Ishan Misra, and Samaneh Azadi.

</span>
<span class="ltx_bibblock">Generating multi-image synthetic data for text-to-image customization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision (ICCV)</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Labs et al. (2025)</span>
<span class="ltx_bibblock">
Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al.

</span>
<span class="ltx_bibblock">Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2506.15742</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al.

</span>
<span class="ltx_bibblock">Llava-onevision: Easy visual task transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2408.03326</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2025)</span>
<span class="ltx_bibblock">
Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al.

</span>
<span class="ltx_bibblock">Uniworld: High-resolution semantic encoders for unified visual understanding and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2506.03147</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lipman et al. (2023)</span>
<span class="ltx_bibblock">
Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.

</span>
<span class="ltx_bibblock">Flow matching for generative modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Neural Information Processing Systems (NeurIPS)</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2025a)</span>
<span class="ltx_bibblock">
Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang.

</span>
<span class="ltx_bibblock">Flow-grpo: Training flow matching models via online rl.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Neural Information Processing Systems (NeurIPS)</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2025b)</span>
<span class="ltx_bibblock">
Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al.

</span>
<span class="ltx_bibblock">Step1x-edit: A practical framework for general image editing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2504.17761</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Xingchao Liu, Chengyue Gong, and Qiang Liu.

</span>
<span class="ltx_bibblock">Flow straight and fast: Learning to generate and transfer data with rectified flow.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu &amp; Song (2025)</span>
<span class="ltx_bibblock">
Cheng Lu and Yang Song.

</span>
<span class="ltx_bibblock">Simplifying, stabilizing and scaling continuous-time consistency models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2025)</span>
<span class="ltx_bibblock">
Grace Luo, Jonathan Granskog, Aleksander Holynski, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Dual-process image generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision (ICCV)</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2023)</span>
<span class="ltx_bibblock">
Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang.

</span>
<span class="ltx_bibblock">Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36:76525–76546, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Magar et al. (2025)</span>
<span class="ltx_bibblock">
Nadav Magar, Amir Hertz, Eric Tabellion, Yael Pritch, Alex Rav-Acha, Ariel Shamir, and Yedid Hoshen.

</span>
<span class="ltx_bibblock">Lightlab: Controlling light sources in images with diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers</em>, pp.  1–11, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. (2022)</span>
<span class="ltx_bibblock">
Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.

</span>
<span class="ltx_bibblock">Sdedit: Guided image synthesis and editing with stochastic differential equations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oquab et al. (2023)</span>
<span class="ltx_bibblock">
Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.

</span>
<span class="ltx_bibblock">Dinov2: Learning robust visual features without supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research (TMLR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parmar et al. (2023)</span>
<span class="ltx_bibblock">
Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu.

</span>
<span class="ltx_bibblock">Zero-shot image-to-image translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">ACM SIGGRAPH 2023 Conference Proceedings</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peebles &amp; Xie (2023)</span>
<span class="ltx_bibblock">
William Peebles and Saining Xie.

</span>
<span class="ltx_bibblock">Scalable diffusion models with transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision (ICCV)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qwen-Team (2024)</span>
<span class="ltx_bibblock">
Qwen-Team.

</span>
<span class="ltx_bibblock">Qwen2 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.10671</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qwen-Team (2025)</span>
<span class="ltx_bibblock">
Qwen-Team.

</span>
<span class="ltx_bibblock">Qwen2.5-vl, January 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://qwenlm.github.io/blog/qwen2.5-vl/" title="">https://qwenlm.github.io/blog/qwen2.5-vl/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Machine Learning (ICML)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et al. (2022)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.06125</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruiz et al. (2023)</span>
<span class="ltx_bibblock">
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.

</span>
<span class="ltx_bibblock">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salimans &amp; Ho (2022)</span>
<span class="ltx_bibblock">
Tim Salimans and Jonathan Ho.

</span>
<span class="ltx_bibblock">Progressive distillation for fast sampling of diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sauer et al. (2024a)</span>
<span class="ltx_bibblock">
Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach.

</span>
<span class="ltx_bibblock">Fast high-resolution image synthesis with latent adversarial diffusion distillation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">SIGGRAPH Asia Conference Proceedings</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sauer et al. (2024b)</span>
<span class="ltx_bibblock">
Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach.

</span>
<span class="ltx_bibblock">Adversarial diffusion distillation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et al. (2021)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.

</span>
<span class="ltx_bibblock">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.02114</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2024)</span>
<span class="ltx_bibblock">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.03300</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sohn et al. (2023)</span>
<span class="ltx_bibblock">
Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al.

</span>
<span class="ltx_bibblock">Styledrop: Text-to-image generation in any style.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Neural Information Processing Systems (NeurIPS)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2021)</span>
<span class="ltx_bibblock">
Jiaming Song, Chenlin Meng, and Stefano Ermon.

</span>
<span class="ltx_bibblock">Denoising diffusion implicit models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song &amp; Dhariwal (2024)</span>
<span class="ltx_bibblock">
Yang Song and Prafulla Dhariwal.

</span>
<span class="ltx_bibblock">Improved techniques for training consistency models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2023a)</span>
<span class="ltx_bibblock">
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Consistency models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2023b)</span>
<span class="ltx_bibblock">
Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga.

</span>
<span class="ltx_bibblock">Objectstitch: Object compositing with diffusion model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024)</span>
<span class="ltx_bibblock">
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.

</span>
<span class="ltx_bibblock">Generative multimodal models are in-context learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sushko et al. (2025)</span>
<span class="ltx_bibblock">
Peter Sushko, Ayana Bharadwaj, Zhi Yang Lim, Vasily Ilin, Ben Caffee, Dongping Chen, Mohammadreza Salehi, Cheng-Yu Hsieh, and Ranjay Krishna.

</span>
<span class="ltx_bibblock">Realedit: Reddit edits as a large-scale empirical dataset for image transformations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Computer Vision and Pattern Recognition Conference</em>, pp.  13403–13413, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. (2025)</span>
<span class="ltx_bibblock">
Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang.

</span>
<span class="ltx_bibblock">Ominicontrol: Minimal and universal control for diffusion transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision (ICCV)</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wallace et al. (2024)</span>
<span class="ltx_bibblock">
Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik.

</span>
<span class="ltx_bibblock">Diffusion model alignment using direct preference optimization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros.

</span>
<span class="ltx_bibblock">Cnn-generated images are surprisingly easy to spot… for now.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winter et al. (2024)</span>
<span class="ltx_bibblock">
Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen.

</span>
<span class="ltx_bibblock">Objectdrop: Bootstrapping counterfactuals for photorealistic object removal and insertion.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2025)</span>
<span class="ltx_bibblock">
Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu.

</span>
<span class="ltx_bibblock">Qwen-image technical report, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2508.02324" title="">https://arxiv.org/abs/2508.02324</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2025)</span>
<span class="ltx_bibblock">
Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu.

</span>
<span class="ltx_bibblock">Omnigen: Unified image generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the Computer Vision and Pattern Recognition Conference</em>, pp.  13294–13304, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.

</span>
<span class="ltx_bibblock">Imagereward: Learning and evaluating human preferences for text-to-image generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36:15903–15935, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou.

</span>
<span class="ltx_bibblock">Ufogen: You forward once large scale text-to-image generation via diffusion gans.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2023)</span>
<span class="ltx_bibblock">
Wilson Yan, Andrew Brown, Pieter Abbeel, Rohit Girdhar, and Samaneh Azadi.

</span>
<span class="ltx_bibblock">Motion-conditioned image animation for video editing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.18827</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024a)</span>
<span class="ltx_bibblock">
Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li.

</span>
<span class="ltx_bibblock">Using human feedback to fine-tune diffusion models without any reward model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  8941–8951, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024b)</span>
<span class="ltx_bibblock">
Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan.

</span>
<span class="ltx_bibblock">Editworld: Simulating world dynamics for instruction-following image editing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.14785</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024c)</span>
<span class="ltx_bibblock">
Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui.

</span>
<span class="ltx_bibblock">Consistency flow matching: Defining straight flows with velocity consistency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.02398</em>, 2024c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2025)</span>
<span class="ltx_bibblock">
Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan.

</span>
<span class="ltx_bibblock">Imgedit: A unified image editing dataset and benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.20275</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2024a)</span>
<span class="ltx_bibblock">
Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman.

</span>
<span class="ltx_bibblock">Improved distribution matching distillation for fast image synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2024b)</span>
<span class="ltx_bibblock">
Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, and Taesung Park.

</span>
<span class="ltx_bibblock">One-step diffusion with distribution matching distillation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2025)</span>
<span class="ltx_bibblock">
Xin Yu, Tianyu Wang, Soo Ye Kim, Paul Guerrero, Xi Chen, Qing Liu, Zhe Lin, and Xiaojuan Qi.

</span>
<span class="ltx_bibblock">Objectmover: Generative object movement with video prior.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al. (2023)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.

</span>
<span class="ltx_bibblock">Sigmoid loss for language image pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision (ICCV)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.

</span>
<span class="ltx_bibblock">Magicbrush: A manually annotated dataset for instruction-guided image editing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36:31428–31449, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal.

</span>
<span class="ltx_bibblock">Generative verifiers: Reward modeling as next-token prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2408.15240</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018)</span>
<span class="ltx_bibblock">
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of deep features as a perceptual metric.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2024)</span>
<span class="ltx_bibblock">
Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang.

</span>
<span class="ltx_bibblock">Ultraedit: Instruction-based fine-grained image editing at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 37:3058–3093, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2025)</span>
<span class="ltx_bibblock">
Linqi Zhou, Stefano Ermon, and Jiaming Song.

</span>
<span class="ltx_bibblock">Inductive moment matching.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2503.07565</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2024)</span>
<span class="ltx_bibblock">
Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang.

</span>
<span class="ltx_bibblock">Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Forty-first International Conference on Machine Learning</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2017)</span>
<span class="ltx_bibblock">
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.

</span>
<span class="ltx_bibblock">Unpaired image-to-image translation using cycle-consistent adversarial networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision (ICCV)</em>, 2017.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Appendix</span></p>
</div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Comparison with Baseline Methods</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Local image-editing.</span>   Here, we compare on another commonly adopted image-editing benchmark, ImgEdit <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib85" title="">2025</a>)</cite> (Basic), which covers nine local editing types across diverse semantic categories with a total of <math alttext="734" class="ltx_Math" display="inline" id="A1.p1.m1" intent=":literal"><semantics><mn>734</mn><annotation encoding="application/x-tex">734</annotation></semantics></math> samples. Quantitative results under their proposed GPT4o-based evaluation protocol are reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A1.T5" title="Table 5 ‣ Appendix A Additional Comparison with Baseline Methods ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">5</span></a>, with VIEScore <cite class="ltx_cite ltx_citemacro_citep">(Ku et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib37" title="">2024</a>)</cite> results in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A1.T6" title="Table 6 ‣ Appendix A Additional Comparison with Baseline Methods ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">6</span></a>. Consistent with the trend observed on GEdit-Bench, our method has better or on-par performance in the few-step setting and remains comparable to many of the multi-step baselines as well. Qualitative comparisons are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A7.F11" title="Figure 11 ‣ Appendix G Societal Impact ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">11</span></a>, with additional examples on GEdit-Bench in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A7.F10" title="Figure 10 ‣ Appendix G Societal Impact ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Customization or free-form editing.</span>   Here, we report additional metrics commonly used for evaluation. Specifically, CLIPScore <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib58" title="">2021</a>)</cite> and TIFA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib28" title="">2023</a>)</cite> to measure text alignment; and similarity in DINOv2 <cite class="ltx_cite ltx_citemacro_citep">(Oquab et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib53" title="">2023</a>)</cite> feature space after background masking to measure identity alignment, denoted as MDINOv2-I. We also report an overall Geometric score <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib81" title="">2023</a>)</cite> by taking the geometric mean of TIFA and MDINOv2-I. The results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A1.T7" title="Table 7 ‣ Appendix A Additional Comparison with Baseline Methods ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">7</span></a>. Consistent with the VIEScore evaluation reported in the main paper, our method performs comparably with other baselines in the few-step sampling regime. We show more sample comparisons in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A7.F12" title="Figure 12 ‣ Appendix G Societal Impact ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span> <span class="ltx_text ltx_font_bold">ImgEdit-Bench comparison</span> using their proposed GPT-4o based evaluation protocal and few-step sampling setting. Our method outperforms baseline methods on the <em class="ltx_emph ltx_font_italic">Avg</em> of all edit types.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:42pt;vertical-align:-19.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-99.1pt,9.6pt) scale(0.686392941053656,0.686392941053656) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\#" class="ltx_Math" display="inline" id="A1.T5.m1" intent=":literal"><semantics><mi mathvariant="normal">#</mi><annotation encoding="application/x-tex">\#</annotation></semantics></math><span class="ltx_text ltx_font_bold">Param</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Action<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Bg</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.m3" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Style</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.m4" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Adjust</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.m5" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Replace</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.m6" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Add</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.m7" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Extract</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.m8" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Remove</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.m9" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Compose</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.m10" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Avg</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T5.m11" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Qwen-Image-Edit</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">20B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">3.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">2.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">1.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">2.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.06</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.02</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Flux.1-Kontext</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.51</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">2.97</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.89</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.04</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.15</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.31</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">1.82</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">2.37</span></td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">2.46</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">2.95</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Step1X Edit</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.66</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.60</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.46</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.44</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.50</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.25</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.77</td>
<td class="ltx_td ltx_align_center" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">2.41</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.38</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.83</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">NP-Edit (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">2B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">4.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">4.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">4.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">4.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">2.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">2.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" style="--ltx-bg-color:#FFFFFF;">3.18</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="--ltx-bg-color:#FFFFFF;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#FFFFFF;">3.63</span></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="A1.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span> <span class="ltx_text ltx_font_bold">VIEScore evaluation on ImgEdit-Bench</span>. Our method performs on par or better than baselines under the few-step setting. For multi-step sampling, it still outperforms OmniGen and remains competitive with many of the larger-scale models like BAGEL and FLUX.1 Kontext. All numbers reported in <math alttext="\times 10" class="ltx_Math" display="inline" id="A1.T6.m2" intent=":literal"><semantics><mrow><mi></mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\times 10</annotation></semantics></math></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:104.4pt;vertical-align:-49.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.7pt,2.6pt) scale(0.952126401594615,0.952126401594615) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\#" class="ltx_Math" display="inline" id="A1.T6.m3" intent=":literal"><semantics><mi mathvariant="normal">#</mi><annotation encoding="application/x-tex">\#</annotation></semantics></math><span class="ltx_text ltx_font_bold">Param</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\#" class="ltx_Math" display="inline" id="A1.T6.m4" intent=":literal"><semantics><mi mathvariant="normal">#</mi><annotation encoding="application/x-tex">\#</annotation></semantics></math><span class="ltx_text ltx_font_bold">Step</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">SC Score<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T6.m5" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">PQ Score</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T6.m6" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Overall</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T6.m7" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">BAGEL</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">50</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">7.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">6.22</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">6.47</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">FLUX.1-Kontext</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.94</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.73</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.19</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Step-1X Edit v1.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.26</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.30</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.72</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">QwenImage Edit</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">8.30</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.77</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.85</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">QwenImage Edit</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">20B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">6.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">5.14</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">5.46</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">FLUX.1-Kontext</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.08</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.22</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.14</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Step-1X Edit v1.1</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.00</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">5.37</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.14</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">NP-Edit (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">2B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.78</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">6.62</span></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="A1.T7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span> <span class="ltx_text ltx_font_bold">Quantitative evaluation of free-form editing task, <em class="ltx_emph ltx_font_italic">Customization</em>, on DreamBooth dataset</span>. All numbers reported in <math alttext="\times 10" class="ltx_Math" display="inline" id="A1.T7.m2" intent=":literal"><semantics><mrow><mi></mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\times 10</annotation></semantics></math></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:403.3pt;height:119.2pt;vertical-align:-57.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-44.7pt,13.2pt) scale(0.818649548760771,0.818649548760771) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\#" class="ltx_Math" display="inline" id="A1.T7.m3" intent=":literal"><semantics><mi mathvariant="normal">#</mi><annotation encoding="application/x-tex">\#</annotation></semantics></math><span class="ltx_text ltx_font_bold">Param</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\#" class="ltx_Math" display="inline" id="A1.T7.m4" intent=":literal"><semantics><mi mathvariant="normal">#</mi><annotation encoding="application/x-tex">\#</annotation></semantics></math><span class="ltx_text ltx_font_bold">Step</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">MDINO Score<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T7.m5" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">CLIP Score</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T7.m6" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">TIFA</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T7.m7" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Geometric Score</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T7.m8" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">DSD</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">28</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">6.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">3.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">8.71</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">7.32</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SynCD</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">30</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.34</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.09</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.53</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.71</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">FLUX.1-Kontext</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.72</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.07</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.88</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.14</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Qwen-Image-Edit</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">50</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.47</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">3.14</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">9.37</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">8.22</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">OminiControl</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">6.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">3.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">8.12</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">6.64</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">DSD</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.88</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">3.15</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">8.93</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.99</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">SynCD</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.11</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">3.16</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">9.11</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.79</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">FLUX.1-Kontext</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.50</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.08</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.83</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">7.98</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">Qwen-Image-Edit</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">7.29</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.08</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">8.96</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline">7.91</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">NP-Edit (Ours)</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6.82</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2.97</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.73</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.54</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">NP-Edit (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">2B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">7.03</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">3.04</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">8.89</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">7.72</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Ablation Study</h2>
<figure class="ltx_figure" id="A2.fig3">
<div class="ltx_block">
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="A2.F5" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="901" id="A2.g1" src="x5.png" width="830"/>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold">Performance for each sub edit-type.</span> Training with only DMD loss fails to achieve certain tasks like removal and style changes. In addition, using binary cross-entropy loss and VLM identity-based questions helps improve the overall performance. </figcaption>
</figure>
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="A2.F6" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="876" id="A2.g2" src="x6.png" width="830"/>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold">Training with only VLM-editing loss</span> leads to lower fidelity samples with the model only maximizing the edit success probability. Current general-purpose VLMs are often not good at subjective tasks like evaluating image fidelity, highlighting the requirement of distribution matching loss in our framework.</figcaption>
</figure>
</div>
</figure>
<figure class="ltx_figure" id="A2.fig6">
<div class="ltx_block">
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="A2.F7" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="521" id="A2.g3" src="x7.png" width="830"/>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold">Unreliable VLM response on intermediate outputs of a multi-step diffusion model.</span> Here we show a 28-step diffusion process, denoising predictions from early steps (e.g., <math alttext="t=4" class="ltx_Math" display="inline" id="A2.F7.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">t=4</annotation></semantics></math>), which correspond to high noise levels, are blurry and semantically ambiguous. This can lead to unreliable responses from the VLM, as shown here. Therefore, we adopt a few-step diffusion model that always generates sharp images.</figcaption>
</figure>
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="A2.F8" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="660" id="A2.g4" src="x8.png" width="830"/>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span class="ltx_text ltx_font_bold">Our (4-step) vs single-step editing model.</span> We compare our final 4-step model with a single-step model, both trained via our approach. Editing an image in a single step is still challenging and leads to lower-fidelity outputs. </figcaption>
</figure>
</div>
</figure>
<figure class="ltx_figure" id="A2.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="290" id="A2.F9.g1" src="x9.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span class="ltx_text ltx_font_bold">Limitation.</span> Our method can struggle to maintain exact pixel consistency between the input and edited image. Having LPIPS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib92" title="">2018</a>)</cite> loss between the input and output edited image can resolve it to an extent (top row) but at the cost of reduced editing success (bottom row). </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Training objective.</span>   Here, we provide a more detailed analysis by examining performance across different editing sub-types. As a reminder, we ablated our training objective under four settings: (1) using only the distribution matching loss, (2) using only the VLM-based editing loss, (3) removing the identity-preservation question from <math alttext="\mathcal{D}_{QA}" class="ltx_Math" display="inline" id="A2.p1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><mi>A</mi></mrow></msub><annotation encoding="application/x-tex">\mathcal{D}_{QA}</annotation></semantics></math>, and (4) replacing the binary-cross entropy loss as explained in Eqn. <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S4.E4" title="In 4.2 Training Objective ‣ 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">4</span></a> with standard cross-entropy over full vocabulary length. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A2.F5" title="Figure 5 ‣ Appendix B Ablation Study ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">5</span></a>, training with only the DMD loss yields comparable performance on certain sub-edit types such as <em class="ltx_emph ltx_font_italic">Color change</em>, since DMD matches the text-conditioned score between the fine-tuned and pre-trained models, thus improving overall text alignment. However, it fails on tasks like <em class="ltx_emph ltx_font_italic">Removal</em>, underscoring the importance of VLM-based editing loss and its generalizability across diverse editing instructions. In addition, VLM-based loss also helps in maintaining better consistency between input and edited images (first row of Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.F4" title="Figure 4 ‣ 5.3 Ablation ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">4</span></a> in the main paper). However, when training with only the VLM-based editing loss, there’s a gradual degradation in image quality, as Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A2.F6" title="Figure 6 ‣ Appendix B Ablation Study ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">6</span></a> shows, highlighting the complementary role of distribution matching losses such as DMD.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Sampling steps.</span>   For our method, we chose to train a few-step image-editing model instead of a multi-step diffusion model, as multi-step diffusion has a noisy or blurry estimate of the final output in the early stages of diffusion. This can make it difficult to get a reliable response from the VLM, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A2.F7" title="Figure 7 ‣ Appendix B Ablation Study ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">7</span></a>. On the other hand, predicting an edited image in one step is still challenging, as mentioned in the main paper, and shown here in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A2.F8" title="Figure 8 ‣ Appendix B Ablation Study ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">8</span></a>. Thus few-step provides a nice balance between the two extremes of single and multi-step sampling for our purposes.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Limitation</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p">A limitation of our framework is the lack of pixel-wise supervision to preserve regions that should remain unchanged under a given edit instruction. Consequently, edited images can deviate from the input image in details or spatial alignment, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A2.F9" title="Figure 9 ‣ Appendix B Ablation Study ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">9</span></a>, <math alttext="1^{\text{st}}" class="ltx_Math" display="inline" id="A3.p1.m1" intent=":literal"><semantics><msup><mn>1</mn><mtext>st</mtext></msup><annotation encoding="application/x-tex">1^{\text{st}}</annotation></semantics></math> column. While our VLM-based editing loss includes a question to check consistency between the input and edited images, it does not enforce pixel-level alignment. Empirically, we find that current VLMs struggle to detect subtle changes. To mitigate this, we experiment with an additional LPIPS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib92" title="">2018</a>)</cite> loss between the input and output edited images. As shown in the last column of Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A2.F9" title="Figure 9 ‣ Appendix B Ablation Study ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">9</span></a>, this improves consistency but also negatively impacts editing quality, particularly for edit-types like <em class="ltx_emph ltx_font_italic">Removal</em>. Future work could explore specialized VLMs that are more sensitive to fine-grained, pixel-level differences.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Dataset Construction Details</h2>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p">Each tuple in our dataset <math alttext="\mathcal{X}=\{({\mathbf{y}}_{i},{\mathbf{c}}_{i},{\mathbf{c}}^{{\mathbf{y}}}_{i},{\mathbf{c}}^{{\mathbf{x}}}_{i})\}_{i=1}^{N}" class="ltx_Math" display="inline" id="A4.p1.m1" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐲</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝐜</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>𝐜</mi><mi>i</mi><mi>𝐲</mi></msubsup><mo>,</mo><msubsup><mi>𝐜</mi><mi>i</mi><mi>𝐱</mi></msubsup><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{X}=\{({\mathbf{y}}_{i},{\mathbf{c}}_{i},{\mathbf{c}}^{{\mathbf{y}}}_{i},{\mathbf{c}}^{{\mathbf{x}}}_{i})\}_{i=1}^{N}</annotation></semantics></math> consists of a real reference-image, <math alttext="{\mathbf{y}}" class="ltx_Math" display="inline" id="A4.p1.m2" intent=":literal"><semantics><mi>𝐲</mi><annotation encoding="application/x-tex">{\mathbf{y}}</annotation></semantics></math>, corresponding edit instruction, <math alttext="{\mathbf{c}}" class="ltx_Math" display="inline" id="A4.p1.m3" intent=":literal"><semantics><mi>𝐜</mi><annotation encoding="application/x-tex">{\mathbf{c}}</annotation></semantics></math>, and text prompt corresponding to the reference and edited image, <math alttext="{\mathbf{c}}^{{\mathbf{y}}}" class="ltx_Math" display="inline" id="A4.p1.m4" intent=":literal"><semantics><msup><mi>𝐜</mi><mi>𝐲</mi></msup><annotation encoding="application/x-tex">{\mathbf{c}}^{{\mathbf{y}}}</annotation></semantics></math> and <math alttext="{\mathbf{c}}^{{\mathbf{x}}}" class="ltx_Math" display="inline" id="A4.p1.m5" intent=":literal"><semantics><msup><mi>𝐜</mi><mi>𝐱</mi></msup><annotation encoding="application/x-tex">{\mathbf{c}}^{{\mathbf{x}}}</annotation></semantics></math>, respectively. We use a text-image dataset corpus to select reference images. Given a reference image, we prompt Qwen-2.5-32B VLM to suggest different possible editing instructions. The system and user prompt for it are as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p2">
<span class="ltx_inline-block"><svg class="ltx_picture" height="1063.6" id="A4.p2.pic1" overflow="visible" version="1.1" viewbox="0 0 600 1063.6" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,1063.6) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0" style="--ltx-fill-color:#404040;"><path d="M 0 5.91 L 0 1057.7 C 0 1060.96 2.64 1063.6 5.91 1063.6 L 594.09 1063.6 C 597.36 1063.6 600 1060.96 600 1057.7 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.97 5.91 L 1.97 1057.7 C 1.97 1059.87 3.73 1061.64 5.91 1061.64 L 594.09 1061.64 C 596.27 1061.64 598.03 1059.87 598.03 1057.7 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 16.47)"><foreignobject color="#000000" height="1036.05" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:74.68em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 1033.35)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: system, <span class="ltx_text ltx_font_bold">Content</span>: You are a helpful assistant and an expert in image editing.</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: user, <span class="ltx_text ltx_font_bold">Content</span>: Task: As a researcher in image editing, your task is to generate simple editing instructions based on the given image.</span>
<span class="ltx_p">The edit types you can use include:
1) local color change,
2) local texture,
3) adjust (shape change),
4) add,
5) remove,
6) replace,
7) bg,
8) style,
9) action, and
10) text manipulation</span>
<span class="ltx_p">**Important**: Ensure that you create a balanced distribution of these edit types when generating the instructions. Each example should utilize a different edit type, and the edit types should be evenly distributed across all examples.</span>
<span class="ltx_p">When using the “add” edit type, DO NOT USE vague placements like ‘near’, ‘under’, or ‘beside’, instead, specify the exact location where the object should be placed. For example, instead of “add a castle near the trees” use “add a castle in the clearing between the trees”.</span>
<span class="ltx_p">Ensure that each instruction is straightforward and points to a single, clear edit change. Avoid complex or multi-step instructions.</span>
<span class="ltx_p">**Avoid Redundancy**: Make sure to introduce diversity in the edit instructions.</span>
<span class="ltx_p">Given the input image, could you generate simple edit instructions for different possible edit types by following the “format” of examples below and based on what you have seen in the image?</span>
<span class="ltx_p">Here are some examples showing the use of various edit types:</span>
<span class="ltx_p">Good example 1: {color change example}</span>
<span class="ltx_p">Good example 2: {texture change example}</span>
<span class="ltx_p">Good example 3: {adjust shape example}</span>
<span class="ltx_p">Good example 4: {add example}</span>
<span class="ltx_p">Good example 5: {remove example}</span>
<span class="ltx_p">Good example 6: {replace example}</span>
<span class="ltx_p">Good example 7: {bg example}</span>
<span class="ltx_p">Good example 8: {style example }</span>
<span class="ltx_p">Good example 9: {action example}</span>
<span class="ltx_p">Good example 10: {text manipulation example}</span>
<span class="ltx_p">Bad Examples: the edit instructions are hard/impossible to perform well, or mention vague terms that make the editing model struggle to perform well, and you should not follow.</span>
<span class="ltx_p">Bad example 1:</span>
<span class="ltx_p">- Instruction: make this dog look like it’s ready for a formal evening out?</span>
<span class="ltx_p">- Type: add</span>
<span class="ltx_p">- Reasoning: This instruction is bad because it does not mention the exact changes that are needed to make the dog look like it’s ready for a formal evening out.</span>
<span class="ltx_p">Bad example 2:</span>
<span class="ltx_p">- Instruction: remove the balloon [given an image of only balloons on a white background]</span>
<span class="ltx_p">- Type: remove</span>
<span class="ltx_p">- Reasoning: This instruction is bad as it removes the only object in the image.</span>
<span class="ltx_p">**Important Considerations**:</span>
<span class="ltx_p">1. Avoid repetition of specific phrases: Do not reuse examples or themes from the above examples. Create entirely new and diverse themes and scenarios.</span>
<span class="ltx_p">2. Logical Flow: Ensure that each instruction is logical and makes sense given the image.</span>
<span class="ltx_p">3. Specificity in Insertions: When adding objects, use precise placement (e.g., “in the sky” or “on the lake”). Avoid vague terms like “next to”, “around”, or “near”.</span>
<span class="ltx_p">4. Balanced use of edit types: Use a variety of edit types such as [insertion], [replace], [local texture], [shape change], [style], [remove], [local color change], and [bg]. Ensure an even distribution of these edit types across your examples.</span>
<span class="ltx_p">5. Diverse scenarios: Introduce variety in the scenarios, such as futuristic, historical, magical, surreal, or natural settings. Avoid overusing common tropes.</span>
<span class="ltx_p">6. DO NOT suggest instructions that change a very small/minute part of the image.</span>
<span class="ltx_p">Could you now generate 4 examples of **new, creative, and contextually relevant** edit instructions by following the format above? Avoid using the specific phrases, themes, or scenarios from the examples provided above.</span>
<span class="ltx_p">**Each example must use a different edit type** from the ones listed above. Also, make sure to use each edit type equally across all generated examples.</span>
<span class="ltx_p">Finally, you should make the edit instructions as simple as possible so that the downstream editing model is able to work well.</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A4.p3">
<p class="ltx_p">In the above user prompt, for the good examples, we randomly select an edit instruction for each editing type out of a fixed set of manually defined edit instructions. Given edit instructions for each image, we again prompt the VLM to check the validity of the edit instruction and, if valid, to suggest a possible caption for the edited image. The system and user prompt for this is:</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p4">
<span class="ltx_inline-block"><svg class="ltx_picture" height="897.56" id="A4.p4.pic1" overflow="visible" version="1.1" viewbox="0 0 600 897.56" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,897.56) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0" style="--ltx-fill-color:#404040;"><path d="M 0 5.91 L 0 891.65 C 0 894.92 2.64 897.56 5.91 897.56 L 594.09 897.56 C 597.36 897.56 600 894.92 600 891.65 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.97 5.91 L 1.97 891.65 C 1.97 893.83 3.73 895.59 5.91 895.59 L 594.09 895.59 C 596.27 895.59 598.03 893.83 598.03 891.65 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 16.47)"><foreignobject color="#000000" height="870" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:62.68em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 867.31)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: system, <span class="ltx_text ltx_font_bold">Content</span>: You are a helpful assistant and an expert in image editing.</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: user, <span class="ltx_text ltx_font_bold">Content</span>: Task: As a researcher in image editing, given the input image, edit type, and the edit instruction, your task is to check if a given edit instruction is valid and can be applied to the image. If it is valid, generate a descriptive caption for what the image would look like after applying the edit instruction. If it is not valid, return “invalid” and explain why it is not valid, and output “NA” for the edited image caption.</span>
<span class="ltx_p">An edit instruction is invalid if it:</span>
<span class="ltx_p">1. mentions to modify/remove/replace an object that is NOT PRESENT in the image.</span>
<span class="ltx_p">2. is TOO HARD to make editing model to understand and perform well, e.g., “remove any visible accessories.”</span>
<span class="ltx_p">3. DOES NOT change the image in any meaningful way, e.g., given the image of a forest, “change the background to a dense forest.”</span>
<span class="ltx_p">For the “remove” edit type:</span>
<span class="ltx_p">- DO NOT mention the object that is removed during the edit in the edited image caption. For example, given an image of a cat in a living room on a sofa with the edit type ”remove” and edit instruction: “remove the cat”</span>
<span class="ltx_p">Bad Example: A cat is removed from the sofa in a living room.</span>
<span class="ltx_p">Good Example: A living room with a sofa.</span>
<span class="ltx_p">Given the edit instruction and the original caption:</span>
<span class="ltx_p">Edit type: {edit type}</span>
<span class="ltx_p">Edit instruction: {simple edit instruction}</span>
<span class="ltx_p">Output format:</span>
<span class="ltx_p">Validity: …</span>
<span class="ltx_p">Reasoning: …</span>
<span class="ltx_p">Edited image Caption: …</span>
<span class="ltx_p">Please provide a concise but complete caption describing the edited image. Focus on the changes that would be made according to the edit instruction.</span>
<span class="ltx_p">Here are some more examples:</span>
<span class="ltx_p">Example 1:</span>
<span class="ltx_p">- Edit type: bg</span>
<span class="ltx_p">- Edit instruction: change the background to a sunset view</span>
<span class="ltx_p">- Validity: valid</span>
<span class="ltx_p">- Reasoning: The edit instruction is valid because it adjusts the current blue sky to a sunset view, which is a meaningful change.</span>
<span class="ltx_p">- Edited image caption: A park with a sunset view. People are walking around in the park.</span>
<span class="ltx_p">Example 2:</span>
<span class="ltx_p">- Edit type: remove</span>
<span class="ltx_p">- Edit instruction: remove the wine glass</span>
<span class="ltx_p">- Validity: invalid</span>
<span class="ltx_p">- Reasoning: The edit instruction is invalid because it mentions removing a wine glass that is not present in the image.</span>
<span class="ltx_p">- Edited image caption: NA</span>
<span class="ltx_p">**Important Considerations**:</span>
<span class="ltx_p">1. DO NOT use instruction words like replaced, added, removed, modified, etc. in the caption.</span>
<span class="ltx_p">2. Keep the caption general to explain any possible images resulting from the edit instruction.</span>
<span class="ltx_p">Only output the validity, reasoning, and edited image caption. Do not include any other text or explanations.</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A4.p5">
<p class="ltx_p">After filtering the list of generated editing instructions using the above procedure, our final dataset consists of approximately <math alttext="3" class="ltx_Math" display="inline" id="A4.p5.m1" intent=":literal"><semantics><mn>3</mn><annotation encoding="application/x-tex">3</annotation></semantics></math>M unique reference images with corresponding editing instructions spanning the <math alttext="10" class="ltx_Math" display="inline" id="A4.p5.m2" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math> edit sub-types. Within the constraints of our available computational resources, this represents the largest dataset we were able to construct.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p6">
<p class="ltx_p">For the customization task, we first instruct the VLM, to identify if the image has a prominent object in the center. We provide an in-context sample image as well to the model. The exact system and user prompt for this is:</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p7">
<span class="ltx_inline-block"><svg class="ltx_picture" height="549.64" id="A4.p7.pic1" overflow="visible" version="1.1" viewbox="0 0 600 549.64" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,549.64) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0" style="--ltx-fill-color:#404040;"><path d="M 0 5.91 L 0 543.73 C 0 546.99 2.64 549.64 5.91 549.64 L 594.09 549.64 C 597.36 549.64 600 546.99 600 543.73 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.97 5.91 L 1.97 543.73 C 1.97 545.91 3.73 547.67 5.91 547.67 L 594.09 547.67 C 596.27 547.67 598.03 545.91 598.03 543.73 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 17.24)"><foreignobject color="#000000" height="522.08" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:37.48em;--fo_depth :0.25em;" transform="matrix(1 0 0 -1 0 518.62)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: system, <span class="ltx_text ltx_font_bold">Content</span>: You are a helpful assistant and an expert in image personalization/customization.</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: user, <span class="ltx_text ltx_font_bold">Content</span>: Task:
You are assisting in a research project on image personalization. Your goal is to evaluate whether the SECOND image contains a **single, uniquely identifiable object** prominently positioned near the center of the frame.</span>
<span class="ltx_p">- The FIRST image (image_path1) is an example of a valid case.</span>
<span class="ltx_p">- The specific object category in the second image can be different — focus only on **object uniqueness** and **image composition**.</span>
<span class="ltx_p">Good examples include object categories that can be personalized, have unique texture, and are not general objects:</span>
<span class="ltx_p">- Backpack, purse, toy, cat, dog, cup, bowl, water bottle, wearables, plushies, bike, car, clocks, etc.</span>
<span class="ltx_p">Bad examples include object categories that are general objects, and different instances of the category can not be distinguished:</span>
<span class="ltx_p">- Tree, building, door, flowers, food, vegetables, fruits, natural scenes, roads, etc.</span>
<span class="ltx_p">**Important Considerations:**</span>
<span class="ltx_p">1. The object should be clearly recognizable and **visually distinct** from the background.</span>
<span class="ltx_p">2. The object should be **near the center** of the image.</span>
<span class="ltx_p">3. The **entire object** should be visible — it should NOT be a tight or zoomed-in crop.</span>
<span class="ltx_p">4. The background can be natural but should not be overly cluttered or visually distracting.</span>
<span class="ltx_p">5. The image should feature a **single primary object**, not multiple equally prominent objects.</span>
<span class="ltx_p">Could you now judge the SECOND image and only provide the output, reasoning, and object name, in the following format:</span>
<span class="ltx_p">Output: True/False</span>
<span class="ltx_p">Reasoning: Brief explanation</span>
<span class="ltx_p">Object Name: The name of the object (e.g., “backpack”, “cat”, “toy”).</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A4.p8">
<p class="ltx_p">If the VLM response predicts a valid image, we then query it again to suggest a new background context for the object category as follows:
</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p9">
<span class="ltx_inline-block"><svg class="ltx_picture" height="463.16" id="A4.p9.pic1" overflow="visible" version="1.1" viewbox="0 0 600 463.16" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,463.16) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0" style="--ltx-fill-color:#404040;"><path d="M 0 5.91 L 0 457.25 C 0 460.51 2.64 463.16 5.91 463.16 L 594.09 463.16 C 597.36 463.16 600 460.51 600 457.25 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.97 5.91 L 1.97 457.25 C 1.97 459.42 3.73 461.19 5.91 461.19 L 594.09 461.19 C 596.27 461.19 598.03 459.42 598.03 457.25 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="435.6" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:31.48em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 435.6)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: system, <span class="ltx_text ltx_font_bold">Content</span>: You are a helpful assistant and an expert in image personalization/customization.</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: user, <span class="ltx_text ltx_font_bold">Content</span>: Given an image of an object category, you have to suggest three DIVERSE background captions for the object. Provide a detailed description of the background scene. Only suggest plausible backgrounds. DO NOT add the object name in the caption. DO NOT use emotional words in the caption. Be concise and factual but not too short. DO NOT mention the object name in the output captions. If the object is not a thing, but a scene, then output None.</span>
<span class="ltx_p">Example background captions for “White plastic bottle” are:</span>
<span class="ltx_p">1. near the edge of a marbled kitchen counter, surrounded by a cutting board with chopped vegetables, a salt shaker, and a stainless steel sink in the background.</span>
<span class="ltx_p">2. rests on a tiled bathroom shelf, accompanied by a toothbrush holder, a mirror with foggy edges, and a shower curtain partially drawn open.</span>
<span class="ltx_p">Example background captions for “a blue truck” are:</span>
<span class="ltx_p">1. parked beside a graffiti-covered brick wall under a cloudy sky, with city skyscrapers rising in the background.</span>
<span class="ltx_p">2. resting in a grassy field surrounded by wildflowers, with distant mountains and a golden sunset in the background.</span>
<span class="ltx_p">Object: {object category name}</span>
<span class="ltx_p">Output:</span>
<span class="ltx_p">1.</span>
<span class="ltx_p">2.</span>
<span class="ltx_p">3.</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Training Implementation Details</h2>
<section class="ltx_subsection" id="A5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Local-image editing</h3>
<div class="ltx_para ltx_noindent" id="A5.SS1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Training hyperparameters.</span>   We train on a batch-size of <math alttext="32" class="ltx_Math" display="inline" id="A5.SS1.p1.m1" intent=":literal"><semantics><mn>32</mn><annotation encoding="application/x-tex">32</annotation></semantics></math> using Adam <cite class="ltx_cite ltx_citemacro_citep">(Adam et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib1" title="">2014</a>)</cite> optimizer with a learning rate of <math alttext="2\times 10^{-6}" class="ltx_Math" display="inline" id="A5.SS1.p1.m2" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2\times 10^{-6}</annotation></semantics></math>, <math alttext="\beta_{1}" class="ltx_Math" display="inline" id="A5.SS1.p1.m3" intent=":literal"><semantics><msub><mi>β</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\beta_{1}</annotation></semantics></math> as 0, and <math alttext="\beta_{2}" class="ltx_Math" display="inline" id="A5.SS1.p1.m4" intent=":literal"><semantics><msub><mi>β</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\beta_{2}</annotation></semantics></math> as <math alttext="0.9" class="ltx_Math" display="inline" id="A5.SS1.p1.m5" intent=":literal"><semantics><mn>0.9</mn><annotation encoding="application/x-tex">0.9</annotation></semantics></math>. We train for a total of <math alttext="10" class="ltx_Math" display="inline" id="A5.SS1.p1.m6" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math>K iteratuions with auxiliary network, <math alttext="A_{\phi}" class="ltx_Math" display="inline" id="A5.SS1.p1.m7" intent=":literal"><semantics><msub><mi>A</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">A_{\phi}</annotation></semantics></math>, being updated <math alttext="10" class="ltx_Math" display="inline" id="A5.SS1.p1.m8" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math> times for every generator, <math alttext="G_{\theta}" class="ltx_Math" display="inline" id="A5.SS1.p1.m9" intent=":literal"><semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">G_{\theta}</annotation></semantics></math>, update, following similar strategy adopted in DMD2 <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib86" title="">2024a</a>)</cite>. We train with the identity loss (Section <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S4.SS3" title="4.3 Training details ‣ 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">4.3</span></a>) for <math alttext="250" class="ltx_Math" display="inline" id="A5.SS1.p1.m10" intent=":literal"><semantics><mn>250</mn><annotation encoding="application/x-tex">250</annotation></semantics></math> iterations. For faster convergence, the first <math alttext="4" class="ltx_Math" display="inline" id="A5.SS1.p1.m11" intent=":literal"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math>K training iterations are trained with a single step prediction (<math alttext="t=1" class="ltx_Math" display="inline" id="A5.SS1.p1.m12" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t=1</annotation></semantics></math> in Line 3 of Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#alg1" title="In 4.3 Training details ‣ 4 Method ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">1</span></a>), and then we start the 2-step unrolling of the diffusion trajectory. The final loss is a weighted combination of VLM-based editing loss and distribution matching loss with <math alttext="\lambda_{\text{VLM}}=0.01" class="ltx_Math" display="inline" id="A5.SS1.p1.m13" intent=":literal"><semantics><mrow><msub><mi>λ</mi><mtext>VLM</mtext></msub><mo>=</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">\lambda_{\text{VLM}}=0.01</annotation></semantics></math> and <math alttext="\lambda_{\text{DMD}}=0.5" class="ltx_Math" display="inline" id="A5.SS1.p1.m14" intent=":literal"><semantics><mrow><msub><mi>λ</mi><mtext>DMD</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\lambda_{\text{DMD}}=0.5</annotation></semantics></math>. During training, we also add a “do nothing” editing task with <math alttext="\mathcal{L}_{2}" class="ltx_Math" display="inline" id="A5.SS1.p1.m15" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{L}_{2}</annotation></semantics></math> loss between the input and edited image as regularization with a <math alttext="1\%" class="ltx_Math" display="inline" id="A5.SS1.p1.m16" intent=":literal"><semantics><mrow><mn>1</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">1\%</annotation></semantics></math> probability. This helps the model learn to maintain consistency between input and edited images. During training, we sample the editing instruction corresponding to each subtype uniformly, except <em class="ltx_emph ltx_font_italic">removal</em>, which is sampled with <math alttext="25\%" class="ltx_Math" display="inline" id="A5.SS1.p1.m17" intent=":literal"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">25\%</annotation></semantics></math> probability. This is because, empirically, we observe that <em class="ltx_emph ltx_font_italic">removal</em> is more difficult than other edit-types like <em class="ltx_emph ltx_font_italic">Color change</em>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A5.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Template questions for VLM-based editing loss.</span>
As mentioned in the main paper, we evaluate VLM-based loss on two questions per edit type. Specifically for any edit type except removal, we use the following template:</p>
</div>
<div class="ltx_para ltx_noindent" id="A5.SS1.p3">
<span class="ltx_inline-block"><svg class="ltx_picture" height="233.38" id="A5.SS1.p3.pic1" overflow="visible" version="1.1" viewbox="0 0 600 233.38" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,233.38) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0" style="--ltx-fill-color:#404040;"><path d="M 0 5.91 L 0 227.48 C 0 230.74 2.64 233.38 5.91 233.38 L 594.09 233.38 C 597.36 233.38 600 230.74 600 227.48 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.97 5.91 L 1.97 227.48 C 1.97 229.65 3.73 231.42 5.91 231.42 L 594.09 231.42 C 596.27 231.42 598.03 229.65 598.03 227.48 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 16.47)"><foreignobject color="#000000" height="205.83" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:14.68em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 203.13)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: user, <span class="ltx_text ltx_font_bold">Content</span>: You are a professional digital artist and an expert image editor. You will be provided with two images.</span>
<span class="ltx_p">The first being the original real image, and the second being an edited version of the first.</span>
<span class="ltx_p">The objective is to evaluate if the editing instruction has been executed in the second image.</span>
<span class="ltx_p">Editing instruction: {edit instruction}</span>
<span class="ltx_p">Answer with a Yes or No.</span>
<span class="ltx_p">Note that sometimes the two images might look identical due to the failure of image editing. Answer No in that case.</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A5.SS1.p4">
<span class="ltx_inline-block"><svg class="ltx_picture" height="166.97" id="A5.SS1.p4.pic1" overflow="visible" version="1.1" viewbox="0 0 600 166.97" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,166.97) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0" style="--ltx-fill-color:#404040;"><path d="M 0 5.91 L 0 161.06 C 0 164.32 2.64 166.97 5.91 166.97 L 594.09 166.97 C 597.36 166.97 600 164.32 600 161.06 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.97 5.91 L 1.97 161.06 C 1.97 163.24 3.73 165 5.91 165 L 594.09 165 C 596.27 165 598.03 163.24 598.03 161.06 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 16.47)"><foreignobject color="#000000" height="139.41" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:9.88em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 136.72)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: user, <span class="ltx_text ltx_font_bold">Content</span>: You are a professional digital artist and an expert image editor. You will be provided with two images.</span>
<span class="ltx_p">Answer with a Yes or No if the second image is exactly the same as the first image. IGNORE the changes in the second image because of the edit: {edit instruction}. Everything else should be the same.</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A5.SS1.p5">
<p class="ltx_p">For the removal edit-type, we change the first question to explicitly ask about the presence of the target object to be removed, with the ground truth answer in this case being <em class="ltx_emph ltx_font_italic">No</em>. We find it to be more effective than a generic template.</p>
</div>
<div class="ltx_para ltx_noindent" id="A5.SS1.p6">
<span class="ltx_inline-block"><svg class="ltx_picture" height="134.53" id="A5.SS1.p6.pic1" overflow="visible" version="1.1" viewbox="0 0 600 134.53" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,134.53) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0" style="--ltx-fill-color:#404040;"><path d="M 0 5.91 L 0 128.62 C 0 131.88 2.64 134.53 5.91 134.53 L 594.09 134.53 C 597.36 134.53 600 131.88 600 128.62 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.97 5.91 L 1.97 128.62 C 1.97 130.8 3.73 132.56 5.91 132.56 L 594.09 132.56 C 596.27 132.56 598.03 130.8 598.03 128.62 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 17.24)"><foreignobject color="#000000" height="106.97" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:7.48em;--fo_depth :0.25em;" transform="matrix(1 0 0 -1 0 103.51)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: user, <span class="ltx_text ltx_font_bold">Content</span>: You are a professional digital artist and an expert image captioner. You will be provided with an image.</span>
<span class="ltx_p">Answer with a Yes or No if the image has {object name}.</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
</section>
<section class="ltx_subsection" id="A5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Free-form editing (Customization)</h3>
<div class="ltx_para ltx_noindent" id="A5.SS2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Training hyperparameters.</span>   We reduce the warmup iterations for which we train with the identity loss to <math alttext="100" class="ltx_Math" display="inline" id="A5.SS2.p1.m1" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> in this case, since customization often requires more drastic changes in the output image compared to the input reference image. Further, we increase <math alttext="\lambda_{\text{DMD}}=2" class="ltx_Math" display="inline" id="A5.SS2.p1.m2" intent=":literal"><semantics><mrow><msub><mi>λ</mi><mtext>DMD</mtext></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\lambda_{\text{DMD}}=2</annotation></semantics></math> instead of <math alttext="0.5" class="ltx_Math" display="inline" id="A5.SS2.p1.m3" intent=":literal"><semantics><mn>0.5</mn><annotation encoding="application/x-tex">0.5</annotation></semantics></math> as in the case of local image-editing. The rest of the hyperparameters remain the same. Both during training and inference, the input text prompt to the few-step generator, <math alttext="G_{\theta}" class="ltx_Math" display="inline" id="A5.SS2.p1.m4" intent=":literal"><semantics><msub><mi>G</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">G_{\theta}</annotation></semantics></math>, is in the following template: Generate the main object shown in the first image in a different setting and pose: { background scene description}. We train the 4-step model for <math alttext="10" class="ltx_Math" display="inline" id="A5.SS2.p1.m5" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math>K iterations. For the 8-step model, we fine-tune for <math alttext="5" class="ltx_Math" display="inline" id="A5.SS2.p1.m6" intent=":literal"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math>K additional training steps starting from the 4-step model.</p>
</div>
<div class="ltx_para ltx_noindent" id="A5.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Template questions for VLM-based editing loss.</span>
Here, we modify the questions to instead evaluate if the background context and pose are different in the generated image, i.e., editing success, and if the object identity is similar, i.e., image alignment and consistency between the input reference and edited image. The exact questions are as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="A5.SS2.p3">
<span class="ltx_inline-block"><svg class="ltx_picture" height="166.97" id="A5.SS2.p3.pic1" overflow="visible" version="1.1" viewbox="0 0 600 166.97" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,166.97) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0" style="--ltx-fill-color:#404040;"><path d="M 0 5.91 L 0 161.06 C 0 164.32 2.64 166.97 5.91 166.97 L 594.09 166.97 C 597.36 166.97 600 164.32 600 161.06 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.97 5.91 L 1.97 161.06 C 1.97 163.24 3.73 165 5.91 165 L 594.09 165 C 596.27 165 598.03 163.24 598.03 161.06 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 16.47)"><foreignobject color="#000000" height="139.41" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:9.88em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 136.72)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: user, <span class="ltx_text ltx_font_bold">Content</span>: You are a professional digital artist and an expert in image editing. You will be provided with two images.</span>
<span class="ltx_p">Answer with a Yes or No if the {object_name} in the second image is in a different pose and location than in the first image.
Note that sometimes the second image might not have the same object because of the failure of image editing. Answer No in that case.</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A5.SS2.p4">
<span class="ltx_inline-block"><svg class="ltx_picture" height="180.88" id="A5.SS2.p4.pic1" overflow="visible" version="1.1" viewbox="0 0 600 180.88" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,180.88) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0" style="--ltx-fill-color:#404040;"><path d="M 0 5.91 L 0 174.98 C 0 178.24 2.64 180.88 5.91 180.88 L 594.09 180.88 C 597.36 180.88 600 178.24 600 174.98 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.97 5.91 L 1.97 174.98 C 1.97 177.15 3.73 178.91 5.91 178.91 L 594.09 178.91 C 596.27 178.91 598.03 177.15 598.03 174.98 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="153.32" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:11.08em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 153.32)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: user, <span class="ltx_text ltx_font_bold">Content</span>: You are a professional digital artist and an expert in image editing. You will be provided with two images.</span>
<span class="ltx_p">Answer with a Yes or No if the {object_name} in the second image is the exact same identity, with similar color, shape, and texture as in the first image.
Note that sometimes the second image might not have the same object because of the failure of image editing. Answer No in that case.</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
</section>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Other Baseline Details</h2>
<div class="ltx_para ltx_noindent" id="A6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Flow-GRPO <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib45" title="">2025a</a>)</cite>.</span>   We follow the open-source implementation of Flow-GRPO and train with the same computational budget as our method, i.e., across <math alttext="4" class="ltx_Math" display="inline" id="A6.p1.m1" intent=":literal"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math> A100 GPUs and <math alttext="2.5" class="ltx_Math" display="inline" id="A6.p1.m2" intent=":literal"><semantics><mn>2.5</mn><annotation encoding="application/x-tex">2.5</annotation></semantics></math> days of training. The final model is fine-tuned from a pre-trained image-editing model for <math alttext="5" class="ltx_Math" display="inline" id="A6.p1.m3" intent=":literal"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math>K iterations. During training, we collect <math alttext="16" class="ltx_Math" display="inline" id="A6.p1.m4" intent=":literal"><semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation></semantics></math> images per-prompt with <math alttext="12" class="ltx_Math" display="inline" id="A6.p1.m5" intent=":literal"><semantics><mn>12</mn><annotation encoding="application/x-tex">12</annotation></semantics></math> denoising steps (<math alttext="28" class="ltx_Math" display="inline" id="A6.p1.m6" intent=":literal"><semantics><mn>28</mn><annotation encoding="application/x-tex">28</annotation></semantics></math> during inference) for computing the mean and standard deviation in GRPO <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib65" title="">2024</a>)</cite>. Following their official implementation, we train with LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib27" title="">2022</a>)</cite> of rank <math alttext="32" class="ltx_Math" display="inline" id="A6.p1.m7" intent=":literal"><semantics><mn>32</mn><annotation encoding="application/x-tex">32</annotation></semantics></math>, <math alttext="\alpha=64" class="ltx_Math" display="inline" id="A6.p1.m8" intent=":literal"><semantics><mrow><mi>α</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">\alpha=64</annotation></semantics></math>, learning rate <math alttext="1\times 10^{-4}" class="ltx_Math" display="inline" id="A6.p1.m9" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-4}</annotation></semantics></math>, and use the VLM to score edits on a scale of 0 to 9, which is normalized between 0-1 to get the final reward. The exact prompt used to query the VLM is derived from VIEScore <cite class="ltx_cite ltx_citemacro_citep">(Ku et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib37" title="">2024</a>)</cite> and is shown below.</p>
</div>
<div class="ltx_para ltx_noindent" id="A6.p2">
<span class="ltx_inline-block"><svg class="ltx_picture" height="383.59" id="A6.p2.pic1" overflow="visible" version="1.1" viewbox="0 0 600 383.59" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,383.59) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0" style="--ltx-fill-color:#404040;"><path d="M 0 5.91 L 0 377.69 C 0 380.95 2.64 383.59 5.91 383.59 L 594.09 383.59 C 597.36 383.59 600 380.95 600 377.69 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0" style="--ltx-fill-color:#F2F2F2;"><path d="M 1.97 5.91 L 1.97 377.69 C 1.97 379.86 3.73 381.62 5.91 381.62 L 594.09 381.62 C 596.27 381.62 598.03 379.86 598.03 377.69 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 17.24)"><foreignobject color="#000000" height="356.03" overflow="visible" style="--ltx-fg-color:#000000;--fo_width :40.23em;--fo_height:25.48em;--fo_depth :0.25em;" transform="matrix(1 0 0 -1 0 352.57)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: system, <span class="ltx_text ltx_font_bold">Content</span>: You are a helpful assistant and an expert in image editing.</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Role</span>: user, <span class="ltx_text ltx_font_bold">Content</span>: You are a professional digital artist. You will have to evaluate the effectiveness of AI-generated edited image(s) based on given rules.</span>
<span class="ltx_p">You will have to give your output in this way (Keep your reasoning VERY CONCISE and SHORT):</span>
<span class="ltx_p">score : …,</span>
<span class="ltx_p">reasoning : …</span>
<span class="ltx_p">RULES:</span>
<span class="ltx_p">Two images will be provided: The first being the original real image and the second being an edited version of the first.</span>
<span class="ltx_p">The objective is to evaluate how successfully the editing instruction has been executed in the second image.</span>
<span class="ltx_p">Note that sometimes the two images might look identical due to the failure of image edit.</span>
<span class="ltx_p">From scale 0 to 9:</span>
<span class="ltx_p">A score from 0 to 9 will be given based on the success of the editing. (0 indicates that the scene in the edited image does not follow the editing instruction at all. 9 indicates that the scene in the edited image follows the editing instruction text perfectly.)</span>
<span class="ltx_p">Editing instruction: {edit instruction}</span>
</span></span></span></foreignobject></g></g></svg></span>
</div>
<div class="ltx_para ltx_noindent" id="A6.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Supervised Fine-Tuning.</span>   We train with the standard velocity prediction flow-objective for <math alttext="30" class="ltx_Math" display="inline" id="A6.p3.m1" intent=":literal"><semantics><mn>30</mn><annotation encoding="application/x-tex">30</annotation></semantics></math>K iterations with a batch-size of <math alttext="32" class="ltx_Math" display="inline" id="A6.p3.m2" intent=":literal"><semantics><mn>32</mn><annotation encoding="application/x-tex">32</annotation></semantics></math> and learning rate <math alttext="2\times 10^{-6}" class="ltx_Math" display="inline" id="A6.p3.m3" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2\times 10^{-6}</annotation></semantics></math> with a linear warmup of <math alttext="2" class="ltx_Math" display="inline" id="A6.p3.m4" intent=":literal"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math>K iterations. To enable classifier-free guidance, we drop the image and text conditions <math alttext="10\%" class="ltx_Math" display="inline" id="A6.p3.m5" intent=":literal"><semantics><mrow><mn>10</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">10\%</annotation></semantics></math> of the time.</p>
</div>
<div class="ltx_para ltx_noindent" id="A6.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Sampling parameters for local image-editing baselines.</span>   We follow the open-source implementation to sample images from all the baseline models for the benchmark evaluations. The turbo-edit <cite class="ltx_cite ltx_citemacro_citep">(Deutch et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib14" title="">2024</a>)</cite> baseline requires a caption corresponding to the edited image as well, and we use Qwen-2.5-32B-VLM to generate these captions for GEdit-Bench images.</p>
</div>
<div class="ltx_para ltx_noindent" id="A6.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Sampling parameters for customization baselines.</span>   Here as well, we follow the open-source implementation to sample images from all the baseline models for the benchmark evaluations. In the case of DSD <cite class="ltx_cite ltx_citemacro_citep">(Cai et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib6" title="">2025</a>)</cite>, it employs Gemini-1.5 to convert the input user-prompt into a detailed prompt. However, we skipped this step for a fair evaluation with other methods, which do not use any prompt rewriting tools. In the case of SynCD <cite class="ltx_cite ltx_citemacro_citep">(Kumari et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib39" title="">2025</a>)</cite>, though it supports multiple reference images as input, we evaluated it with a single reference image, to keep the setup similar to other baseline methods and Ours. For sampling images with OminiControl <cite class="ltx_cite ltx_citemacro_citep">(Tan et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib73" title="">2025</a>)</cite> and DSD <cite class="ltx_cite ltx_citemacro_citep">(Cai et al., <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib6" title="">2025</a>)</cite>, we follow their recommended prompt setting and replace the category name with “this item”.</p>
</div>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Societal Impact</h2>
<div class="ltx_para ltx_noindent" id="A7.p1">
<p class="ltx_p">Our work introduces a training framework for fine-tuning text-to-image models into a <em class="ltx_emph ltx_font_italic">few-step</em> image-editing model without using paired supervision. By enabling few-step sampling, our method improves inference efficiency and reduces computational cost. Nonetheless, the broader risks of generative models, such as creating deepfakes and misleading content, also apply to our approach. Possible ways to mitigate this are watermarking generative content <cite class="ltx_cite ltx_citemacro_cite">Fernandez et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib15" title="">2023</a>)</cite> and reliable detection of generated images <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib75" title="">2020</a>); Corvi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib12" title="">2023</a>); Cazenavette et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#bib.bib8" title="">2024</a>)</cite></p>
</div>
<figure class="ltx_figure" id="A7.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1244" id="A7.F10.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span class="ltx_text ltx_font_bold">Qualitative comparison on GEdit-Bench.</span> We show results of our and baseline image-editing methods under the few-step sampling setting. For comparison, we also show the results of the best method with multi-step sampling, as measured by the quantitative metrics (Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#S5.T1" title="Table 1 ‣ 5.1 Local image-editing ‣ 5 Experiments ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">1</span></a>), in the <math alttext="{}^{1}{\text{st}}" class="ltx_Math" display="inline" id="A7.F10.m2" intent=":literal"><semantics><mmultiscripts><mtext>st</mtext><mprescripts></mprescripts><mrow></mrow><mn>1</mn></mmultiscripts><annotation encoding="application/x-tex">{}^{1}{\text{st}}</annotation></semantics></math> column. Our method performs on par or better than baseline methods across different edit types in the few-step setting.
</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1137" id="A7.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span class="ltx_text ltx_font_bold">Qualitative comparison on ImgEdit-Bench.</span> We show results of our and baseline image-editing methods under the few-step sampling setting. For comparison, we also show the results of the best method with multi-step sampling, as measured by the quantitative metrics (Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14978v1#A1.T6" title="Table 6 ‣ Appendix A Additional Comparison with Baseline Methods ‣ Learning an Image Editing Model without Image Editing Pairs"><span class="ltx_text ltx_ref_tag">6</span></a>), in the <math alttext="1^{\text{st}}" class="ltx_Math" display="inline" id="A7.F11.m2" intent=":literal"><semantics><msup><mn>1</mn><mtext>st</mtext></msup><annotation encoding="application/x-tex">1^{\text{st}}</annotation></semantics></math> column. Our method performs on par or better than baseline methods across different edit types in the few-step setting.
</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1172" id="A7.F12.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span><span class="ltx_text ltx_font_bold">Qualitative comparison on DreamBooth.</span> We show results of our and baseline methods under the few-step sampling setting. For comparison, we also show the results of the best method with multi-step sampling, as measured by the quantitative metrics in the first column. Our method performs comparably with baseline methods on identity alignment while having better image fidelity across different concepts in the few-step setting.
</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 16 17:50:04 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
