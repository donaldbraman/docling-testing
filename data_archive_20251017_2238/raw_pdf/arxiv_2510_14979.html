<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>From Pixels to Words – Towards Native Vision-Language Primitives at Scale</title>
<!--Generated on Thu Oct 16 13:19:47 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2510.14979v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S1" title="In From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S2" title="In From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S2.SS1" title="In 2 Related Works ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Modular Vision-Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S2.SS2" title="In 2 Related Works ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Native Vision-Language Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S3" title="In From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S3.SS1" title="In 3 Methodology ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S3.SS2" title="In 3 Methodology ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training Procedure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4" title="In From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.SS1" title="In 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Training Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.SS2" title="In 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.SS3" title="In 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation Studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S5" title="In From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#A1" title="In From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#A1.SS1" title="In Appendix A Appendix ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Supervised Fine-tuning Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#A1.SS2" title="In Appendix A Appendix ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#A1.SS3" title="In Appendix A Appendix ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Limitation and Discussion</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">From Pixels to Words – Towards Native Vision-Language Primitives at Scale</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_align_center">
Haiwen Diao<sup class="ltx_sup">1</sup> Mingxuan Li<sup class="ltx_sup">2</sup> Silei Wu<sup class="ltx_sup">3</sup> Linjun Dai<sup class="ltx_sup">3</sup> </span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_align_center">
<span class="ltx_text ltx_font_bold">
Xiaohua Wang<sup class="ltx_sup"><span class="ltx_text ltx_font_medium">2</span></sup>  Hanming Deng<sup class="ltx_sup"><span class="ltx_text ltx_font_medium">3</span></sup>  Lewei Lu<sup class="ltx_sup"><span class="ltx_text ltx_font_medium">3</span></sup>  Dahua Lin<sup class="ltx_sup"><span class="ltx_text ltx_font_medium">3</span></sup>  Ziwei Liu<sup class="ltx_sup"><span class="ltx_text ltx_font_medium">1</span></sup>
</span>
</span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_align_center">
<sup class="ltx_sup">1</sup>S-Lab,   Nanyang Technological University
</span>
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_align_center">
<sup class="ltx_sup">2</sup>Xi’an Jiaotong University  <sup class="ltx_sup">3</sup>SenseTime Research
</span>
<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_tabular ltx_parbox ltx_align_middle" style="width:397.5pt;">
<span class="ltx_tr">
<span class="ltx_td ltx_align_left"><span class="ltx_text" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="g1" src="figures/Gallery_Icon.png" width="15"/></span> <span class="ltx_text ltx_font_bold">Website:</span></span>
<span class="ltx_td ltx_align_left"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EvolvingLMMs-Lab/NEO" title="">https://github.com/EvolvingLMMs-Lab/NEO</a></span></span>
</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion:
(-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome?
(-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field.
In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should:
(i) effectively align pixel and word representations within a shared semantic space;
(ii) seamlessly integrate the strengths of formerly separate vision and language modules;
(iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning.
Hence, we launch <span class="ltx_text ltx_font_bold">NEO</span>, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios.
With only 390M image-text examples, <span class="ltx_text ltx_font_bold">NEO</span> efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives.
We position <span class="ltx_text ltx_font_bold">NEO</span> as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p">Recently, Vision-Language Models (VLMs) <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib4" title="">2025</a>); Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib125" title="">2025</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib105" title="">2025b</a>); xAI (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib110" title="">2025</a>); Anthropic (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib2" title="">2025</a>); DeepMind (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib24" title="">2025</a>); Hurst et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib40" title="">2024</a>); OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib83" title="">2025</a>)</cite> have emerged as a major breakthrough, extending the strong linguistic capabilities of Large Language Models (LLMs) to multimodal understanding.
They typically follow a modular design that integrates a pre-trained Visual Encoder (VE) <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib84" title="">2021</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib17" title="">2024f</a>); Fang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib33" title="">2023</a>); Tschannen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib100" title="">2025</a>)</cite>, a Projector <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib1" title="">2022</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib67" title="">2024a</a>); Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib21" title="">2024</a>)</cite>, and an LLM <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib99" title="">2023</a>); Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib113" title="">2025</a>); DeepSeek-AI et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib25" title="">2025</a>)</cite>.
Through multi-stage post-training at scale, they incrementally overcome limitations in image resolution, aspect ratio, and visual encoding flexibility.
Yet, modular designs still contend with strong inductive biases in pre-trained visual semantics, complex infrastructure, and scaling laws needed to harmonize their components.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p">Against this backdrop, native VLMs have arisen as a new avenue of exploration, with Fuyu <cite class="ltx_cite ltx_citemacro_cite">Bavishi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib6" title="">2023</a>)</cite> and EVE <cite class="ltx_cite ltx_citemacro_cite">Diao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib28" title="">2024</a>)</cite> pioneering a promising route towards monolithic VLMs.
Subsequent efforts seek to learn vision perception from scratch and mitigate vision-language conflicts via visual encoder distillation <cite class="ltx_cite ltx_citemacro_cite">Diao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib28" title="">2024</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib56" title="">2025b</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib102" title="">2025a</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib55" title="">2025a</a>)</cite>, mixed training data <cite class="ltx_cite ltx_citemacro_cite">Lei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib50" title="">2025</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib55" title="">2025a</a>)</cite>, and modality-specific decomposition <cite class="ltx_cite ltx_citemacro_cite">Diao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib29" title="">2025</a>); Luo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib75" title="">2024</a>; <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib76" title="">2025</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib55" title="">2025a</a>)</cite>.
Nonetheless, constructing visual representations via mapping functions inside pre-trained LLMs often hinders efficiency <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib15" title="">2024d</a>); Luo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib75" title="">2024</a>)</cite>, destabilizes optimization <cite class="ltx_cite ltx_citemacro_cite">Team (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib97" title="">2024</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib106" title="">2024b</a>)</cite>, and disrupts original linguistic knowledge <cite class="ltx_cite ltx_citemacro_cite">Diao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib28" title="">2024</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib15" title="">2024d</a>)</cite>, even under decoupled designs or large budgets <cite class="ltx_cite ltx_citemacro_cite">Beyer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib7" title="">2024</a>)</cite>.
Besides, HoVLE <cite class="ltx_cite ltx_citemacro_cite">Tao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib95" title="">2025</a>)</cite> and HaploVL <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib112" title="">2025</a>)</cite> address this by first mapping vision-language inputs into a shared space. Yet, their modality-sharing modules, whether derived from LLM or VE layers, neglect intrinsic discrepancy in encoding and interaction across modalities, ultimately compromising VLM’s capacity to unify visual-linguistic properties.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="267" id="S1.F1.g1" src="x1.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our native vision-language frameworks, which project arbitrary-resolution images into a continuous latent space, integrating the virtues of modular VLM architectures and enabling efficient vision-language encoding, alignment, and interaction in an early-fusion manner.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p">Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">1</span></a> outlines a central question: <span class="ltx_text ltx_font_italic">What properties must native VLMs possess to compete with modular ones?</span>
Modular VLMs decouple vision encoders from language models, allowing each to exploit modality-specific characteristics, <span class="ltx_text ltx_font_italic">e</span>.<span class="ltx_text ltx_font_italic">g</span>., bi-directional versus causal attention, distinct positional embeddings, and varied network configurations. This separation accelerates the development of visual and linguistic competencies and permits flexible combinations of individual components. However, it fragments the training procedure, increases alignment costs, and leaves the intermodal balance unresolved. Motivated by these analyses, we formulate the following strategies accordingly:</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">(1) Native VLM Primitive.</span>
Native VLMs should embody a unified vision–language primitive that simultaneously integrates encoding, alignment, and reasoning across modalities in one single module.
Its design should encompass three principles:
(i) a Flexible Position Encoding scheme that generalizes effectively to dynamic spatial structures;
(ii) a Multi-Head Native Attention (MHNA) that jointly processes visual–textual connectivity;
(iii) Native Rotary Position Embeddings (Native-RoPE) with modality-specific frequencies, preserving compatibility with pretrained LLM’s weights while absorbing original VE’s interaction patterns.
Guided by these tenets, we supercharge the fundamental LLM layers with a hybrid attention, expanded heads, and targeted RoPE across modalities, synchronously capturing multi-dimensional relationships for fine-grained and comprehensive correspondence.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">(2) Pre-Buffer and Post-LLM.</span>
The next crucial issue is to efficiently scale visual training while securing consistent pixel-word alignment.
Here, we partition the monolithic backbone into pre-Buffer and post-LLM layers during pre-training, each rooted in identical native primitive architectures.
This transient stage enables pretrained LLMs to steer visual learning and establish coherent relevance with later stages.
As mid-training and supervised fine-tuning advance, the partition dissolves, yielding a unified architecture that autonomously allocates the VLM’s capacities to their respective functions.
This end-to-end training reduces semantic biases of separate pretraining and large overheads of post-stage alignment, effectively bridging native and modular VLMs. Crucially, pre-Buffer persists as a reusable pretrained asset, facilitating sustainable resources for native VLM development.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="165" id="S1.F2.g1" src="x2.png" width="585"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of our proposed NEO architecture. We begin with lightweight patch and word embedding layers that encode images and text into token sequences, which are subsequently processed by a monolithic decoder-only architecture. The pre-Buffer and post-LLM components, each stacked with multiple native primitives, facilitate efficient and precise pixel–word alignment and reasoning.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p">We launch <span class="ltx_text ltx_font_bold">NEO</span>, an innovative native VLM that reimagines multi-modal integration from first principles. Unlike typical modular designs, <span class="ltx_text ltx_font_bold">NEO</span> rests on unified primitives that natively encode, align, and reason across modalities, forming coherent pixel–word correspondences from the outset.
Through streamlined end-to-end training on 390M image–text samples, <span class="ltx_text ltx_font_bold">NEO</span> acquires strong visual perception and rivals leading modular VLMs of comparable scale across diverse benchmarks.
Beyond competitive results, <span class="ltx_text ltx_font_bold">NEO</span> offers reusable components that simplify subsequent development and reduce barriers to promoting native exploration.
This reveals that next-generation multimodal systems could also originate from architectures that are native, unified, and intrinsically multimodal.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Modular Vision-Language Models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p">Current Vision–Language Models (VLMs) have converged on a modular paradigm, where a pretrained Vision Encoder (VE) is paired with a Large Language Model (LLM) via lightweight adapters, <span class="ltx_text ltx_font_italic">e</span>.<span class="ltx_text ltx_font_italic">g</span>., projection layers <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib52" title="">2024a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib53" title="">b</a>)</cite> or cross-attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib1" title="">2022</a>); Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib21" title="">2024</a>)</cite>. This architecture underlies both leading proprietary systems, including Claude <cite class="ltx_cite ltx_citemacro_cite">Anthropic (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib3" title="">2024</a>; <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib2" title="">2025</a>)</cite>, GPT <cite class="ltx_cite ltx_citemacro_cite">Hurst et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib40" title="">2024</a>); Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib114" title="">2023</a>); OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib83" title="">2025</a>)</cite>, and Gemini <cite class="ltx_cite ltx_citemacro_cite">Comanici et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib20" title="">2025</a>); DeepMind (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib24" title="">2025</a>)</cite>, as well as prominent open-source frameworks such as InternVL <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib125" title="">2025</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib105" title="">2025b</a>)</cite>, Qwen-VL <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib104" title="">2024a</a>); Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib4" title="">2025</a>)</cite>, and Grok <cite class="ltx_cite ltx_citemacro_cite">xAI (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib109" title="">2024</a>; <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib110" title="">2025</a>)</cite>.
By harnessing the complementary strengths of vision and language components, modular architectures, adhering to the “ViT-MLP-LLM” pipeline, achieve unprecedented performance across diverse multimodal benchmarks and have emerged as the dominant design principle in the field.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p">Despite empirical successes, modular VLMs remain constrained by multi-stage training and heterogeneous structures. Extensive post-training interventions are often required to mitigate rigid inductive biases in pretrained VEs <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib104" title="">2024a</a>)</cite>, which limit resolution flexibility, erode fine-grained details, and blunt sensitivity to features across scales. Besides, pretraining semantic biases and capacity trade-offs between VEs and LLMs collectively impede design simplicity, deployment efficiency, and seamless integration of vision and language, underscoring the urgent need for a monolithic backbone.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Native Vision-Language Models</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p">Native VLMs embrace early-fusion integration rather than grafting VEs onto LLMs. Early Fuyu <cite class="ltx_cite ltx_citemacro_cite">Bavishi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib6" title="">2023</a>)</cite>, EVE <cite class="ltx_cite ltx_citemacro_cite">Diao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib28" title="">2024</a>)</cite>, and SOLO <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib15" title="">2024d</a>)</cite>, embed image patches via linear projections, whereas Chameleon <cite class="ltx_cite ltx_citemacro_cite">Team (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib97" title="">2024</a>)</cite>, MoMA <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib61" title="">2024</a>)</cite>, and MoT <cite class="ltx_cite ltx_citemacro_cite">Liang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib59" title="">2024</a>)</cite> transform images into symbolic sequences via discrete tokenizers.
Later studies <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib75" title="">2024</a>); Diao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib29" title="">2025</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib56" title="">2025b</a>); Luo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib76" title="">2025</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib55" title="">2025a</a>)</cite> leverage Mixture-of-Experts (MoE) or Divide-and-Conquer (DaC) strategies to suppress vision-language interference, while others <cite class="ltx_cite ltx_citemacro_cite">Diao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib28" title="">2024</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib56" title="">2025b</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib102" title="">2025a</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib55" title="">2025a</a>)</cite> upgrade visual encoder supervision to accelerate the acquisition of visual concepts.
Empirical evidence <cite class="ltx_cite ltx_citemacro_cite">Beyer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib7" title="">2024</a>); Luo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib75" title="">2024</a>); Lei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib50" title="">2025</a>)</cite> reveals that, with sufficient data and progressive training, native VLMs rapidly approach modular counterparts, corroborating recent scaling-law insights <cite class="ltx_cite ltx_citemacro_cite">Shukor et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib91" title="">2025b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib90" title="">a</a>)</cite>.
Besides, recent methods <cite class="ltx_cite ltx_citemacro_cite">Tao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib95" title="">2025</a>); Yan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib112" title="">2025</a>); Xiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib111" title="">2025</a>)</cite> indicate that multi-modality encoding modules with the LLM or VE style slightly resolve vision-language misalignment, yet fail to fully integrate the distinct properties of each modality.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="227" id="S2.F3.g1" src="x3.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Overview of our native primitive, which integrates native attention with bi-directional dependencies within images and word / frame-wise causal interactions, together with native rotary position embeddings parameterized by modality-specific frequency, channel, and index allocation. It is inherently unified and intrinsically multimodal, substantially enhancing pixel–word correspondence.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p">Notably, NEO redefines native VLMs as a unibody system built from first-principle primitives. Every component—from image patch encoding, attention mechanism to rotary position embeddings—ensures full compatibility with the intrinsic modeling patterns of VEs and LLMs. Meanwhile, NEO evolves modular VLM strengths via the modality-agnostic pre-Buffer and end-to-end training, dramatically enhancing pixel-word alignment and pushing the frontier of native VLM research.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Architecture</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p">Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the proposed NEO framework, which comprises lightweight patch and word embedding layers, a pre-Buffer, and a post-LLM, built upon stacked native VLM primitives.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Patch and Word Embeddings.</span>
Given an image <math alttext="\bm{\mathrm{I}}" class="ltx_Math" display="inline" id="S3.SS1.p2.m1" intent=":literal"><semantics><mi>𝐈</mi><annotation encoding="application/x-tex">\bm{\mathrm{I}}</annotation></semantics></math>, we convert it into token sequences via a lightweight Patch Embedding Layer (PEL) with two Convolutional layers (Conv1–2) <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib47" title="">2012</a>)</cite> and a Gaussian Error Linear Unit (GELU) <cite class="ltx_cite ltx_citemacro_cite">Hendrycks &amp; Gimpel (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib38" title="">2016</a>)</cite>. For input text <math alttext="\bm{\mathrm{T}}" class="ltx_Math" display="inline" id="S3.SS1.p2.m2" intent=":literal"><semantics><mi>𝐓</mi><annotation encoding="application/x-tex">\bm{\mathrm{T}}</annotation></semantics></math>, we encode it into word tokens using the original LLM Tokenizer as Word Embedding Layer (WEL):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{x_{v}}=\text{Conv2}(\text{GELU}(\text{Conv1}(\bm{\mathrm{I}}))+\bm{\mathrm{PE}}),\quad\bm{x_{t}}=\text{Tokenizer}(\bm{\mathrm{T}})," class="ltx_Math" display="block" id="S3.E1.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>𝒙</mi><mi>𝒗</mi></msub><mo>=</mo><mrow><mtext>Conv2</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mtext>GELU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mtext>Conv1</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐈</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>𝐏𝐄</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mi>𝒙</mi><mi>𝒕</mi></msub><mo>=</mo><mrow><mtext>Tokenizer</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>𝐓</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\bm{x_{v}}=\text{Conv2}(\text{GELU}(\text{Conv1}(\bm{\mathrm{I}}))+\bm{\mathrm{PE}}),\quad\bm{x_{t}}=\text{Tokenizer}(\bm{\mathrm{T}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\bm{x_{v}}\in\mathbb{R}^{(h\times w)\times d}" class="ltx_Math" display="inline" id="S3.SS1.p2.m3" intent=":literal"><semantics><mrow><msub><mi>𝒙</mi><mi>𝒗</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>h</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>w</mi></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{x_{v}}\in\mathbb{R}^{(h\times w)\times d}</annotation></semantics></math> / <math alttext="\bm{x_{t}}\in\mathbb{R}^{n\times d}" class="ltx_Math" display="inline" id="S3.SS1.p2.m4" intent=":literal"><semantics><mrow><msub><mi>𝒙</mi><mi>𝒕</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bm{x_{t}}\in\mathbb{R}^{n\times d}</annotation></semantics></math> denote visual / textual tokens, and <math alttext="\bm{\mathrm{PE}}" class="ltx_Math" display="inline" id="S3.SS1.p2.m5" intent=":literal"><semantics><mi>𝐏𝐄</mi><annotation encoding="application/x-tex">\bm{\mathrm{PE}}</annotation></semantics></math> is 2D Sinusoidal Positional Encoding <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib31" title="">2021</a>)</cite>. The stride of Conv1 / Conv2 is 16 / 2, <span class="ltx_text ltx_font_italic">i</span>.<span class="ltx_text ltx_font_italic">e</span>., each visual token corresponds to a <math alttext="32\times 32" class="ltx_Math" display="inline" id="S3.SS1.p2.m6" intent=":literal"><semantics><mrow><mn>32</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">32\times 32</annotation></semantics></math> image patch.
Notably, Conv2 performs token folding like pixel unshuffle <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib16" title="">2024e</a>)</cite>, with the special <span class="ltx_text ltx_font_typewriter">&lt;img&gt;</span>
and <span class="ltx_text ltx_font_typewriter">&lt;/img&gt;</span> tokens inserted at the boundaries of visual tokens, while mapping position and patch embeddings into a unified space.
Afterward, visual and textual tokens are merged and propagated through the unified backbone.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Native VLM Primitive.</span>
It adopts RMSNorm <cite class="ltx_cite ltx_citemacro_cite">Zhang &amp; Sennrich (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib122" title="">2019</a>)</cite> and SwiGLU <cite class="ltx_cite ltx_citemacro_cite">Dauphin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib23" title="">2017</a>)</cite> consistent with the original LLM layers.
Unlike prior methods <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib104" title="">2024a</a>); Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib4" title="">2025</a>); Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib125" title="">2025</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib105" title="">2025b</a>)</cite> that collapse visual tokens into 1D representations or merely reallocate pre-trained LLM head dimensions across temporal (T), height (H), and width (W), we expand Query (Q) and Key (K) head dimensions while fully decoupling H, W, and T relations in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S2.F3" title="Figure 3 ‣ 2.2 Native Vision-Language Models ‣ 2 Related Works ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">3</span></a>(1), causing an extra 10% parameter counts over the original Transformer block. The original T dimension is preserved, and additional H and W dimensions are added with their respective QK normalization <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib113" title="">2025</a>)</cite>. Crucially, the linear weights of K for H and W channels are zero-initialized, and attention scale matches that of LLMs, maintaining the LLM pre-training paradigm and progressively activating multimodal capabilities for visual spatial relationships.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p">This philosophy aligns with our <span class="ltx_text ltx_font_bold">Native Rotary Position Embedding (Native-RoPE)</span> in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S2.F3" title="Figure 3 ‣ 2.2 Native Vision-Language Models ‣ 2 Related Works ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">3</span></a>(2), which eliminates correlations between H / W and T indexes, while decoupling channel allocations of H, W, and T under the original LLM frequency.
<span class="ltx_text ltx_font_italic">(1) Index Allocation</span>:
For text, T index is retained while H / W indexes are zeroed. Notably, Native-RoPE is equivalent to 1D-RoPE before training. For images, each visual token has a constant T index, with unique H / W indexes encoding spatial location. Videos, treated as sequences of frames, increment T index per frame, while H / W indexes follow the same spatial scheme as images. In multimodal inputs, each modality’s T index starts from the maximum ID of the preceding modality, ensuring continuous and unambiguous positional encoding across modalities.
This serves two purposes:
(a) For image pairs, H / W indexes start independently from (0,0), and tokens at corresponding positions share identical dependency, strongly reinforcing correlations and interactions across matching regions <cite class="ltx_cite ltx_citemacro_cite">Liao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib60" title="">2025</a>); Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib108" title="">2025</a>)</cite>;
(b) For image-text pairs, H / W indexes are decoupled from T index and bounded within (0,0) and (H,W), preventing large T index growth from disproportionately affecting H / W indexes <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib104" title="">2024a</a>); Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib4" title="">2025</a>)</cite> and thereby keeping spatial dependencies between long-range text and images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p">Another key aspect is <span class="ltx_text ltx_font_italic">(2) Channel and Frequency Allocation</span>. Unlike recent 3D-RoPE methods <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib4" title="">2025</a>); Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib107" title="">2025</a>); Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib118" title="">2025</a>); Liao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib60" title="">2025</a>)</cite>, we fully decouple the channel and frequency allocation of H, W, and T, equipped with additional Q/K head dimensions for H and W.
This resolves two issues:
(a) Zeroing H / W indexes for pure text would disrupt the modeling patterns and linguistic capacity of the LLM if restricted to its original channels. Repairing this disruption requires substantial resources;
(b) Even with interleaved or segmented reallocation, H and W are theoretically equivalent but are assigned different frequencies.
Meanwhile, the RoPE frequency in LLMs is far lower than that of visual encoders in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S2.F3" title="Figure 3 ‣ 2.2 Native Vision-Language Models ‣ 2 Related Works ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">3</span></a>(2).
This mismatch limits the modeling of relative distances and local semantics.
The problem is exacerbated by the disparity in scales, with temporal ranges spanning up to one million and spatial ranges only a few hundred.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p">Specifically, Native-RoPE assigns distinct base frequencies to T, H, and W within their own dimensions, <span class="ltx_text ltx_font_italic">i</span>.<span class="ltx_text ltx_font_italic">e</span>., original LLM head dimension for T and new head dimension for H / W as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Theta_{T}=\left\{\beta^{-\frac{2k}{d}}_{T}\mid k\in[0,\frac{d}{2})\right\},\;\Theta_{H}=\left\{\beta^{-\frac{4i}{d}}_{H}\mid i\in[0,\frac{d}{4})\right\},\;\Theta_{W}=\left\{\beta^{-\frac{4j}{d}}_{W}\mid j\in[0,\frac{d}{4})\right\}" class="ltx_Math" display="block" id="S3.E2.m1" intent=":literal"><semantics><mrow><mrow><msub><mi mathvariant="normal">Θ</mi><mi>T</mi></msub><mo>=</mo><mrow><mo>{</mo><msubsup><mi>β</mi><mi>T</mi><mrow><mo>−</mo><mfrac><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>k</mi></mrow><mi>d</mi></mfrac></mrow></msubsup><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><mi>k</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mfrac><mi>d</mi><mn>2</mn></mfrac><mo stretchy="false">)</mo></mrow></mrow><mo>}</mo></mrow></mrow><mo rspace="0.447em">,</mo><mrow><mrow><msub><mi mathvariant="normal">Θ</mi><mi>H</mi></msub><mo>=</mo><mrow><mo>{</mo><msubsup><mi>β</mi><mi>H</mi><mrow><mo>−</mo><mfrac><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>i</mi></mrow><mi>d</mi></mfrac></mrow></msubsup><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><mi>i</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mfrac><mi>d</mi><mn>4</mn></mfrac><mo stretchy="false">)</mo></mrow></mrow><mo>}</mo></mrow></mrow><mo rspace="0.447em">,</mo><mrow><msub><mi mathvariant="normal">Θ</mi><mi>W</mi></msub><mo>=</mo><mrow><mo>{</mo><msubsup><mi>β</mi><mi>W</mi><mrow><mo>−</mo><mfrac><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mi>d</mi></mfrac></mrow></msubsup><mo fence="true" lspace="0em" rspace="0em">∣</mo><mrow><mi>j</mi><mo>∈</mo><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mfrac><mi>d</mi><mn>4</mn></mfrac><mo stretchy="false">)</mo></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\Theta_{T}=\left\{\beta^{-\frac{2k}{d}}_{T}\mid k\in[0,\frac{d}{2})\right\},\;\Theta_{H}=\left\{\beta^{-\frac{4i}{d}}_{H}\mid i\in[0,\frac{d}{4})\right\},\;\Theta_{W}=\left\{\beta^{-\frac{4j}{d}}_{W}\mid j\in[0,\frac{d}{4})\right\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\beta" class="ltx_Math" display="inline" id="S3.SS1.p6.m1" intent=":literal"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> and <math alttext="\Theta" class="ltx_Math" display="inline" id="S3.SS1.p6.m2" intent=":literal"><semantics><mi mathvariant="normal">Θ</mi><annotation encoding="application/x-tex">\Theta</annotation></semantics></math> indicate the base and rotation frequency across H, W, and T.
Notably, temporal T dimension captures both local and long-range relations, whereas spatial H / W dimensions emphasize local dependencies.
This also opens avenues for broader applications, <span class="ltx_text ltx_font_italic">e</span>.<span class="ltx_text ltx_font_italic">g</span>., video understanding <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib107" title="">2025</a>)</cite>, multimodal generation <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib27" title="">2025b</a>)</cite>, and editing <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib26" title="">2025a</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p7">
<p class="ltx_p">Inspired by prior works <cite class="ltx_cite ltx_citemacro_cite">Lei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib50" title="">2025</a>); Deng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib27" title="">2025b</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib55" title="">2025a</a>)</cite>, we treat one single image as a unified meta-unit for autoregressive modeling.
To enable this, we propose <span class="ltx_text ltx_font_bold">Native Multi-Modal Attention</span> with mixed masking in Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S2.F3" title="Figure 3 ‣ 2.2 Native Vision-Language Models ‣ 2 Related Works ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">3</span></a>(c).
Text tokens adhere to standard causal attention, attending only to preceding tokens to maintain autoregressive generation.
In contrast, image tokens employ full bidirectional attention, enabling exhaustive interactions among all visual tokens, akin to a visual encoder. This design captures rich spatial and contextual dependencies within images and facilitates vision-language correspondences, thereby supporting complex multimodal reasoning.
We use FlexAttention <cite class="ltx_cite ltx_citemacro_cite">Dong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib30" title="">2024</a>)</cite> to minimize memory overhead and increase throughput, as variable-length block-wise attention is fully optimized through CUDA kernel modifications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p8">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Pre-Buffer and Post-LLM.</span>
Drawing on modular designs <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib4" title="">2025</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib105" title="">2025b</a>)</cite>, we split NEO into encoding and reasoning components at the outset.
In contrast, we build a modality-shared pre-Buffer via native primitives to map vision and language into a unified representation space. We further design a post-LLM via native primitives to absorb the powerful language proficiency and reasoning capabilities of LLMs. This, in turn, promotes deep pixel-word integration within the pre-Buffer—a deliberate design choice to ensure rich multimodal alignment while minimizing disturbance to the LLM.
The layer depth in the pre-Buffer and post-LLM primarily refers to the model parameters of existing VEs and LLMs, ensuring a relatively fair comparison while balancing accuracy and efficiency.
Crucially, this separation exists only during pre-training to boost visual learning; during mid-training and supervised fine-tuning, the components are upgraded to a monolithic backbone, allowing the VLM to automatically allocate capacity for encoding, alignment, and reasoning.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="142" id="S3.F4.g1" src="x4.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Overview of the entire training recipe. During pre-training, NEO learns visual perception from massive web-scale and synthetic image-caption pairs with frozen LLM weights to preserve linguistic knowledge. In mid-training and supervised fine-tuning, the full model is progressively optimized end-to-end using caption, conversation, OCR, detection, and high-quality instruction data.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Procedure</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p">Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S3.F4" title="Figure 4 ‣ 3.1 Model Architecture ‣ 3 Methodology ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the whole training pipeline, which proceeds through three progressive stages: pre-training, mid-training, and supervised fine-tuning. The entire model is optimized end-to-end.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Pre-Training Stage.</span>
In this phase, NEO acquires fundamental visual concepts and contextual dependencies from scratch, guided by pre-trained patterns from LLMs. Training leverages 345M web-scale and synthetic image-caption pairs, including 100M English and 20M Chinese pairs from LAION-400M <cite class="ltx_cite ltx_citemacro_cite">Schuhmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib85" title="">2021</a>)</cite>, 150M English pairs from COYO-700M <cite class="ltx_cite ltx_citemacro_cite">Byeon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib9" title="">2022</a>)</cite>, 20M long-caption examples from BLIP3o <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib12" title="">2025</a>)</cite>, and 5M short-caption pairs from OpenImages <cite class="ltx_cite ltx_citemacro_cite">Kuznetsova et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib48" title="">2018</a>)</cite>, recaptioned with a pre-trained InternVL2-8B model. The dataset is further enriched with 30M samples from LAION-COCO <cite class="ltx_cite ltx_citemacro_cite">Schuhmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib86" title="">2022</a>)</cite> and 20M examples from Wukong <cite class="ltx_cite ltx_citemacro_cite">Gu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib35" title="">2022</a>)</cite> with rich Optical Character
Recognition (OCR) annotations.
A 3:7 ratio of language to multi-modal data is incorporated to reconstruct text projections in the pre-Buffer.
Only the patch embedding layer, the pre-Buffer, and additional QK linear weights and normalization, along with H and W, are trainable and optimized with a simple next-token prediction objective.
Notably, the new QK heads not only counteract the LLM’s strong language bias that limits visual specialization but also safeguard its capabilities against the effects of low-quality data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Mid-Training Stage.</span>
The objective at this stage is to strengthen the alignment between visual and linguistic capabilities while progressively enhancing recognition of high-resolution images, complex scenes, object scales, spatial grounding, and compact OCR content.
The training data is drawn from the pre-training corpus of InternVL-1.5 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib17" title="">2024f</a>)</cite>, comprising 40M samples across image captioning, conversation, detection, and OCR data, which account for approximately 66%, 11%, 8%, and 15% of the total, respectively.
A 3:7 ratio of language to multi-modal data is again applied.
The entire architecture is updated with the same loss functions to consolidate vision-language alignment, thereby equipping NEO with the foundational abilities required for various visual scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Supervised Fine-Tuning Stage.</span>
During the SFT stage, NEO’s ability to follow complex linguistic instructions and varied dialogue patterns is further enhanced, a critical step towards real-world deployment. The full network is optimized across diverse high-quality, multi-source instruction datasets. Following Mono-InternVL <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib75" title="">2024</a>)</cite>, we employ about 4M bilingual instructions for supervised learning, covering tasks such as visual question answering, multimodal dialogue, mathematics, and knowledge reasoning. Details of the instruction data are provided in the Appendix.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Training Settings</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p">Our NEO models are built on Qwen3-1.7B and Qwen3-8B <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib113" title="">2025</a>)</cite> as the LLMs. The pre-Buffer employs <math alttext="L_{1}=" class="ltx_Math" display="inline" id="S4.SS1.p1.m1" intent=":literal"><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">L_{1}=</annotation></semantics></math> 12 primitive layers for NEO-2.2B and <math alttext="L_{1}=" class="ltx_Math" display="inline" id="S4.SS1.p1.m2" intent=":literal"><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">L_{1}=</annotation></semantics></math> 6 for NEO-9B. We extend only the QK head dimension in raw transformer layers, introducing roughly 10% extra parameters over the original design. The base RoPE frequencies <math alttext="\beta_{T}" class="ltx_Math" display="inline" id="S4.SS1.p1.m3" intent=":literal"><semantics><msub><mi>β</mi><mi>T</mi></msub><annotation encoding="application/x-tex">\beta_{T}</annotation></semantics></math>, <math alttext="\beta_{H}" class="ltx_Math" display="inline" id="S4.SS1.p1.m4" intent=":literal"><semantics><msub><mi>β</mi><mi>H</mi></msub><annotation encoding="application/x-tex">\beta_{H}</annotation></semantics></math>, and <math alttext="\beta_{W}" class="ltx_Math" display="inline" id="S4.SS1.p1.m5" intent=":literal"><semantics><msub><mi>β</mi><mi>W</mi></msub><annotation encoding="application/x-tex">\beta_{W}</annotation></semantics></math> are set to <math alttext="1\times 10^{6}" class="ltx_Math" display="inline" id="S4.SS1.p1.m6" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mn>6</mn></msup></mrow><annotation encoding="application/x-tex">1\times 10^{6}</annotation></semantics></math>, <math alttext="1\times 10^{4}" class="ltx_Math" display="inline" id="S4.SS1.p1.m7" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">1\times 10^{4}</annotation></semantics></math>, and <math alttext="1\times 10^{4}" class="ltx_Math" display="inline" id="S4.SS1.p1.m8" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">1\times 10^{4}</annotation></semantics></math>, respectively.
NEO is trained on sixteen 8-GPU (80G) nodes using the AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">Loshchilov &amp; Hutter (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib71" title="">2019</a>)</cite>. The maximum learning rates for pre-training, mid-training, and SFT are <math alttext="8\times 10^{-4}" class="ltx_Math" display="inline" id="S4.SS1.p1.m9" intent=":literal"><semantics><mrow><mn>8</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">8\times 10^{-4}</annotation></semantics></math>, <math alttext="4\times 10^{-5}" class="ltx_Math" display="inline" id="S4.SS1.p1.m10" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">4\times 10^{-5}</annotation></semantics></math>, and <math alttext="5\times 10^{-5}" class="ltx_Math" display="inline" id="S4.SS1.p1.m11" intent=":literal"><semantics><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\times 10^{-5}</annotation></semantics></math>, with a warm-up ratio of 0.01 and a cosine decay scheduler across all stages.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold">
Comparison with modular and native VLMs on general vision-language benchmarks.</span>
“# Data” denotes the dataset scale during pre-training, mid-training, and supervised fine-tuning.
<sup class="ltx_sup">†</sup> indicates models that employ reinforcement learning (RL).
<span class="ltx_text ltx_font_bold">Bold</span> highlights the highest performance.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:289.8pt;vertical-align:-142.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.6pt,36.5pt) scale(0.798927405914674,0.798927405914674) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">LLM</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold"># Data</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">MMMU</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">MMB</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">MMVet</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">MMStar</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">SEED-I</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">POPE</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">HallB</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E8E8E8;">
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" colspan="10" style="padding:0.75pt 1.5pt;">
<math alttext="\blacktriangledown" class="ltx_Math" display="inline" id="S4.T1.m3" intent=":literal" style="--ltx-bg-color:#E8E8E8;"><semantics><mi mathbackground="#E8E8E8" mathvariant="normal" style="--ltx-bg-color:#E8E8E8;">▼</mi><annotation encoding="application/x-tex">\blacktriangledown</annotation></semantics></math><span class="ltx_text" style="--ltx-bg-color:#E8E8E8;"> <em class="ltx_emph ltx_font_italic">Modular Vision-Language Models (2B)</em></span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Qwen2-VL</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Qwen2-1.5B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">– / – / –</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">41.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">74.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">49.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">48.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">41.7</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">InternVL2.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">InternLM2.5-1.8B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">
<span class="ltx_text ltx_font_typewriter">&gt;</span>6B /  100M / 16M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">43.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">74.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">60.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">53.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">90.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">42.6</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Qwen2.5-VL<sup class="ltx_sup">†</sup>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Qwen2.5-1.5B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">– / – / –</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">51.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">79.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">61.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">55.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">46.3</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">InternVL3<sup class="ltx_sup">†</sup>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Qwen2.5-1.5B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">
<span class="ltx_text ltx_font_typewriter">&gt;</span>6B /  100M / 22M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">48.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">81.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">62.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">60.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">89.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">42.5</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#DCEBDC;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">Encoder-Based</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">Qwen3-1.7B</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_typewriter" style="--ltx-bg-color:#DCEBDC;">&gt;<span class="ltx_text ltx_font_serif">6B / 40M / 4M</span></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">47.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">75.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">37.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">52.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBDC;">73.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">87.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">44.4</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E8E8E8;">
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" colspan="10" style="padding:0.75pt 1.5pt;">
<math alttext="\blacktriangledown" class="ltx_Math" display="inline" id="S4.T1.m6" intent=":literal" style="--ltx-bg-color:#E8E8E8;"><semantics><mi mathbackground="#E8E8E8" mathvariant="normal" style="--ltx-bg-color:#E8E8E8;">▼</mi><annotation encoding="application/x-tex">\blacktriangledown</annotation></semantics></math><span class="ltx_text" style="--ltx-bg-color:#E8E8E8;"> <em class="ltx_emph ltx_font_italic">Native Vision-Language Models (2B)</em></span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Mono-InternVL</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">InternLM2-1.8B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1.2B / 143M / 7M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">33.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">65.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">40.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">67.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">34.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Mono-InternVL-1.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">InternLM2-1.8B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">400M / 150M / 7M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">39.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">64.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">54.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">66.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">32.5</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">HoVLE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">InternLM2-1.8B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">550M / 50M / 7M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">32.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">73.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">43.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">70.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">87.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">38.4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">OneCAT</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Qwen2.5-1.5B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">436M / 70M / 13M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">39.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">72.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">42.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">70.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#DCEBF5;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">NEO</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">Qwen3-1.7B</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">345M / 40M / 4M</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">48.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">76.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">49.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">54.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">74.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">87.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">43.1</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E8E8E8;">
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_tt" colspan="10" style="padding:0.75pt 1.5pt;">
<math alttext="\blacktriangledown" class="ltx_Math" display="inline" id="S4.T1.m7" intent=":literal" style="--ltx-bg-color:#E8E8E8;"><semantics><mi mathbackground="#E8E8E8" mathvariant="normal" style="--ltx-bg-color:#E8E8E8;">▼</mi><annotation encoding="application/x-tex">\blacktriangledown</annotation></semantics></math><span class="ltx_text" style="--ltx-bg-color:#E8E8E8;"> <em class="ltx_emph ltx_font_italic">Modular Vision-Language Models (8B)</em></span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Qwen2-VL</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Qwen2-7B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">– / – / –</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">54.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">83</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">62.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">60.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">88.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">50.6</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">InternVL2.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">InternLM2.5-7B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">
<span class="ltx_text ltx_font_typewriter">&gt;</span>6B /  50M / 4M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">56.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">84.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">62.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">64.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">90.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">50.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Qwen2.5-VL<sup class="ltx_sup">†</sup>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Qwen2.5-7B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">– / – / –</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">55.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">83.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">67.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">63.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">86.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">52.9</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">InternVL3<sup class="ltx_sup">†</sup>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Qwen2.5-7B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">
<span class="ltx_text ltx_font_typewriter">&gt;</span>6B /  100M / 22M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">62.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">83.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">81.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">68.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">91.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">49.9</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#DCEBDC;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">Encoder-Based</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">Qwen3-8B</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_typewriter" style="--ltx-bg-color:#DCEBDC;">&gt;<span class="ltx_text ltx_font_serif">6B / 40M / 4M</span></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">54.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">84</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">60.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">63.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBDC;">76.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">87.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">51.4</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E8E8E8;">
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" colspan="10" style="padding:0.75pt 1.5pt;">
<math alttext="\blacktriangledown" class="ltx_Math" display="inline" id="S4.T1.m10" intent=":literal" style="--ltx-bg-color:#E8E8E8;"><semantics><mi mathbackground="#E8E8E8" mathvariant="normal" style="--ltx-bg-color:#E8E8E8;">▼</mi><annotation encoding="application/x-tex">\blacktriangledown</annotation></semantics></math><span class="ltx_text" style="--ltx-bg-color:#E8E8E8;"> <em class="ltx_emph ltx_font_italic">Native Vision-Language Models (8B)</em></span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Fuyu</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Persimmon-8B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">– / – / –</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">27.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">10.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">21.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">59.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">84.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Chameleon</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">from scratch</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1.4B / 0M / 1.8M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">25.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">31.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">8.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">30.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">19.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">17.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">EVE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Vicuna-7B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">33M / 0M / 1.8M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">32.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">52.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">25.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">64.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">85.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">26.4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">SOLO</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Mistral-7B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">44M / 0M / 2M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">67.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">30.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">64.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">78.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Emu3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">from scratch</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">– / – / –</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">31.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">58.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">37.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">68.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">85.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">EVEv2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Qwen2.5-7B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">77M / 15M / 7M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">39.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">66.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">45.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">71.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">87.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">BREEN</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Qwen2.5-7B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">13M / 0M / 4M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">42.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">71.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">38.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">51.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">37.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">VoRA</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Qwen2.5-7B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">30M / 0M / 0.6M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">32.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">61.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">33.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">68.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">85.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">SAIL</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Mistral-7B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">512M / 86M / 6M</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">70.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">46.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">53.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">72.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">85.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">54.2</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#DCEBF5;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">NEO</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">Qwen3-8B</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">345M / 40M / 4M</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">54.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">82.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">53.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">62.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">76.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">88.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">46.4</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p">We conduct standard evaluations with VLMEvalKit <cite class="ltx_cite ltx_citemacro_cite">Duan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib32" title="">2024</a>)</cite> on diverse benchmarks, covering chart, diagram, and document understanding tasks, <span class="ltx_text ltx_font_italic">e</span>.<span class="ltx_text ltx_font_italic">g</span>., AI2D <cite class="ltx_cite ltx_citemacro_cite">Kembhavi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib43" title="">2016</a>)</cite>, DocVQA <cite class="ltx_cite ltx_citemacro_cite">Clark &amp; Gardner (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib19" title="">2018</a>)</cite>, ChartQA <cite class="ltx_cite ltx_citemacro_cite">Masry et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib79" title="">2022</a>)</cite>, InfoVQA <cite class="ltx_cite ltx_citemacro_cite">Mathew et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib80" title="">2022</a>)</cite>, TextVQA <cite class="ltx_cite ltx_citemacro_cite">Singh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib93" title="">2019</a>)</cite>, and OCRBench <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib70" title="">2023e</a>)</cite>;
visual perception and challenging reasoning tasks, <span class="ltx_text ltx_font_italic">e</span>.<span class="ltx_text ltx_font_italic">g</span>., MMMU <cite class="ltx_cite ltx_citemacro_cite">Yue et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib120" title="">2024</a>)</cite>, MMBench-EN (MMB) <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib69" title="">2024b</a>)</cite>, MMVet <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib117" title="">2024</a>)</cite>, MMStar <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib14" title="">2024c</a>)</cite>, SEEDBench-IMG (SEED-I) <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib54" title="">2023a</a>)</cite>;
visual hallucination tasks, <span class="ltx_text ltx_font_italic">e</span>.<span class="ltx_text ltx_font_italic">g</span>., POPE <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib57" title="">2023b</a>)</cite>
and HallusionBench (HallB) <cite class="ltx_cite ltx_citemacro_cite">Guan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib36" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold">Comparison with modular and native VLMs on visual question answering benchmarks.</span>
Any Res., Tile-wise, Any Rat., and Fix Res. refer to any resolution, image tile splitting, any aspect ratio, and fixed resolution.
MoE and DaC are Mixture-of-Experts and Divide-and-Conquer models.
</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:291pt;vertical-align:-143.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-53.5pt,35.9pt) scale(0.801960485066422,0.801960485066422) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">Input</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">RoPE</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">Backbone</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">AI2D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">DocVQA</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">ChartQA</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">InfoVQA</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">TextVQA</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">OCRBench</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E8E8E8;">
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" colspan="10" style="padding:0.75pt 1.5pt;">
<math alttext="\blacktriangledown" class="ltx_Math" display="inline" id="S4.T2.m1" intent=":literal" style="--ltx-bg-color:#E8E8E8;"><semantics><mi mathbackground="#E8E8E8" mathvariant="normal" style="--ltx-bg-color:#E8E8E8;">▼</mi><annotation encoding="application/x-tex">\blacktriangledown</annotation></semantics></math><span class="ltx_text" style="--ltx-bg-color:#E8E8E8;"> <em class="ltx_emph ltx_font_italic">Modular Vision-Language Models (2B)</em></span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Qwen2-VL</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">M-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">74.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">90.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">73.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">65.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">79.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">80.9</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">InternVL2.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Tile-wise</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">74.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">88.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">79.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">60.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">74.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">80.4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Qwen2.5-VL<sup class="ltx_sup">†</sup>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">M-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">81.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">93.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">84.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">77.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">79.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">79.7</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">InternVL3<sup class="ltx_sup">†</sup>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Tile-wise</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">78.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">88.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">80.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">66.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">77.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">83.5</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#DCEBDC;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">Encoder-Based</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">Tile-wise</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">1D-RoPE</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">Dense</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">77.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">89.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">78.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">65.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">73.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBDC;">83.5</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E8E8E8;">
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" colspan="10" style="padding:0.75pt 1.5pt;">
<math alttext="\blacktriangledown" class="ltx_Math" display="inline" id="S4.T2.m4" intent=":literal" style="--ltx-bg-color:#E8E8E8;"><semantics><mi mathbackground="#E8E8E8" mathvariant="normal" style="--ltx-bg-color:#E8E8E8;">▼</mi><annotation encoding="application/x-tex">\blacktriangledown</annotation></semantics></math><span class="ltx_text" style="--ltx-bg-color:#E8E8E8;"> <em class="ltx_emph ltx_font_italic">Native Vision-Language Models (2B)</em></span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Mono-InternVL</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Tile-wise.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">MoE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">68.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">80.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">73.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">43.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">72.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">76.7</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Mono-InternVL-1.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Tile-wise.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">DaC</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">67.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">81.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">72.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">47.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">73.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">80.1</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">HoVLE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Tile-wise.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">73.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">86.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">78.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">55.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">70.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">74.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">OneCAT</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">M-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">72.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">87.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">76.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">56.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">67.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#DCEBF5;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">NEO</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">Any Res.</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">Native-RoPE</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">Dense</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">80.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">89.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">81.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">63.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">74.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">77.1</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E8E8E8;">
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_tt" colspan="10" style="padding:0.75pt 1.5pt;">
<math alttext="\blacktriangledown" class="ltx_Math" display="inline" id="S4.T2.m5" intent=":literal" style="--ltx-bg-color:#E8E8E8;"><semantics><mi mathbackground="#E8E8E8" mathvariant="normal" style="--ltx-bg-color:#E8E8E8;">▼</mi><annotation encoding="application/x-tex">\blacktriangledown</annotation></semantics></math><span class="ltx_text" style="--ltx-bg-color:#E8E8E8;"> <em class="ltx_emph ltx_font_italic">Modular Vision-Language Models (8B)</em></span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Qwen2-VL</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">M-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">83.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">94.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">83</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">76.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">84.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">86.6</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">InternVL2.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Tile-wise</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">84.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">93.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">84.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">77.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">79.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">82.2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Qwen2.5-VL<sup class="ltx_sup">†</sup>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">M-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">83.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">95.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">87.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">82.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">84.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">86.4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">InternVL3<sup class="ltx_sup">†</sup>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Tile-wise</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">85.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">92.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">86.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">76.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">80.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">88</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#DCEBDC;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">Encoder-Based</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">Tile-wise</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">1D-RoPE</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">Dense</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">82.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">92.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">83.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">75</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">77.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBDC;">85.3</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#E8E8E8;">
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" colspan="10" style="padding:0.75pt 1.5pt;">
<math alttext="\blacktriangledown" class="ltx_Math" display="inline" id="S4.T2.m8" intent=":literal" style="--ltx-bg-color:#E8E8E8;"><semantics><mi mathbackground="#E8E8E8" mathvariant="normal" style="--ltx-bg-color:#E8E8E8;">▼</mi><annotation encoding="application/x-tex">\blacktriangledown</annotation></semantics></math><span class="ltx_text" style="--ltx-bg-color:#E8E8E8;"> <em class="ltx_emph ltx_font_italic">Native Vision-Language Models (8B)</em></span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Fuyu</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">64.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">36.6</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Chameleon</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Fix Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">46.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">2.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">5.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">4.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">0.7</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">EVE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Rat.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">61.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">53.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">59.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">25.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">56.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">39.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">SOLO</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">61.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">12.6</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">Emu3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Fix Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">70</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">76.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">68.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">43.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">64.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">68.7</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">EVEv2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Rat.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">DaC</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">74.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">73.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">71.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">70.2</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">BREEN</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">MoE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">76.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">65.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">VoRA</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">61.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">58.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">SAIL</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Any Res.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">M-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Dense</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">76.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">–</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">77.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">78.3</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#DCEBF5;">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">NEO</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">Any Res.</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">Native-RoPE</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">Dense</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">83.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">88.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">82.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold" style="--ltx-bg-color:#DCEBF5;">60.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">75.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;"><span class="ltx_text" style="--ltx-bg-color:#DCEBF5;">77.7</span></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p">Following InternVL3 <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib125" title="">2025</a>)</cite>, we construct the <span class="ltx_text ltx_font_italic">Encoder-Based</span> by combining Qwen3 <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib113" title="">2025</a>)</cite> and InternViT-300M <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib125" title="">2025</a>)</cite>. In the mid-training stage, we first train the projector on 10M samples, and further unfreeze the vision encoder utilizing another 30M samples.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Comparison with Modular VLMs.</span>
As demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.T1" title="Table 1 ‣ 4.1 Training Settings ‣ 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.T2" title="Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">2</span></a>, NEO achieves highly competitive performance at both the 2B and 8B scales, despite using relatively limited pre-training and supervised fine-tuning data and without reinforcement learning. Remarkably, NEO approaches the performance of top-tier modular VLMs, <span class="ltx_text ltx_font_italic">e</span>.<span class="ltx_text ltx_font_italic">g</span>., Qwen2-VL <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib104" title="">2024a</a>)</cite>, InternVL2.5 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib16" title="">2024e</a>)</cite>, Qwen2.5-VL <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib4" title="">2025</a>)</cite>, and InternVL3 <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib125" title="">2025</a>)</cite> across multiple benchmarks, rivaling architectures trained on billions of additional samples. These results highlight the effectiveness of our end-to-end training strategy and unified model design. By combining native attention mechanisms with Native-RoPE, NEO enhances interactions between visual and linguistic features, enabling it to match more complex modular systems despite its simpler architecture.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Comparison with Native VLMs.</span>
From Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.T1" title="Table 1 ‣ 4.1 Training Settings ‣ 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.T2" title="Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">2</span></a>, NEO delivers substantial gains on visual-centric benchmarks over the best competitors, <span class="ltx_text ltx_font_italic">e</span>.<span class="ltx_text ltx_font_italic">g</span>., Mono-InterVL <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib75" title="">2024</a>; <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib76" title="">2025</a>)</cite>, HoVLE <cite class="ltx_cite ltx_citemacro_cite">Tao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib95" title="">2025</a>)</cite>, OnCAT <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib55" title="">2025a</a>)</cite>, EVE <cite class="ltx_cite ltx_citemacro_cite">Diao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib28" title="">2024</a>; <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib29" title="">2025</a>)</cite>, Emu3 <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib106" title="">2024b</a>)</cite>, BREEN <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib56" title="">2025b</a>)</cite>, VoRA <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib102" title="">2025a</a>)</cite>, and SAIL <cite class="ltx_cite ltx_citemacro_cite">Lei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib50" title="">2025</a>)</cite>. By seamlessly integrating post-LLM components with the pre-Buffer for large-scale visual learning, NEO aligns visual inputs with textual features from scratch and supports complex visual reasoning, even without visual encoder supervision <cite class="ltx_cite ltx_citemacro_cite">Diao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib28" title="">2024</a>); Tao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib95" title="">2025</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib55" title="">2025a</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib102" title="">2025a</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib56" title="">2025b</a>)</cite>, highlighting the strengths of its native primitive designs and training strategies. These design choices allow NEO to surpass many native VLMs using fewer training resources, demonstrating the advantages of our primitives with efficient data-scaling capability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p5">
<p class="ltx_p">Despite strong results, NEO lags on knowledge-/OCR-heavy tasks, <span class="ltx_text ltx_font_italic">e</span>.<span class="ltx_text ltx_font_italic">g</span>., MMMU, InfoVQA, and TextVQA.
<span class="ltx_text ltx_font_italic">Interestingly, NEO-9B does not surpass NEO-2B on DocVQA and InfoVQA</span>, indicating limitations in our current training corpus.
Even so, NEO performs well under constraints, highlighting the native VLM as a scalable paradigm.
Larger datasets and resources can unlock its full potential.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Studies</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p">Unless otherwise specified, we report the average evaluation results, denoted as <span class="ltx_text ltx_font_bold">Avg.</span>, across ten vision-language benchmark datasets in Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.T3" title="Table 3 ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">3</span></a>. The pre-Buffer and new head dimensions in the post-LLM are trained on 20M pre-training samples, followed by full-backbone fine-tuning on 2M SFT instruction data. These constitute the standard training settings for our ablation studies.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="S4.F5.g1" src="x5.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Configurations of pre-Buffer.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Hyperparameters of the Pre-Buffer Layer.</span>
Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.F5" title="Figure 5 ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the relationship between the number of pre-Buffer layers and the model’s average accuracy, using Qwen3-1.7B as the post-LLM. Performance improves consistently as the layer count increases, but gains begin to saturate beyond eight layers. To maximize accuracy while maintaining the same capacity as publicly available vision encoders <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib17" title="">2024f</a>); Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib84" title="">2021</a>); Zhai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib121" title="">2023</a>)</cite>, we select 12 layers for NEO-2.2B.
Notably, we choose 6 layers for NEO-9B, mainly due to the good trade-off between performance and efficiency.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Configurations of attention and RoPE. MMS, CQA, IVQA, and OCRB denote MMStar, ChartQA, InfoVQA, and OCRBench. <math alttext="\star" class="ltx_Math" display="inline" id="S4.T3.m2" intent=":literal"><semantics><mo>⋆</mo><annotation encoding="application/x-tex">\star</annotation></semantics></math> indicates that the base RoPE frequencies for height and width are set to 1M. To ensure fairness, we add new head dimensions of equal size across all models.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:98.2pt;vertical-align:-47.1pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.7pt,11.7pt) scale(0.807593717871809,0.807593717871809) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">Attention</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">RoPE</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">MMMU</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">MMB</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">MMS</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">SEED-I</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">AI2D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">CQA</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">IVQA</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">TVQA</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">OCRB</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">POPE</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">Avg.</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding:0.75pt 1.5pt;">A</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">Causal</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">40.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">48.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">36.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">55.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">63.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">16.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">22.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">16.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">13.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:0.75pt 1.5pt;">78.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">39.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">B</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Mixed</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;">1D-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">40.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">48.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">36.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">57.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">63.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">16.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">21.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">17.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">16.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;">79.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">39.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">C</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Mixed</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;">IL-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">40.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">47.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">36.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">57.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">62.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">18.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">23.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">17.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">13.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;">78.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">39.5</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">D</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Mixed</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;">M-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">40.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">49.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">37.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">57.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">64.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">23.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">25.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">20.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">18.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;">79.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">41.7</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">E</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Mixed</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;">MM-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">40.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">50.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">37.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">58.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">65.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">25.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">26.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">22.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">18.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;">78.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">42.4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">F</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Mixed</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;">Video-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">40.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">51.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">37.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">58.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">64.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">27.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">26.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">23.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">21.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">81.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">43.2</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding:0.75pt 1.5pt;">G</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">Causal</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:0.75pt 1.5pt;">Native-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">40.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">49.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">36.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">57.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">63.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">19.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">23.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">19.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">16.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding:0.75pt 1.5pt;">77.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:0.75pt 1.5pt;">40.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:0.75pt 1.5pt;">H</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;">Mixed</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;">Native-RoPE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">40.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">51.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">38.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">58.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">65.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">30.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">26.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">24.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">23.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">80.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:0.75pt 1.5pt;"><span class="ltx_text ltx_font_bold">44.0</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" style="padding:0.75pt 1.5pt;">I</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;">Mixed</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding:0.75pt 1.5pt;">Native-RoPE<math alttext="\star" class="ltx_Math" display="inline" id="S4.T3.m3" intent=":literal"><semantics><mo>⋆</mo><annotation encoding="application/x-tex">\star</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;">40.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;">50.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;">36.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;">57.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;">64.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;">25.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;">25.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;">21.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;">20.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding:0.75pt 1.5pt;">78.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding:0.75pt 1.5pt;">42.0</td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F7.fig1" style="width:230.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="275" id="S4.F7.g1" src="x6.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparison with pre-Buffer and vision encoders. All models are initialized post-LLM using Qwen3-1.7B.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F7.fig2" style="width:147.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="425" id="S4.F7.g2" src="x7.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Evaluation results across three progressive training procedures.</figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Configurations of Native Primitives.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.T3" title="Table 3 ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">3</span></a> compares various attention and RoPE designs. The pre-Buffer depth is 4, and the post-LLM is initialized with Qwen3-1.7B. All models share the same new QK head dimensions and normalization.
<span class="ltx_text ltx_font_italic">(1) Attention mode</span>. Comparing models A/B and G/H reveals consistent gains of mixed attention over causal one, reflecting its stronger capacity to model comprehensive dependencies and cross-modal alignment.
<span class="ltx_text ltx_font_italic">(2) RoPE mode</span>. Native-RoPE outperforms 1D-RoPE <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib125" title="">2025</a>)</cite>, IL-RoPE <cite class="ltx_cite ltx_citemacro_cite">Liao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib60" title="">2025</a>)</cite>, M-RoPE <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib4" title="">2025</a>)</cite>, MM-RoPE <cite class="ltx_cite ltx_citemacro_cite">Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib118" title="">2025</a>)</cite>, and Video-RoPE <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib107" title="">2025</a>)</cite>, with at least a 0.8% gain. This validates the importance of disentangling height, width, and temporal components in RoPE to enhance spatial–temporal representations and fine-grained interactions. By contrast, setting the base RoPE frequency to 1M for height and width severely impairs the ability to perceive local semantics.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Comparison between Pre-Buffer and Vision Encoders.</span>
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.F7" title="Figure 7 ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">7</span></a>, PB 1–3 denotes the Pre-Buffer after stage 1–3. For all models except NEO, the post-LLMs are initialized via Qwen3-1.7B for our pre-Buffer, InternViT-300M <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib16" title="">2024e</a>)</cite>, CLIP-vit-large-patch14 <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib84" title="">2021</a>)</cite>, and SigLIP-so400m-patch14-384 <cite class="ltx_cite ltx_citemacro_cite">Zhai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib121" title="">2023</a>)</cite>. After two-stage re-training, PB3 shows only an average gap of 2.5 / 2.4 / 1.7 / 3.7% over NEO / InternViT / CLIP / SigLIP using billion-scale training data. This substantially reduces the training costs of building native VLMs for subsequent research.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Performance Gains across Stages.</span>
Figure <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.F7" title="Figure 7 ‣ 4.3 Ablation Studies ‣ 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">7</span></a> presents the result evolution across training stages. In Stages 1 and 2, the model is fine-tuned on 2M SFT examples. Performance improves consistently as training data scales increase across 2.2B and 9B model sizes. Following progressive training, NEO shows strong multimodal capabilities, enabling robust performance across diverse real-world tasks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p">We introduce NEO, a native VLM that seamlessly integrates vision and language into a single unified framework, eliminating the need for separate visual encoders or ad-hoc alignment modules. By leveraging hybrid attention and modality-aware rotary position embeddings, NEO captures rich, fine-grained interactions between pixels and words from the outset. Its pre-Buffer and post-LLM training paradigm ensures efficient convergence and robust alignment while maintaining end-to-end learning. Experiments show that this unified design not only advances multimodal understanding and reasoning but also lays the foundation for reusable, scalable components. Our native primitives highlight a promising path toward intrinsically multimodal, unified, and adaptable architectures.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p">All resources are drawn from open-access datasets with explicitly defined usage policies. Our work seeks to advance multimodal learning capabilities without introducing ethical or safety concerns beyond those already associated with existing models. Nevertheless, risks such as dataset biases and potential misuse cannot be entirely ruled out. We emphasize the importance of careful data curation, responsible deployment, and transparent reporting as essential practices to mitigate these challenges.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Reproducibility Statement</h2>
<div class="ltx_para ltx_noindent" id="Sx2.p1">
<p class="ltx_p">We place strong emphasis on reproducibility, providing detailed descriptions to facilitate replication and validation.
Information about dataset selection, training strategies, and evaluation settings is provided in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S3.SS2" title="3.2 Training Procedure ‣ 3 Methodology ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">3.2</span></a> and Sec. <a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#S4.SS1" title="4.1 Training Settings ‣ 4 Experiments ‣ From Pixels to Words – Towards Native Vision-Language Primitives at Scale"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
We commit to releasing the code, model weights, and detailed documentation to allow the community to reproduce our findings in future research.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances of Neural Information Processing Systems</em>, New Orleans, LA, USA, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2025)</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Claude 3.7 sonnet: A hybrid reasoning ai model, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/claude-3-7-sonnet" title="">https://www.anthropic.com/news/claude-3-7-sonnet</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2024)</span>
<span class="ltx_bibblock">
AI Anthropic.

</span>
<span class="ltx_bibblock">The claude 3 model family: opus, sonnet, haiku, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf" title="">https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2025)</span>
<span class="ltx_bibblock">
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin.

</span>
<span class="ltx_bibblock">Qwen2.5-vl technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2502.13923, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2024)</span>
<span class="ltx_bibblock">
Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Junting Zhou, Ziqiang Liu, Feiteng Fang, Mingshan Chang, Tianyu Zheng, Xincheng Zhang, et al.

</span>
<span class="ltx_bibblock">Coig-cqia: Quality is all you need for chinese instruction fine-tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2403.18058, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bavishi et al. (2023)</span>
<span class="ltx_bibblock">
Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar.

</span>
<span class="ltx_bibblock">Introducing our multimodal models, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.adept.ai/blog/fuyu-8b" title="">https://www.adept.ai/blog/fuyu-8b</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beyer et al. (2024)</span>
<span class="ltx_bibblock">
Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey A. Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bosnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier J. Hénaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai.

</span>
<span class="ltx_bibblock">Paligemma: a versatile 3b vlm for transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2407.07726, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biten et al. (2019)</span>
<span class="ltx_bibblock">
Ali Furkan Biten, Rubèn Tito, Andrés Mafla, Lluís Gómez i Bigorda, Marçal Rusiñol, C. V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas.

</span>
<span class="ltx_bibblock">Scene text visual question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision</em>, pp.  4290–4300, Seoul, Korea (South), 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Byeon et al. (2022)</span>
<span class="ltx_bibblock">
Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.

</span>
<span class="ltx_bibblock">Coyo-700m: Image-text pair dataset, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/kakaobrain/coyo-dataset" title="">https://github.com/kakaobrain/coyo-dataset</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao &amp; Xiao (2022)</span>
<span class="ltx_bibblock">
Jie Cao and Jing Xiao.

</span>
<span class="ltx_bibblock">An augmented benchmark dataset for geometric question answering through dual parallel text encoding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Computational Linguistics</em>, pp.  1511–1520, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024a)</span>
<span class="ltx_bibblock">
Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang.

</span>
<span class="ltx_bibblock">Allava: harnessing gpt4v-synthesized data for a lite vision-language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2402.11684, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2025)</span>
<span class="ltx_bibblock">
Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu.

</span>
<span class="ltx_bibblock">Blip3-o: A family of fully open unified multimodal models-architecture, training and dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2505.09568, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024b)</span>
<span class="ltx_bibblock">
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.

</span>
<span class="ltx_bibblock">Sharegpt4v: improving large multi-modal models with better captions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, volume 15075, pp.  370–387, Milan, Italy, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024c)</span>
<span class="ltx_bibblock">
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao.

</span>
<span class="ltx_bibblock">Are we on the right way for evaluating large vision-language models?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances of Neural Information Processing Systems</em>, Vancouver, BC, Canada, 2024c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024d)</span>
<span class="ltx_bibblock">
Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji.

</span>
<span class="ltx_bibblock">A single transformer for scalable vision-language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2407.06438, 2024d.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024e)</span>
<span class="ltx_bibblock">
Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang.

</span>
<span class="ltx_bibblock">Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2412.05271, 2024e.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024f)</span>
<span class="ltx_bibblock">
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang.

</span>
<span class="ltx_bibblock">How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2404.16821, 2024f.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chng et al. (2019)</span>
<span class="ltx_bibblock">
Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al.

</span>
<span class="ltx_bibblock">Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition</em>, pp.  1571–1576, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark &amp; Gardner (2018)</span>
<span class="ltx_bibblock">
Christopher Clark and Matt Gardner.

</span>
<span class="ltx_bibblock">Simple and effective multi-paragraph reading comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>, pp.  845–855, Melbourne, Australia, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Comanici et al. (2025)</span>
<span class="ltx_bibblock">
Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit S. Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, Nan-Jiang Jiang, Krishna Haridasan, Ahmed Omran, Nikunj Saunshi, Dara Bahri, Gaurav Mishra, Eric Chu, Toby Boyd, Brad Hekman, Aaron Parisi, Chaoyi Zhang, Kornraphop Kawintiranon, Tania Bedrax-Weiss, Oliver Wang, Ya Xu, Ollie Purkiss, Uri Mendlovic, Ilaï Deutel, Nam Nguyen, Adam Langley, Flip Korn, Lucia Rossazza, Alexandre Ramé, Sagar Waghmare, Helen Miller, Nathan Byrd, Ashrith Sheshan, Raia Hadsell Sangnie Bhardwaj, Pawel Janus, Tero Rissa, Dan Horgan, Sharon Silver, Ayzaan Wahid, Sergey Brin, Yves Raimond, Klemen Kloboves, Cindy Wang, Nitesh Bharadwaj Gundavarapu, Ilia Shumailov, Bo Wang, Mantas Pajarskas, Joe Heyward, Martin Nikoltchev, Maciej Kula, Hao Zhou, Zachary Garrett, Sushant Kafle, Sercan Arik, Ankita Goel,
Mingyao Yang, Jiho Park, Koji Kojima, Parsa Mahmoudieh, Koray Kavukcuoglu, Grace Chen, Doug Fritz, Anton Bulyenov, Sudeshna Roy, Dimitris Paparas, Hadar Shemtov, Bo-Juen Chen, Robin Strudel, David Reitter, Aurko Roy, Andrey Vlasov, Changwan Ryu, Chas Leichner, Haichuan Yang, Zelda Mariet, Denis Vnukov, Tim Sohn, Amy Stuart, Wei Liang, Minmin Chen, Praynaa Rawlani, Christy Koh, JD Co-Reyes, Guangda Lai, Praseem Banzal, Dimitrios Vytiniotis, Jieru Mei, and Mu Cai.

</span>
<span class="ltx_bibblock">Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2507.06261, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2024)</span>
<span class="ltx_bibblock">
Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping.

</span>
<span class="ltx_bibblock">NVLM: open frontier-class multimodal llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2409.11402, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. (2017)</span>
<span class="ltx_bibblock">
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M. F. Moura, Devi Parikh, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Visual dialog.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  1080–1089, Honolulu, HI, USA, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dauphin et al. (2017)</span>
<span class="ltx_bibblock">
Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier.

</span>
<span class="ltx_bibblock">Language modeling with gated convolutional networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, volume 70, pp.  933–941, Sydney, NSW, Australia, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepMind (2025)</span>
<span class="ltx_bibblock">
Google DeepMind.

</span>
<span class="ltx_bibblock">Gemini 2.5 pro: Google’s most advanced reasoning model, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://deepmind.google/models/gemini/pro/" title="">https://deepmind.google/models/gemini/pro/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI et al. (2025)</span>
<span class="ltx_bibblock">
DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge,
Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li.

</span>
<span class="ltx_bibblock">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2501.12948, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2025a)</span>
<span class="ltx_bibblock">
Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Shi Guang, and Haoqi Fan.

</span>
<span class="ltx_bibblock">Emerging properties in unified multimodal pretraining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2505.14683, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2025b)</span>
<span class="ltx_bibblock">
Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang.

</span>
<span class="ltx_bibblock">Autoregressive video generation without vector quantization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, Singapore, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diao et al. (2024)</span>
<span class="ltx_bibblock">
Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang.

</span>
<span class="ltx_bibblock">Unveiling encoder-free vision-language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2406.11832, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diao et al. (2025)</span>
<span class="ltx_bibblock">
Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, and Xinlong Wang.

</span>
<span class="ltx_bibblock">Evev2: Improved baselines for encoder-free vision-language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2502.06788, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2024)</span>
<span class="ltx_bibblock">
Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He.

</span>
<span class="ltx_bibblock">Flex attention: A programming model for generating optimized attention kernels.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2412.05496, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: transformers for image recognition at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, Austria, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan et al. (2024)</span>
<span class="ltx_bibblock">
Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen.

</span>
<span class="ltx_bibblock">Vlmevalkit: An open-source toolkit for evaluating large multi-modality models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">ACM International Conference on Multimedia</em>, pp.  11198–11201, Melbourne, VIC, Australia, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2023)</span>
<span class="ltx_bibblock">
Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao.

</span>
<span class="ltx_bibblock">EVA: exploring the limits of masked visual representation learning at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  19358–19369, Vancouver, BC, Canada, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: elevating the role of image understanding in visual question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  6325–6334, Honolulu, HI, USA, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2022)</span>
<span class="ltx_bibblock">
Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, Chunjing Xu, and Hang Xu.

</span>
<span class="ltx_bibblock">Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances of Neural Information Processing Systems</em>, New Orleans, LA, USA,, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guan et al. (2024)</span>
<span class="ltx_bibblock">
Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou.

</span>
<span class="ltx_bibblock">Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  14375–14385, Seattle, WA, USA, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin.

</span>
<span class="ltx_bibblock">Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.10755, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks &amp; Gimpel (2016)</span>
<span class="ltx_bibblock">
Dan Hendrycks and Kevin Gimpel.

</span>
<span class="ltx_bibblock">Gaussian error linear units (gelus).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/1606.08415, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson &amp; Manning (2019)</span>
<span class="ltx_bibblock">
Drew A. Hudson and Christopher D. Manning.

</span>
<span class="ltx_bibblock">GQA: a new dataset for real-world visual reasoning and compositional question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  6700–6709, Long Beach, CA, USA, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hurst et al. (2024)</span>
<span class="ltx_bibblock">
Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon
Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn.

</span>
<span class="ltx_bibblock">Gpt-4o system card.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2410.21276, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jimmycarter (2023)</span>
<span class="ltx_bibblock">
Jimmycarter.

</span>
<span class="ltx_bibblock">Textocr gpt-4v dataset, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/jimmycarter/textocr-gpt4v" title="">https://huggingface.co/datasets/jimmycarter/textocr-gpt4v</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle et al. (2018)</span>
<span class="ltx_bibblock">
Kushal Kafle, Brian L. Price, Scott Cohen, and Christopher Kanan.

</span>
<span class="ltx_bibblock">DVQA: understanding data visualizations via question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  5648–5656, Salt Lake City, UT, USA, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kembhavi et al. (2016)</span>
<span class="ltx_bibblock">
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Min Joon Seo, Hannaneh Hajishirzi, and Ali Farhadi.

</span>
<span class="ltx_bibblock">A diagram is worth a dozen images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, volume 9908, pp.  235–251, Amsterdam, The Netherlands, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kembhavi et al. (2017)</span>
<span class="ltx_bibblock">
Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  4999–5007, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2022)</span>
<span class="ltx_bibblock">
Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.

</span>
<span class="ltx_bibblock">Ocr-free document understanding transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, volume 13688, pp.  498–517, Tel Aviv, Israel, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Visual genome: connecting language and vision using crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 123(1):32–73, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. (2012)</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances of Neural Information Processing Systems</em>, pp.  1106–1114, Lake Tahoe, Nevada, US, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuznetsova et al. (2018)</span>
<span class="ltx_bibblock">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio Ferrari.

</span>
<span class="ltx_bibblock">The open images dataset v4: unified image classification, object detection, and visual relationship detection at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/1811.00982, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LAION (2023)</span>
<span class="ltx_bibblock">
LAION.

</span>
<span class="ltx_bibblock">Gpt-4v dataset, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/laion/gpt4v-dataset" title="">https://huggingface.co/datasets/laion/gpt4v-dataset</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2025)</span>
<span class="ltx_bibblock">
Weixian Lei, Jiacong Wang, Haochen Wang, Xiangtai Li, Jun Hao Liew, Jiashi Feng, and Zilong Huang.

</span>
<span class="ltx_bibblock">The scalability of simplicity: Empirical analysis of vision-language learning with a single transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2504.10462, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lerner et al. (2022)</span>
<span class="ltx_bibblock">
Paul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé Le Borgne, Romaric Besançon, José G Moreno, and Jesús Lovón Melgarejo.

</span>
<span class="ltx_bibblock">Viquae, a dataset for knowledge-based visual question answering about named entities.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">ACM SIGIR Conference on Research and Development in Information Retrieval</em>, pp.  3108–3120, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024a)</span>
<span class="ltx_bibblock">
Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li.

</span>
<span class="ltx_bibblock">Llava-next: stronger llms supercharge multimodal capabilities in the wild, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/" title="">https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024b)</span>
<span class="ltx_bibblock">
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li.

</span>
<span class="ltx_bibblock">Llava-onevision: easy visual task transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2408.03326, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.

</span>
<span class="ltx_bibblock">Seed-bench: benchmarking multimodal llms with generative comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.16125, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2025a)</span>
<span class="ltx_bibblock">
Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, and Hongkai Xiong.

</span>
<span class="ltx_bibblock">Onecat: Decoder-only auto-regressive model for unified understanding and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2509.03498, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2025b)</span>
<span class="ltx_bibblock">
Tianle Li, Yongming Rao, Winston Hu, and Yu Cheng.

</span>
<span class="ltx_bibblock">BREEN: bridge data-efficient encoder-free multimodal learning with learnable queries.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2503.12446, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Evaluating object hallucination in large vision-language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language Processing</em>, pp.  292–305, Singapore, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023c)</span>
<span class="ltx_bibblock">
Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan L Yuille.

</span>
<span class="ltx_bibblock">Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  14963–14973, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2024)</span>
<span class="ltx_bibblock">
Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, and Xi Victoria Lin.

</span>
<span class="ltx_bibblock">Mixture-of-transformers: A sparse and scalable architecture for multi-modal foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2411.04996, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. (2025)</span>
<span class="ltx_bibblock">
Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang.

</span>
<span class="ltx_bibblock">Mogao: An omni foundation model for interleaved multi-modal generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2505.05472, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Ghosh, Luke Zettlemoyer, and Armen Aghajanyan.

</span>
<span class="ltx_bibblock">Moma: efficient early-fusion pre-training with mixture of modality-aware experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2407.21770, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lindström &amp; Abraham (2022)</span>
<span class="ltx_bibblock">
Adam Dahlgren Lindström and Savitha Sam Abraham.

</span>
<span class="ltx_bibblock">Clevr-math: A dataset for compositional language, visual and mathematical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2208.05358, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Fangyu Liu, Guy Emerson, and Nigel Collier.

</span>
<span class="ltx_bibblock">Visual spatial reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:635–651, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.

</span>
<span class="ltx_bibblock">Aligning large multi-modal model with robust instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023c)</span>
<span class="ltx_bibblock">
Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu.

</span>
<span class="ltx_bibblock">Mmc: Advancing multimodal chart understanding with large-scale instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2311.10774, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023d)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances of Neural Information Processing Systems</em>, New Orleans, LA, USA, 2023d.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  26286–26296, Seattle, WA, USA, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Xi Liu, Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao, et al.

</span>
<span class="ltx_bibblock">Icdar 2019 robust reading challenge on reading chinese text on signboard.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/1912.09641, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024b)</span>
<span class="ltx_bibblock">
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin.

</span>
<span class="ltx_bibblock">Mmbench: is your multi-modal model an all-around player?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, volume 15064, pp.  216–233, Milan, Italy, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023e)</span>
<span class="ltx_bibblock">
Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, and Xiang Bai.

</span>
<span class="ltx_bibblock">On the hidden mystery of ocr in large multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.07895, 2023e.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, New Orleans, LA, USA, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2021)</span>
<span class="ltx_bibblock">
Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu.

</span>
<span class="ltx_bibblock">Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2105.04165, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022a)</span>
<span class="ltx_bibblock">
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.

</span>
<span class="ltx_bibblock">Learn to explain: multimodal reasoning via thought chains for science question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances of Neural Information Processing Systems</em>, volume 35, pp.  2507–2521, New Orleans, LA, USA, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022b)</span>
<span class="ltx_bibblock">
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan.

</span>
<span class="ltx_bibblock">Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2209.14610, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2024)</span>
<span class="ltx_bibblock">
Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu.

</span>
<span class="ltx_bibblock">Mono-internvl: pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2410.08202, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2025)</span>
<span class="ltx_bibblock">
Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Mono-internvl-1.5: Towards cheaper and faster monolithic multimodal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2507.12566, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. (2016)</span>
<span class="ltx_bibblock">
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L. Yuille, and Kevin Murphy.

</span>
<span class="ltx_bibblock">Generation and comprehension of unambiguous object descriptions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  11–20, Las Vegas, NV, USA, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al. (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.

</span>
<span class="ltx_bibblock">OK-VQA: a visual question answering benchmark requiring external knowledge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  3195–3204, Vienna, Austria, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et al. (2022)</span>
<span class="ltx_bibblock">
Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque.

</span>
<span class="ltx_bibblock">Chartqa: a benchmark for question answering about charts with visual and logical reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>, pp.  2263–2279, Dublin, Ireland, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathew et al. (2022)</span>
<span class="ltx_bibblock">
Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V. Jawahar.

</span>
<span class="ltx_bibblock">Infographicvqa.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Winter Conference on Applications of Computer Vision</em>, pp.  2582–2591, Waikoloa, HI, USA, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Methani et al. (2020)</span>
<span class="ltx_bibblock">
Nitesh Methani, Pritha Ganguly, Mitesh M Khapra, and Pratyush Kumar.

</span>
<span class="ltx_bibblock">Plotqa: Reasoning over scientific plots.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Winter Conference on Applications of Computer Vision</em>, pp.  1527–1536, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. (2019)</span>
<span class="ltx_bibblock">
Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty.

</span>
<span class="ltx_bibblock">OCR-VQA: visual question answering by reading text in images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition</em>, pp.  947–952, Sydney, Australia, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2025)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-5: A unified multimodal model, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/gpt-5" title="">https://openai.com/gpt-5</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, volume 139, pp.  8748–8763, virtual, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et al. (2021)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.

</span>
<span class="ltx_bibblock">LAION-400M: open dataset of clip-filtered 400 million image-text pairs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2111.02114, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et al. (2022)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes, and Romain Beaumont.

</span>
<span class="ltx_bibblock">Laion coco: 600m synthetic captions from laion2b-en, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://laion.ai/blog/laion-coco/" title="">https://laion.ai/blog/laion-coco/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk et al. (2022)</span>
<span class="ltx_bibblock">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.

</span>
<span class="ltx_bibblock">A-OKVQA: a benchmark for visual question answering using world knowledge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, volume 13668, pp.  146–162, Tel Aviv, Israel, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al. (2019)</span>
<span class="ltx_bibblock">
Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar.

</span>
<span class="ltx_bibblock">Kvqa: Knowledge-aware visual question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">AAAI Conference on Artificial Intelligence</em>, volume 33, pp.  8876–8884, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2017)</span>
<span class="ltx_bibblock">
Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang, Pei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang Bai.

</span>
<span class="ltx_bibblock">Icdar2017 competition on reading chinese text in the wild (rctw-17).

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition</em>, volume 1, pp.  1429–1434. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shukor et al. (2025a)</span>
<span class="ltx_bibblock">
Mustafa Shukor, Louis Béthune, Dan Busbridge, David Grangier, Enrico Fini, Alaaeldin El-Nouby, and Pierre Ablin.

</span>
<span class="ltx_bibblock">Scaling laws for optimal data mixtures.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2507.09404, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shukor et al. (2025b)</span>
<span class="ltx_bibblock">
Mustafa Shukor, Enrico Fini, Victor Guilherme Turrisi da Costa, Matthieu Cord, Joshua M. Susskind, and Alaaeldin El-Nouby.

</span>
<span class="ltx_bibblock">Scaling laws for native multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2504.07951, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidorov et al. (2020)</span>
<span class="ltx_bibblock">
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.

</span>
<span class="ltx_bibblock">Textcaps: a dataset for image captioning with reading comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, volume 12347, pp.  742–758, Glasgow, UK, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2019)</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Towards vqa models that can read.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  8317–8326, Long Beach, CA, USA, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2019)</span>
<span class="ltx_bibblock">
Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, et al.

</span>
<span class="ltx_bibblock">Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition</em>, pp.  1557–1562, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tao et al. (2025)</span>
<span class="ltx_bibblock">
Chenxin Tao, Shiqian Su, Xizhou Zhu, Chenyu Zhang, Zhe Chen, Jiawen Liu, Wenhai Wang, Lewei Lu, Gao Huang, Yu Qiao, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Hovle: Unleashing the power of monolithic vision-language models with holistic vision-language embedding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  14559–14569, Nashville, TN, USA, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto.

</span>
<span class="ltx_bibblock">Alpaca: A strong, replicable instruction-following model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html</em>, 3(6):7, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2024)</span>
<span class="ltx_bibblock">
Chameleon Team.

</span>
<span class="ltx_bibblock">Chameleon: mixed-modal early-fusion foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2405.09818, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teknium (2023)</span>
<span class="ltx_bibblock">
Teknium.

</span>
<span class="ltx_bibblock">Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/teknium/OpenHermes-2.5" title="">https://huggingface.co/datasets/teknium/OpenHermes-2.5</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom.

</span>
<span class="ltx_bibblock">Llama 2: open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.09288, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tschannen et al. (2025)</span>
<span class="ltx_bibblock">
Michael Tschannen, Alexey A. Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier J. Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai.

</span>
<span class="ltx_bibblock">Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2502.14786, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Veit et al. (2016)</span>
<span class="ltx_bibblock">
Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie.

</span>
<span class="ltx_bibblock">Coco-text: Dataset and benchmark for text detection and recognition in natural images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/1601.07140, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2025a)</span>
<span class="ltx_bibblock">
Han Wang, Yongjie Ye, Bingru Li, Yuxiang Nie, Jinghui Lu, Jingqun Tang, Yanjie Wang, and Can Huang.

</span>
<span class="ltx_bibblock">Vision as lora.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2503.20680, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang.

</span>
<span class="ltx_bibblock">To see is to believe: prompting GPT-4V for better visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2311.07574, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024a)</span>
<span class="ltx_bibblock">
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin.

</span>
<span class="ltx_bibblock">Qwen2-vl: enhancing vision-language model’s perception of the world at any resolution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2409.12191, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2025b)</span>
<span class="ltx_bibblock">
Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al.

</span>
<span class="ltx_bibblock">Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2508.18265, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024b)</span>
<span class="ltx_bibblock">
Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang.

</span>
<span class="ltx_bibblock">Emu3: next-token prediction is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2409.18869, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2025)</span>
<span class="ltx_bibblock">
Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, and Dahua Lin.

</span>
<span class="ltx_bibblock">Videorope: What makes for good video rotary position embedding?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2502.05173, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2025)</span>
<span class="ltx_bibblock">
Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu.

</span>
<span class="ltx_bibblock">Omnigen2: Exploration to advanced multimodal generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2506.18871, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">xAI (2024)</span>
<span class="ltx_bibblock">
xAI.

</span>
<span class="ltx_bibblock">Grok-1.5 vision preview, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://x.ai/blog/grok-1.5v" title="">https://x.ai/blog/grok-1.5v</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">xAI (2025)</span>
<span class="ltx_bibblock">
xAI.

</span>
<span class="ltx_bibblock">Grok 3: xAI’s flagship ai model, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://x.ai/news/grok-3" title="">https://x.ai/news/grok-3</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2025)</span>
<span class="ltx_bibblock">
Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Zunnan Xu, Zhaoyang Zhang, Yixiao Ge, Xiu Li, and Ying Shan.

</span>
<span class="ltx_bibblock">Haploomni: Unified single transformer for multimodal video understanding and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2506.02975, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2025)</span>
<span class="ltx_bibblock">
Rui Yan, Lin Song, Yicheng Xiao, Runhui Huang, Yixiao Ge, Ying Shan, and Hengshuang Zhao.

</span>
<span class="ltx_bibblock">Haplovl: A single-transformer baseline for multi-modal understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2503.14694, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2025)</span>
<span class="ltx_bibblock">
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu.

</span>
<span class="ltx_bibblock">Qwen3 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2505.09388, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.

</span>
<span class="ltx_bibblock">The dawn of lmms: preliminary explorations with gpt-4v(ision).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.17421, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2016)</span>
<span class="ltx_bibblock">
Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg.

</span>
<span class="ltx_bibblock">Modeling context in referring expressions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, volume 9906, pp.  69–85, Amsterdam, The Netherlands, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.

</span>
<span class="ltx_bibblock">Metamath: Bootstrap your own mathematical questions for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2309.12284, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2024)</span>
<span class="ltx_bibblock">
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.

</span>
<span class="ltx_bibblock">Mm-vet: evaluating large multimodal models for integrated capabilities.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, Vienna, Austria, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2025)</span>
<span class="ltx_bibblock">
Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, Hao Luo, Jiasheng Tang, Fan Wang, and Yi Yang.

</span>
<span class="ltx_bibblock">Lumos-1: On autoregressive video generation from a unified model perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2507.08801, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2019)</span>
<span class="ltx_bibblock">
Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang Mu, and Shi-Min Hu.

</span>
<span class="ltx_bibblock">A large chinese text dataset in the wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Journal of Computer Science and Technology</em>, 34(3):509–521, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al. (2024)</span>
<span class="ltx_bibblock">
Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.

</span>
<span class="ltx_bibblock">MMMU: a massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition</em>, pp.  9556–9567, Seattle, WA, USA, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al. (2023)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.

</span>
<span class="ltx_bibblock">Sigmoid loss for language image pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision</em>, pp.  11941–11952, Paris, France, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Sennrich (2019)</span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Advances of Neural Information Processing Systems</em>, pp.  12360–12371, Vancouver, BC, Canada, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Bo Zhao, Boya Wu, and Tiejun Huang.

</span>
<span class="ltx_bibblock">SVIT: scaling up visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.04087, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances of Neural Information Processing Systems</em>, 36:46595–46623, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2025)</span>
<span class="ltx_bibblock">
Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang.

</span>
<span class="ltx_bibblock">Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">CoRR</em>, abs/2504.10479, 2025.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SSx1">
<h3 class="ltx_title ltx_title_subsection">Usage of Large Language Models</h3>
<div class="ltx_para ltx_noindent" id="A1.SSx1.p1">
<p class="ltx_p">During manuscript preparation, large language models were used solely as writing assistants.
They helped to check grammar, refine sentence structure, and provide style alternatives.
All content related to methodology, experiments, and conclusions was developed entirely by the authors.
LLM outputs were reviewed critically, and only human-verified edits were incorporated into the final text.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Supervised Fine-tuning Datasets</h3>
<figure class="ltx_table" id="A1.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Dataset summary in supervised fine-tuning stage.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:108.4pt;vertical-align:-53.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-313.7pt,78.4pt) scale(0.40865034652936,0.40865034652936) ;">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold">Task</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt"><span class="ltx_text ltx_font_bold">Dataset</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Captioning</td>
<td class="ltx_td ltx_align_left ltx_border_t">TextCaps (en) <cite class="ltx_cite ltx_citemacro_cite">Sidorov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib92" title="">2020</a>)</cite>, ShareGPT4V (en&amp;zh) <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib13" title="">2024b</a>)</cite>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">VQAv2 (en) <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib34" title="">2017</a>)</cite>, GQA (en) <cite class="ltx_cite ltx_citemacro_cite">Hudson &amp; Manning (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib39" title="">2019</a>)</cite>, OKVQA (en) <cite class="ltx_cite ltx_citemacro_cite">Marino et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib78" title="">2019</a>)</cite>,</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#ECECEC;">
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">General QA</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">VSR (en) <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib63" title="">2023a</a>)</cite>, VisualDialog (en) <cite class="ltx_cite ltx_citemacro_cite">Das et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib22" title="">2017</a>)</cite></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Science</td>
<td class="ltx_td ltx_align_left">AI2D (en) <cite class="ltx_cite ltx_citemacro_cite">Kembhavi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib43" title="">2016</a>)</cite>, ScienceQA (en) <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib73" title="">2022a</a>)</cite>, TQA (en) <cite class="ltx_cite ltx_citemacro_cite">Kembhavi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib44" title="">2017</a>)</cite>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">ChartQA (en) <cite class="ltx_cite ltx_citemacro_cite">Masry et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib79" title="">2022</a>)</cite>, MMC-Inst (en) <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib65" title="">2023c</a>)</cite>, DVQA (en) <cite class="ltx_cite ltx_citemacro_cite">Kafle et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib42" title="">2018</a>)</cite>,</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#ECECEC;">
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">Chart</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">PlotQA (en) <cite class="ltx_cite ltx_citemacro_cite">Methani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib81" title="">2020</a>)</cite>, LRV-Instruction (en) <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib64" title="">2023b</a>)</cite></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left">GeoQA+ (en) <cite class="ltx_cite ltx_citemacro_cite">Cao &amp; Xiao (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib10" title="">2022</a>)</cite>, TabMWP (en) <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib74" title="">2022b</a>)</cite>, MathQA (en) <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib116" title="">2023</a>)</cite>,</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Mathematics</td>
<td class="ltx_td ltx_align_left">CLEVR-Math/Super (en) <cite class="ltx_cite ltx_citemacro_cite">Lindström &amp; Abraham (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib62" title="">2022</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib58" title="">2023c</a>)</cite>, Geometry3K (en) <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib72" title="">2021</a>)</cite>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">KVQA (en) <cite class="ltx_cite ltx_citemacro_cite">Shah et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib88" title="">2019</a>)</cite>, A-OKVQA (en) <cite class="ltx_cite ltx_citemacro_cite">Schwenk et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib87" title="">2022</a>)</cite>, ViQuAE (en) <cite class="ltx_cite ltx_citemacro_cite">Lerner et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib51" title="">2022</a>)</cite>,</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#ECECEC;">
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">Knowledge</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">Wikipedia (en&amp;zh) <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib37" title="">2023</a>)</cite></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left">OCRVQA (en) <cite class="ltx_cite ltx_citemacro_cite">Mishra et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib82" title="">2019</a>)</cite>, InfoVQA (en) <cite class="ltx_cite ltx_citemacro_cite">Mathew et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib80" title="">2022</a>)</cite>, TextVQA (en) <cite class="ltx_cite ltx_citemacro_cite">Singh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib93" title="">2019</a>)</cite>,</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left">ArT (en&amp;zh) <cite class="ltx_cite ltx_citemacro_cite">Chng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib18" title="">2019</a>)</cite>, COCO-Text (en) <cite class="ltx_cite ltx_citemacro_cite">Veit et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib101" title="">2016</a>)</cite>, CTW (zh) <cite class="ltx_cite ltx_citemacro_cite">Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib119" title="">2019</a>)</cite>,</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left">LSVT (zh) <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib94" title="">2019</a>)</cite>, RCTW-17 (zh) <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib89" title="">2017</a>)</cite>, ReCTs (zh) <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib68" title="">2019</a>)</cite>,</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">OCR</td>
<td class="ltx_td ltx_align_left">SynthDoG (en&amp;zh) <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib45" title="">2022</a>)</cite>, ST-VQA (en) <cite class="ltx_cite ltx_citemacro_cite">Biten et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib8" title="">2019</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#ECECEC;">
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">Document</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">DocVQA (en) <cite class="ltx_cite ltx_citemacro_cite">Clark &amp; Gardner (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib19" title="">2018</a>)</cite>, Common Crawl PDF (en&amp;zh)</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Grounding</td>
<td class="ltx_td ltx_align_left">RefCOCO/+/g (en) <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib115" title="">2016</a>); Mao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib77" title="">2016</a>)</cite>, Visual Genome (en) <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib46" title="">2017</a>)</cite>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">LLaVA-150K (en&amp;zh) <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib66" title="">2023d</a>)</cite>, LVIS-Instruct4V (en) <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib103" title="">2023</a>)</cite>,</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">ALLaVA (en&amp;zh) <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib11" title="">2024a</a>)</cite>, Laion-GPT4V (en) <cite class="ltx_cite ltx_citemacro_cite">LAION (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib49" title="">2023</a>)</cite>,</span></td>
</tr>
<tr class="ltx_tr" style="--ltx-bg-color:#ECECEC;">
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">Conversation</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text" style="--ltx-bg-color:#ECECEC;">TextOCR-GPT4V (en) <cite class="ltx_cite ltx_citemacro_cite">Jimmycarter (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib41" title="">2023</a>)</cite>, SVIT (en&amp;zh) <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib123" title="">2023</a>)</cite></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left">OpenHermes2.5 (en) <cite class="ltx_cite ltx_citemacro_cite">Teknium (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib98" title="">2023</a>)</cite>, Alpaca-GPT4 (en) <cite class="ltx_cite ltx_citemacro_cite">Taori et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib96" title="">2023</a>)</cite>, COIG-CQIA (zh) <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib5" title="">2024</a>)</cite>,</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Text-only</td>
<td class="ltx_td ltx_align_left ltx_border_bb">ShareGPT (en&amp;zh) <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2510.14979v1#bib.bib124" title="">2023</a>)</cite>
</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Implementation Details</h3>
<figure class="ltx_table" id="A1.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Implementation details in the pre-training, mid-training and supervise fine-tuning.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">Configuration</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Pre-Training</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Mid-Training</td>
<td class="ltx_td ltx_align_center ltx_border_tt">Supervised Fine-Tuning</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Resolution</td>
<td class="ltx_td ltx_align_center ltx_border_t"><math alttext="256^{2}-1,024^{2}" class="ltx_Math" display="inline" id="A1.T5.m1" intent=":literal"><semantics><mrow><mrow><msup><mn>256</mn><mn>2</mn></msup><mo>−</mo><mn>1</mn></mrow><mo>,</mo><msup><mn>024</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">256^{2}-1,024^{2}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math alttext="256^{2}-2,048^{2}" class="ltx_Math" display="inline" id="A1.T5.m2" intent=":literal"><semantics><mrow><mrow><msup><mn>256</mn><mn>2</mn></msup><mo>−</mo><mn>2</mn></mrow><mo>,</mo><msup><mn>048</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">256^{2}-2,048^{2}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math alttext="256^{2}-2,048^{2}" class="ltx_Math" display="inline" id="A1.T5.m3" intent=":literal"><semantics><mrow><mrow><msup><mn>256</mn><mn>2</mn></msup><mo>−</mo><mn>2</mn></mrow><mo>,</mo><msup><mn>048</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">256^{2}-2,048^{2}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Optimizer</td>
<td class="ltx_td ltx_align_center" colspan="3">AdamW</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Optimizer hyperparameters</td>
<td class="ltx_td ltx_align_center" colspan="3"><math alttext="\beta_{1}=0.9,\quad\beta_{2}=0.999,\quad eps=1e^{-8}" class="ltx_Math" display="inline" id="A1.T5.m4" intent=":literal"><semantics><mrow><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><mo rspace="1.167em">,</mo><mrow><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi></mrow><mo>=</mo><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>8</mn></mrow></msup></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\beta_{1}=0.9,\quad\beta_{2}=0.999,\quad eps=1e^{-8}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Learning rate schedule</td>
<td class="ltx_td ltx_align_center">cosine with min lr</td>
<td class="ltx_td ltx_align_center">cosine with min lr</td>
<td class="ltx_td ltx_align_center">cosine decay</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Peak learning rate</td>
<td class="ltx_td ltx_align_center"><math alttext="8e^{-4}" class="ltx_Math" display="inline" id="A1.T5.m5" intent=":literal"><semantics><mrow><mn>8</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">8e^{-4}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="4e^{-5}" class="ltx_Math" display="inline" id="A1.T5.m6" intent=":literal"><semantics><mrow><mn>4</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">4e^{-5}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="5e^{-5}" class="ltx_Math" display="inline" id="A1.T5.m7" intent=":literal"><semantics><mrow><mn>5</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5e^{-5}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Min learning rate ratio</td>
<td class="ltx_td ltx_align_center"><math alttext="0.05" class="ltx_Math" display="inline" id="A1.T5.m8" intent=":literal"><semantics><mn>0.05</mn><annotation encoding="application/x-tex">0.05</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="0.1" class="ltx_Math" display="inline" id="A1.T5.m9" intent=":literal"><semantics><mn>0.1</mn><annotation encoding="application/x-tex">0.1</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Weight decay</td>
<td class="ltx_td ltx_align_center" colspan="3"><math alttext="0.01" class="ltx_Math" display="inline" id="A1.T5.m10" intent=":literal"><semantics><mn>0.01</mn><annotation encoding="application/x-tex">0.01</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Training steps</td>
<td class="ltx_td ltx_align_center">
<math alttext="190" class="ltx_Math" display="inline" id="A1.T5.m11" intent=":literal"><semantics><mn>190</mn><annotation encoding="application/x-tex">190</annotation></semantics></math>k</td>
<td class="ltx_td ltx_align_center">
<math alttext="50" class="ltx_Math" display="inline" id="A1.T5.m12" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math>k</td>
<td class="ltx_td ltx_align_center">
<math alttext="6" class="ltx_Math" display="inline" id="A1.T5.m13" intent=":literal"><semantics><mn>6</mn><annotation encoding="application/x-tex">6</annotation></semantics></math>k</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Warm-up steps</td>
<td class="ltx_td ltx_align_center">
<math alttext="2" class="ltx_Math" display="inline" id="A1.T5.m14" intent=":literal"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math>k</td>
<td class="ltx_td ltx_align_center"><math alttext="200" class="ltx_Math" display="inline" id="A1.T5.m15" intent=":literal"><semantics><mn>200</mn><annotation encoding="application/x-tex">200</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="200" class="ltx_Math" display="inline" id="A1.T5.m16" intent=":literal"><semantics><mn>200</mn><annotation encoding="application/x-tex">200</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Max sample length</td>
<td class="ltx_td ltx_align_center"><math alttext="8,192" class="ltx_Math" display="inline" id="A1.T5.m17" intent=":literal"><semantics><mrow><mn>8</mn><mo>,</mo><mn>192</mn></mrow><annotation encoding="application/x-tex">8,192</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="8,192" class="ltx_Math" display="inline" id="A1.T5.m18" intent=":literal"><semantics><mrow><mn>8</mn><mo>,</mo><mn>192</mn></mrow><annotation encoding="application/x-tex">8,192</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="8,192" class="ltx_Math" display="inline" id="A1.T5.m19" intent=":literal"><semantics><mrow><mn>8</mn><mo>,</mo><mn>192</mn></mrow><annotation encoding="application/x-tex">8,192</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Global batch size</td>
<td class="ltx_td ltx_align_center"><math alttext="2,560" class="ltx_Math" display="inline" id="A1.T5.m20" intent=":literal"><semantics><mrow><mn>2</mn><mo>,</mo><mn>560</mn></mrow><annotation encoding="application/x-tex">2,560</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"><math alttext="1,200" class="ltx_Math" display="inline" id="A1.T5.m21" intent=":literal"><semantics><mrow><mn>1</mn><mo>,</mo><mn>200</mn></mrow><annotation encoding="application/x-tex">1,200</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center"> <math alttext="650" class="ltx_Math" display="inline" id="A1.T5.m22" intent=":literal"><semantics><mn>650</mn><annotation encoding="application/x-tex">650</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Text-only ratio</td>
<td class="ltx_td ltx_align_center">0.3</td>
<td class="ltx_td ltx_align_center">0.3</td>
<td class="ltx_td ltx_align_center">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">Numerical precision</td>
<td class="ltx_td ltx_align_center ltx_border_bb" colspan="3"><math alttext="\mathtt{bfloat16}" class="ltx_Math" display="inline" id="A1.T5.m23" intent=":literal"><semantics><mi>𝚋𝚏𝚕𝚘𝚊𝚝𝟷𝟼</mi><annotation encoding="application/x-tex">\mathtt{bfloat16}</annotation></semantics></math></td>
</tr>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Limitation and Discussion</h3>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p">In this study, we innovate network architectures and training strategies for efficiently building native vision-language models.
The full promise of NEO has remained largely untapped, hindered by scarce training data and limited computational resources, especially in knowledge-intensive and OCR-focused domains.
Yet, strikingly, our NEO rivals state-of-the-art VLMs despite these severe constraints.
We envision subsequent directions of NEO for the native VLM community as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Contextual relevance to recent advancements.</span>
Recent models such as Qwen3VL highlight concepts that resonate with our design choices, including dense linking of visual-language features, relative positional encodings, and architectural details like patch embedding and bias. In particular, the DeepStack approach underscores the importance of establishing strong pixel-word associations from the earliest stages, reinforcing the significance of densely integrated visual-language representations.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Maximizing the potential via large investment.</span>
It is in great demand for continuously investing substantial resources, especially during the pre-training stage, to fully unlock NEO’s performance and approach the upper bound of the native model. At the same time, selectively open-sourcing key components during intermediate development can reduce follow-up training costs for future researchers and attract more research to native visual-language models. Moreover, the fundamental models from this work provide a valuable baseline for advancing reinforcement learning research.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Explorations of full-spectrum model capacities.</span>
Expanding the full model sizes remains a critical factor in advancing various real-world applications. Even with limited resources, NEO-2.2B closely matches those of modular visual-language models with equivalent capacity, suggesting that the design philosophy of models in the 0.6 to 8 billion parameter range has matured. Such architectures not only achieve high performance but also facilitate the deployment of lightweight models at the edge, which is crucial for scenarios with limited computational resources or strict real-time requirements.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Upgrading architectures and applications.</span>
To date, our work has focused on dense models for image-text understanding, while a sparse divide-and-conquer architecture is simultaneously under active development.
Notably, we regard NEO not merely as an autoregressive VLM but as a new paradigm for visual-language intelligence.
Its principle is to leverage end-to-end training within a unified architecture, eliminating manually imposed biases and scaling-up complexities by allowing data and models to dictate the learning process.
Besides, our efforts are designed not merely to improve performance but to establish a definitive baseline for visual-language generation, long video understanding, and embodied AI. Crucially, NEO’s architecture systematically integrates the demands of video generation and related tasks, including attention mechanisms and rotary positional encodings, from the ground up. Although currently focused on text and images, NEO is poised to push the boundaries of what is possible across a wide spectrum of application scenarios and input modalities.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p6">
<p class="ltx_p">Constrained by current text corpus and computational resources, we are unable to train a fully native model entirely from scratch without initialization from an existing LLM. This limitation also hinders our ability to mitigate potential biases arising from the dominance of the language modality. Despite these challenges, our NEO extends beyond providing a reusable pre-buffer that lowers the cost of adapting advanced LLMs—with updated weights and stronger capabilities—into VLMs under limited budgets. More importantly, NEO reveals the potential performance ceiling of native VLM architectures and provides valuable insights for future research on de novo multimodal training.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 16 13:19:47 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
