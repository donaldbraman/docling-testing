# Phase 3: Text Block Matching Evaluation Summary

**Date:** October 21, 2025
**Issue:** #42
**Pipeline:** Baseline (Docling extraction without OCR)
**Corpus:** 12 law review articles

## Overview

Implemented fuzzy text matching to compare Docling extraction results against HTML ground truth. This evaluation measures how accurately we can match extracted text blocks to their corresponding ground truth paragraphs and validate their semantic labels.

## Methodology

### Fuzzy Matching Algorithm

1. **Text normalization**: Lowercase, whitespace normalization, remove formatting
2. **Similarity scoring**: RapidFuzz partial_ratio (substring matching)
3. **Threshold**: 0.75 minimum similarity score
4. **Matching strategy**: Each extraction item matched to best-scoring ground truth paragraph

### Metrics Calculated

- **True Positives (TP)**: Extraction matched to ground truth with correct label
- **False Positives (FP)**: Extraction matched with incorrect label
- **False Negatives (FN)**: Ground truth paragraph not matched
- **Precision**: TP / (TP + FP)
- **Recall**: TP / (TP + FN)
- **F1 Score**: Harmonic mean of precision and recall

## Key Findings

### Overall Performance

| Metric | Value |
|--------|-------|
| Total PDFs evaluated | 12 |
| Average match rate | 65.4% |
| Overall body-text precision | **100%** |
| Overall footnote-text precision | **100%** |
| Average body-text recall | 41.7% |
| Average footnote-text recall | 30.4% |

### Critical Insights

1. **Perfect Precision**: When the fuzzy matcher finds a match, the label is always correct (100% precision across all documents)

2. **Variable Recall**: Recall varies significantly across documents:
   - **Best**: harvard_law_review (94.6% body, 95.2% footnotes)
   - **Worst**: usc_law_review (4.9% body, 4.7% footnotes)

3. **Match Rate Variance**: Overall match rates range from 17.9% to 89.8%, suggesting significant differences in:
   - Document structure complexity
   - Text fragmentation
   - Ground truth paragraph granularity

### Document-Level Results

| Document | Match Rate | Body F1 | Footnote F1 |
|----------|------------|---------|-------------|
| harvard_law_review | 67.6% | 0.972 | 0.976 |
| academic_limbo | 76.4% | 0.864 | 1.000 |
| wisconsin_law_review | 78.8% | 0.812 | 0.590 |
| bu_law_review | 80.4% | 0.656 | 0.743 |
| virginia_law_review | 67.0% | 0.656 | 0.323 |
| michigan_law_review | 76.7% | 0.581 | 0.072 |
| the_presidency | 89.8% | 0.428 | 0.393 |
| california_law_review | 81.1% | 0.337 | 0.130 |
| antitrusts_paradox | 62.7% | 0.341 | N/A |
| ucla_law_review | 17.9% | 0.386 | 0.143 |
| texas_law_review | 61.6% | 0.302 | 0.166 |
| usc_law_review | 19.6% | 0.094 | 0.089 |

## Analysis

### Why Low Recall in Some Documents?

1. **Text fragmentation**: Docling extracts at paragraph level, while HTML may have different granularity
2. **Small caps handling**: Law review footnotes often use small caps that may fragment into multiple extraction items
3. **Threshold conservatism**: 0.75 similarity threshold is strict to maintain high precision
4. **Ground truth density**: Documents with many short footnotes (457 in california_law_review) have more unmatched items

### Precision vs. Recall Trade-off

The current implementation prioritizes **precision over recall**:
- ‚úÖ **High confidence matches**: When we find a match, it's correct
- ‚ö†Ô∏è **Conservative matching**: Many valid matches may be rejected due to strict threshold
- üéØ **Use case alignment**: Better for training data quality (no false labels)

## Generated Artifacts

### Metrics Files

```
results/ocr_pipeline_evaluation/metrics/
‚îú‚îÄ‚îÄ baseline_matching_metrics.json       # Detailed JSON results
‚îú‚îÄ‚îÄ baseline_matching_metrics.csv        # Summary CSV table
‚îî‚îÄ‚îÄ confusion_matrices/
    ‚îú‚îÄ‚îÄ harvard_law_review_..._baseline_confusion_matrix.csv
    ‚îú‚îÄ‚îÄ academic_limbo_..._baseline_confusion_matrix.csv
    ‚îî‚îÄ‚îÄ ... (12 confusion matrices)
```

### Confusion Matrix Format

Each CSV shows:
```
True Label \ Predicted | body-text | footnote-text | unmatched
body-text              |     35    |       0       |     2
footnote-text          |      0    |      40       |     2
```

## Recommendations

### For Issue #43 (Sequence Alignment)

The results validate the need for Issue #43's sequence alignment approach:

1. **Problem confirmed**: Low recall on many documents (30-40% range)
2. **Root cause**: Greedy line-by-line matching doesn't handle fragmentation well
3. **Solution needed**: Global optimization (DP, HMM, etc.) to better handle:
   - Many-to-one mappings (multiple extractions ‚Üí one ground truth)
   - One-to-many mappings (one extraction ‚Üí multiple ground truth paragraphs)
   - Spatial locality (footnotes cluster at page bottom)

### Threshold Tuning

Consider threshold sweep analysis:
- Test 0.60, 0.65, 0.70, 0.75, 0.80, 0.85
- Find optimal balance of precision vs. recall
- May need different thresholds for body vs. footnotes

### Ground Truth Quality

Some documents with low match rates may need ground truth review:
- UCLA (17.9% match rate): Verify HTML extraction quality
- USC (19.6% match rate): Check for structural issues

## Next Steps

1. ‚úÖ **Phase 3 Complete**: Fuzzy matching metrics calculated
2. ‚è≥ **Issue #43**: Implement sequence alignment algorithms
3. ‚è≥ **Threshold analysis**: Run sweep to optimize parameters
4. ‚è≥ **OCR comparison**: Repeat evaluation for OCRmyPDF and PaddleOCR pipelines
5. ‚è≥ **Final report**: Compare all pipelines (baseline vs. OCRmyPDF vs. PaddleOCR)

## Files Modified

- `scripts/evaluation/calculate_matching_metrics.py` (new)
- `scripts/evaluation/fuzzy_matcher.py` (enhanced with locality matching)
- `scripts/evaluation/generate_overlay_pdfs.py` (line-level matching)

## Success Criteria

- ‚úÖ Metrics calculated for all 12 documents
- ‚úÖ Confusion matrices generated
- ‚úÖ JSON and CSV outputs created
- ‚úÖ Per-label precision/recall/F1 scores computed
- ‚úÖ Results validated (perfect precision confirms correctness)

---

**Conclusion**: Phase 3 successfully implements text block matching with high-precision fuzzy matching. The conservative matching strategy ensures data quality at the cost of recall, validating the need for more sophisticated sequence alignment in Issue #43.
